{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09523532-cc0d-4986-a608-ea69304f41e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 开始 Block 1: 数据预处理与环境初始化 ---\n",
      "检测到系统核心数: 14, 将使用 12 个核心进行并行计算。\n",
      "数据处理完成:\n",
      "- 已加载边数: 89,655\n",
      "- 已加载 OD 对: 5,000\n",
      "- 路网平均容量: 30002.34\n",
      "--- Block 1 准备就绪，可以执行 Block 2 ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "\n",
    "# ==========================================\n",
    "# Block 1: 高性能环境与数据预处理\n",
    "# ==========================================\n",
    "\n",
    "# 1. 绝对路径定义\n",
    "base_path = r\"D:\\PyCode\\论文复现与改进\\2025-D\\2507692\\论文复现与优化\\2025_Problem_D_Data\"\n",
    "file_edges = os.path.join(base_path, \"edges_fully_aligned.csv\")\n",
    "file_od = os.path.join(base_path, \"od_demand_matrix_calibrated.csv\")\n",
    "\n",
    "print(\"--- 开始 Block 1: 数据预处理与环境初始化 ---\")\n",
    "\n",
    "# 2. 加载核心数据集\n",
    "# 加载路网数据 (56万条边)\n",
    "edges_df = pd.read_csv(file_edges)\n",
    "# 加载校准后的需求矩阵 (5,000对 OD)\n",
    "od_df = pd.read_csv(file_od)\n",
    "\n",
    "# 3. 路网数据清洗与容量鲁棒性处理 (数据防炸逻辑)\n",
    "# BPR公式中饱和度 V/C 的分母不能为 0。确保所有 capacity 大于 0\n",
    "# 对于支路或缺失数据，设定最小逻辑容量阈值 (如 100 辆/日)\n",
    "min_capacity = 100\n",
    "edges_df['capacity'] = edges_df['capacity'].apply(lambda x: max(x, min_capacity))\n",
    "\n",
    "# 确保自由流时间 (T0) 为正数\n",
    "edges_df['free_flow_time'] = edges_df['free_flow_time'].apply(lambda x: max(x, 0.01))\n",
    "\n",
    "# 4. 向量化参数准备 (用于高性能计算)\n",
    "# 将 BPR 需要的常数转化为 NumPy 数组，以支持 Block 2 中的矩阵运算\n",
    "t0 = edges_df['free_flow_time'].values\n",
    "capacity = edges_df['capacity'].values\n",
    "alpha = 0.15 # BPR 标准参数\n",
    "beta = 4     # BPR 标准参数\n",
    "\n",
    "# 5. 构建 NetworkX 高性能图对象\n",
    "# 预计算初始状态下的阻抗 (V=0 时的自由流时间) 作为搜索起点\n",
    "G = nx.from_pandas_edgelist(\n",
    "    edges_df, 'u', 'v', \n",
    "    edge_attr=['free_flow_time', 'capacity'], \n",
    "    create_using=nx.Graph()\n",
    ")\n",
    "\n",
    "# 6. 多进程并行环境初始化\n",
    "# 获取系统 CPU 核心数，留出 1-2 个核心保证系统稳定\n",
    "num_cores = max(1, mp.cpu_count() - 2)\n",
    "print(f\"检测到系统核心数: {mp.cpu_count()}, 将使用 {num_cores} 个核心进行并行计算。\")\n",
    "\n",
    "# 7. 任务分片 (将 5,000 对 OD 划分为 Batch 以优化并行效率)\n",
    "od_pairs = od_df[['origin', 'destination', 'demand_qij']].values\n",
    "batch_size = int(np.ceil(len(od_pairs) / num_cores))\n",
    "od_batches = [od_pairs[i:i + batch_size] for i in range(0, len(od_pairs), batch_size)]\n",
    "\n",
    "print(f\"数据处理完成:\")\n",
    "print(f\"- 已加载边数: {len(edges_df):,}\")\n",
    "print(f\"- 已加载 OD 对: {len(od_df):,}\")\n",
    "print(f\"- 路网平均容量: {edges_df['capacity'].mean():.2f}\")\n",
    "print(\"--- Block 1 准备就绪，可以执行 Block 2 ---\")\n",
    "\n",
    "# 输出对齐后的结果，供后续 Block 直接引用\n",
    "# 核心变量: G, od_pairs, t0, capacity, alpha, beta, num_cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "727a59be-9b10-43f9-8940-a81c600fb3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 开始 Block 2: BPR 引擎构建 ---\n",
      "初始状态 (V=0) Beckmann 目标函数值: 0.0000\n",
      "测试场景 (V/C=0.5) 平均通行时间: 15.30 min\n",
      "相比自由流时间 (V=0) 增长率: 0.94%\n",
      "--- Block 2 完成 ---\n",
      "说明文档已保存至: D:\\PyCode\\论文复现与改进\\2025-D\\2507692\\论文复现与优化\\2025_Problem_D_Data\\README_Block2.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ==========================================\n",
    "# Block 2: BPR 引擎与 Beckmann 目标函数\n",
    "# ==========================================\n",
    "\n",
    "# 1. 路径设置 (沿用绝对路径)\n",
    "base_path = r\"D:\\PyCode\\论文复现与改进\\2025-D\\2507692\\论文复现与优化\\2025_Problem_D_Data\"\n",
    "file_edges = os.path.join(base_path, \"edges_fully_aligned.csv\")\n",
    "file_readme = os.path.join(base_path, \"README_Block2.txt\")\n",
    "\n",
    "print(\"--- 开始 Block 2: BPR 引擎构建 ---\")\n",
    "\n",
    "# 2. 核心数学函数 (向量化实现)\n",
    "\n",
    "def update_travel_time(flow, t0, capacity, alpha=0.15, beta=4):\n",
    "    \"\"\"\n",
    "    BPR路阻函数: 计算包含拥堵延迟的实时通行时间\n",
    "    T = T0 * (1 + alpha * (V/C)^beta)\n",
    "    \"\"\"\n",
    "    # 向量化运算，flow/t0/capacity 均为 NumPy 数组\n",
    "    travel_time = t0 * (1.0 + alpha * (flow / capacity)**beta)\n",
    "    return travel_time\n",
    "\n",
    "def calculate_beckmann_objective(flow, t0, capacity, alpha=0.15, beta=4):\n",
    "    \"\"\"\n",
    "    Beckmann 目标函数: 计算全网阻抗积分之和\n",
    "    Z = Sum( Va*T0 * (1 + (alpha/(beta+1)) * (Va/Ca)^beta) )\n",
    "    \"\"\"\n",
    "    # 计算每一条边的积分值\n",
    "    integrals = flow * t0 * (1.0 + (alpha / (beta + 1.0)) * (flow / capacity)**beta)\n",
    "    # 返回全网总和 (标量)\n",
    "    return np.sum(integrals)\n",
    "\n",
    "# 3. 数据载入与初始化\n",
    "edges_df = pd.read_csv(file_edges)\n",
    "\n",
    "# 将核心字段转化为向量，确保计算效率\n",
    "# 鲁棒性处理：确保容量 > 0\n",
    "edges_df['capacity'] = edges_df['capacity'].replace(0, 100) \n",
    "t0_vec = edges_df['free_flow_time'].values\n",
    "capacity_vec = edges_df['capacity'].values\n",
    "\n",
    "# 初始化流量向量 (初始状态全网流量为 0)\n",
    "current_flow = np.zeros(len(edges_df))\n",
    "\n",
    "# 4. 示例演示 (机理验证)\n",
    "initial_obj = calculate_beckmann_objective(current_flow, t0_vec, capacity_vec)\n",
    "print(f\"初始状态 (V=0) Beckmann 目标函数值: {initial_obj:,.4f}\")\n",
    "\n",
    "# 模拟一个 50% 饱和度的场景测试 BPR 引擎\n",
    "test_flow = capacity_vec * 0.5\n",
    "updated_time = update_travel_time(test_flow, t0_vec, capacity_vec)\n",
    "print(f\"测试场景 (V/C=0.5) 平均通行时间: {np.mean(updated_time):.2f} min\")\n",
    "print(f\"相比自由流时间 (V=0) 增长率: {((np.mean(updated_time)/np.mean(t0_vec))-1)*100:.2f}%\")\n",
    "\n",
    "# 5. 生成说明文档\n",
    "readme_content = \"\"\"MCM/ICM 2025 Problem D - Block 2: BPR Engine & Beckmann Objective\n",
    "----------------------------------------------------------------------\n",
    "本模块实现了交通流分配的核心反馈机制：\n",
    "1. update_travel_time: 基于流量 V 计算非线性延时 T。\n",
    "2. calculate_beckmann_objective: 计算用于收敛判断的全局目标函数。\n",
    "这些函数利用 NumPy 向量化技术，专门针对 8.9 万条边的大规模路网进行了性能优化。\n",
    "\"\"\"\n",
    "with open(file_readme, \"w\", encoding='utf-8') as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "print(f\"--- Block 2 完成 ---\")\n",
    "print(f\"说明文档已保存至: {file_readme}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa41acea-8d89-44e9-9ffb-cc6e4c8b068c",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import os\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "\n",
    "# ==========================================\n",
    "# Block 3: 并行化全有全无分配 (Parallel AON)\n",
    "# ==========================================\n",
    "\n",
    "# 1. 核心 Worker 函数: 在子进程中执行路径搜索\n",
    "def worker_aon_assignment(od_chunk, edge_id_map, edges_list, weight_dict):\n",
    "    \"\"\"\n",
    "    单个进程的任务：计算分配给它的 OD 块的最短路径并累加流量\n",
    "    \"\"\"\n",
    "    # 局部图构建（每个子进程拥有独立的图对象以避免冲突）\n",
    "    local_G = nx.Graph()\n",
    "    for u, v, weight in edges_list:\n",
    "        local_G.add_edge(u, v, weight=weight_dict.get((u, v), weight))\n",
    "    \n",
    "    local_flow = np.zeros(len(edge_id_map))\n",
    "    \n",
    "    for origin, destination, demand in od_chunk:\n",
    "        try:\n",
    "            # 执行 Dijkstra 寻找最短路径\n",
    "            path = nx.shortest_path(local_G, source=int(origin), target=int(destination), weight='weight')\n",
    "            \n",
    "            # 将需求加载到路径上的每一条边\n",
    "            for i in range(len(path) - 1):\n",
    "                u, v = path[i], path[i+1]\n",
    "                # 检查边在 edge_id_map 中的索引\n",
    "                edge_key = tuple(sorted((u, v))) # 处理无向图\n",
    "                if edge_key in edge_id_map:\n",
    "                    idx = edge_id_map[edge_key]\n",
    "                    local_flow[idx] += demand\n",
    "        except nx.NetworkXNoPath:\n",
    "            continue\n",
    "            \n",
    "    return local_flow\n",
    "\n",
    "# 2. 主执行函数\n",
    "def run_parallel_aon(edges_df, od_df, current_weights, num_workers=12):\n",
    "    \"\"\"\n",
    "    协调多进程执行全有全无分配\n",
    "    \"\"\"\n",
    "    print(f\"--- 启动并法化 AON 计算 (使用 {num_workers} 核心) ---\")\n",
    "    \n",
    "    # 准备边映射表: (u, v) -> index\n",
    "    edge_id_map = {}\n",
    "    edges_list = []\n",
    "    weight_dict = {}\n",
    "    for i, row in edges_df.iterrows():\n",
    "        u, v = int(row['u']), int(row['v'])\n",
    "        key = tuple(sorted((u, v)))\n",
    "        edge_id_map[key] = i\n",
    "        edges_list.append((u, v, current_weights[i]))\n",
    "        weight_dict[(u, v)] = current_weights[i]\n",
    "\n",
    "    # OD 任务切分\n",
    "    od_tasks = od_df[['origin', 'destination', 'demand_qij']].values\n",
    "    chunks = np.array_split(od_tasks, num_workers)\n",
    "    \n",
    "    # 并行池启动\n",
    "    # 固定核心组件，传入 partial 函数\n",
    "    worker_func = partial(worker_aon_assignment, edge_id_map=edge_id_map, \n",
    "                          edges_list=edges_list, weight_dict=weight_dict)\n",
    "    \n",
    "    with Pool(processes=num_workers) as pool:\n",
    "        results = pool.map(worker_func, chunks)\n",
    "    \n",
    "    # 结果聚合 (Reduction)\n",
    "    Y_auxiliary_flow = np.sum(results, axis=0)\n",
    "    \n",
    "    return Y_auxiliary_flow\n",
    "\n",
    "# 3. 示例测试与 snapshot 输出\n",
    "if __name__ == \"__main__\":\n",
    "    base_path = r\"D:\\PyCode\\论文复现与改进\\2025-D\\2507692\\论文复现与优化\\2025_Problem_D_Data\"\n",
    "    edges_file = os.path.join(base_path, \"edges_fully_aligned.csv\")\n",
    "    od_file = os.path.join(base_path, \"od_demand_matrix_calibrated.csv\")\n",
    "    snapshot_file = os.path.join(base_path, \"AON_debug_snapshot.txt\")\n",
    "\n",
    "    # 预载入数据\n",
    "    edges = pd.read_csv(edges_file)\n",
    "    od = pd.read_csv(od_file)\n",
    "    \n",
    "    # 模拟 Iteration 0 (使用自由流时间)\n",
    "    t0 = edges['free_flow_time'].values\n",
    "    \n",
    "    Y = run_parallel_aon(edges, od, t0, num_workers=12)\n",
    "    \n",
    "    # 结果审计\n",
    "    total_flow_assigned = np.sum(Y)\n",
    "    print(f\"--- Block 3 完成 ---\")\n",
    "    print(f\"全网总需求量: {od['demand_qij'].sum():,.2f}\")\n",
    "    print(f\"AON 已成功分配流量: {total_flow_assigned:,.2f}\")\n",
    "    \n",
    "    with open(snapshot_file, \"w\") as f:\n",
    "        f.write(f\"AON Snapshot\\nTotal Demand: {od['demand_qij'].sum()}\\nTotal Assigned: {total_flow_assigned}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e31489-fd82-43f2-9eb2-35ba95915bc9",
   "metadata": {},
   "source": [
    "# 考虑将上述计算交给大模型云端进行 我直接让其输出数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea91e36-8bcb-46cd-a579-fb822a6c5ddb",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "def generate_aon_results():\n",
    "    print(\"正在加载数据...\")\n",
    "    edges_df = pd.read_csv('edges_fully_aligned.csv')\n",
    "    od_df = pd.read_csv('od_demand_matrix_calibrated.csv')\n",
    "    \n",
    "    # 1. 构建无向图 (依据你的 original 代码逻辑)\n",
    "    G = nx.Graph()\n",
    "    edge_id_map = {}\n",
    "    current_weights = edges_df['free_flow_time'].values\n",
    "    \n",
    "    for i, row in edges_df.iterrows():\n",
    "        u, v = int(row['u']), int(row['v'])\n",
    "        key = tuple(sorted((u, v)))\n",
    "        edge_id_map[key] = i\n",
    "        G.add_edge(u, v, weight=current_weights[i])\n",
    "    \n",
    "    # 2. 初始化流量数组\n",
    "    y_flow = np.zeros(len(edges_df))\n",
    "    unique_origins = od_df['origin'].unique()\n",
    "    \n",
    "    print(f\"开始计算 AON 分配 (共 {len(unique_origins)} 个起点)...\")\n",
    "    \n",
    "    for count, origin in enumerate(unique_origins, 1):\n",
    "        origin = int(origin)\n",
    "        if not G.has_node(origin): continue\n",
    "            \n",
    "        # 使用 Dijkstra 计算该起点到所有节点的最短路径\n",
    "        paths = nx.single_source_dijkstra_path(G, origin, weight='weight')\n",
    "        \n",
    "        # 提取该起点的所有 OD 需求\n",
    "        origin_demands = od_df[od_df['origin'] == origin]\n",
    "        \n",
    "        for _, row in origin_demands.iterrows():\n",
    "            dest = int(row['destination'])\n",
    "            demand = row['demand_qij']\n",
    "            if dest in paths:\n",
    "                p = paths[dest]\n",
    "                for i in range(len(p) - 1):\n",
    "                    edge_key = tuple(sorted((p[i], p[i+1])))\n",
    "                    if edge_key in edge_id_map:\n",
    "                        y_flow[edge_id_map[edge_key]] += demand\n",
    "        \n",
    "        if count % 20 == 0:\n",
    "            print(f\"已完成: {count}/{len(unique_origins)}\")\n",
    "\n",
    "    # 3. 保存结果\n",
    "    edges_df['assigned_flow'] = y_flow\n",
    "    edges_df.to_csv('edges_with_aon_flow_LOCAL.csv', index=False)\n",
    "    \n",
    "    print(\"\\n--- 计算完成 ---\")\n",
    "    print(f\"结果已保存至: edges_with_aon_flow_LOCAL.csv\")\n",
    "    print(f\"总分配流量验证: {np.sum(y_flow):,.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_aon_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b22764d7-b233-4f1d-a908-835e67bc2882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 正在从以下路径读取数据:\n",
      "   D:\\PyCode\\论文复现与改进\\2025-D\\2507692\\论文复现与优化\\2025_Problem_D_Data\n",
      "2. 正在构建网络图 (边数: 89655)...\n",
      "3. 开始计算 AON 流量分配 (总起点数: 100)...\n",
      "   进度: 已完成 20/100 个起点的路径分配...\n",
      "   进度: 已完成 40/100 个起点的路径分配...\n",
      "   进度: 已完成 60/100 个起点的路径分配...\n",
      "   进度: 已完成 80/100 个起点的路径分配...\n",
      "   进度: 已完成 100/100 个起点的路径分配...\n",
      "4. 正在保存结果至: D:\\PyCode\\论文复现与改进\\2025-D\\2507692\\论文复现与优化\\2025_Problem_D_Data\\edges_with_aon_flow.csv\n",
      "\n",
      "==============================\n",
      "计算成功完成！\n",
      "总分配流量 (Sum of Flows): 120,465,331.26\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import os\n",
    "\n",
    "def generate_aon_results_absolute():\n",
    "    # ================= 配置绝对路径 =================\n",
    "    base_path = r\"D:\\PyCode\\论文复现与改进\\2025-D\\2507692\\论文复现与优化\\2025_Problem_D_Data\"\n",
    "    edges_file = os.path.join(base_path, \"edges_fully_aligned.csv\")\n",
    "    od_file = os.path.join(base_path, \"od_demand_matrix_calibrated.csv\")\n",
    "    output_file = os.path.join(base_path, \"edges_with_aon_flow.csv\")\n",
    "    # ===============================================\n",
    "\n",
    "    print(f\"1. 正在从以下路径读取数据:\\n   {base_path}\")\n",
    "    \n",
    "    if not os.path.exists(edges_file) or not os.path.exists(od_file):\n",
    "        print(\"错误：找不到指定的 CSV 文件，请检查路径是否正确。\")\n",
    "        return\n",
    "\n",
    "    edges_df = pd.read_csv(edges_file)\n",
    "    od_df = pd.read_csv(od_file)\n",
    "    \n",
    "    print(f\"2. 正在构建网络图 (边数: {len(edges_df)})...\")\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # 快速构建图：提取 u, v 和权重列\n",
    "    # 注意：这里使用 free_flow_time 作为初始权重\n",
    "    edge_data = zip(edges_df['u'].astype(int), \n",
    "                    edges_df['v'].astype(int), \n",
    "                    edges_df['free_flow_time'])\n",
    "    G.add_weighted_edges_from(edge_data)\n",
    "    \n",
    "    # 预先构建边键 (u, v) 到索引的映射，处理无向图逻辑\n",
    "    edge_keys = [tuple(sorted((int(u), int(v)))) for u, v in zip(edges_df['u'], edges_df['v'])]\n",
    "    edge_id_map = {key: i for i, key in enumerate(edge_keys)}\n",
    "    \n",
    "    # 3. 初始化流量数组\n",
    "    y_flow = np.zeros(len(edges_df))\n",
    "    \n",
    "    # 4. 按起点进行分组计算\n",
    "    unique_origins = od_df['origin'].unique()\n",
    "    print(f\"3. 开始计算 AON 流量分配 (总起点数: {len(unique_origins)})...\")\n",
    "    \n",
    "    od_groups = od_df.groupby('origin')\n",
    "    \n",
    "    for count, (origin, group) in enumerate(od_groups, 1):\n",
    "        origin_node = int(origin)\n",
    "        if not G.has_node(origin_node):\n",
    "            continue\n",
    "            \n",
    "        # 核心优化：一次性计算出该起点到全网所有可达节点的最短路径\n",
    "        # single_source_dijkstra_path 会返回一个字典：{终点: [路径节点列表]}\n",
    "        try:\n",
    "            paths = nx.single_source_dijkstra_path(G, origin_node, weight='weight')\n",
    "        except Exception:\n",
    "            continue\n",
    "        \n",
    "        # 遍历该起点下的所有目的地需求\n",
    "        for _, row in group.iterrows():\n",
    "            dest_node = int(row['destination'])\n",
    "            demand = row['demand_qij']\n",
    "            \n",
    "            if dest_node in paths:\n",
    "                p = paths[dest_node]\n",
    "                # 将流量加载到路径上的每一条边\n",
    "                for i in range(len(p) - 1):\n",
    "                    key = tuple(sorted((p[i], p[i+1])))\n",
    "                    if key in edge_id_map:\n",
    "                        idx = edge_id_map[key]\n",
    "                        y_flow[idx] += demand\n",
    "        \n",
    "        if count % 20 == 0:\n",
    "            print(f\"   进度: 已完成 {count}/{len(unique_origins)} 个起点的路径分配...\")\n",
    "\n",
    "    # 5. 合并结果并保存\n",
    "    print(f\"4. 正在保存结果至: {output_file}\")\n",
    "    edges_df['assigned_flow'] = y_flow\n",
    "    edges_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"计算成功完成！\")\n",
    "    print(f\"总分配流量 (Sum of Flows): {np.sum(y_flow):,.2f}\")\n",
    "    print(\"=\"*30)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_aon_results_absolute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1907dbb2-3536-4a82-98e6-dce2090e50ff",
   "metadata": {},
   "source": [
    "# 2025 MCM Problem D 实战指南\n",
    "## 第二阶段：Block 3 复盘与技术性能分析 (Stage 2: Block 3 Review)\n",
    "\n",
    "在完成 **Block 3 (AON Assignment)** 任务后，我们对生成的 `edges_with_aon_flow.csv` 进行了系统性审计，并针对计算过程中的性能表现进行了深度分析。\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 数据集检查结果 (Dataset Audit)\n",
    "\n",
    "通过对输出文件的初步分析，确认 AON 分配阶段的计算目标已达成：\n",
    "\n",
    "* **字段完整性**：文件中已成功增加了 `assigned_flow` 字段。\n",
    "* **分配特征分布**：\n",
    "    * **覆盖率**：在全网 89,655 条边中，共有 2,964 条边的流量大于 0（约占总体的 **3.3%**）。这符合 AON 分配的数学特性，即流量会严格集中在理论最短路径上，导致网络覆盖呈现高度稀疏性。\n",
    "    * **流量规模**：流量最大值达到了 **1,106,741**，平均流量约为 1,343。\n",
    "* **结论**：流量已成功根据最短路径逻辑注入路网，为后续 **Block 4 (UE 迭代)** 提供了必要的起始流量向量。\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 2. 计算效率与算法优化思考 (Algorithmic Analysis)\n",
    "\n",
    "针对大规模图计算（8.9 万条边，5,000 对 OD）中可能出现的“计算缓慢”问题，我们从以下维度进行了技术复盘：\n",
    "\n",
    "### A. 算法复杂度优化 (Complexity Optimization)\n",
    "全有全无分配的核心是路径搜索。传统的针对每一个 OD 对调用一次 Dijkstra 算法，其计算量为 $O(N_{OD} \\times E \\log V)$。\n",
    "\n",
    "* **优化策略 (SSSP)**：应采用 **单源最短路径 (Single Source Shortest Path)** 逻辑。针对每一个“起点（Origin）”，计算其到所有“终点（Destinations）”的一棵最短路径树。\n",
    "    * **收益**：计算次数从“OD 对数量”减少到了“起点数量”，复杂度降至 $O(N_{Origin} \\times E \\log V)$。\n",
    "* **数据结构优化**：NetworkX 作为纯 Python 实现，在处理大规模对象时存在内存开销。\n",
    "    * **建议**：在高性能场景下，可考虑更换基于 C++/Rust 实现的底层引擎（如 `igraph` 或 `graph-tool`）。\n",
    "\n",
    "---\n",
    "\n",
    "## 3. CPU 调用率瓶颈分析 (Hardware Utilization)\n",
    "\n",
    "在计算过程中观察到 CPU 调用率偏低（10%-15%），其深层原因主要源于以下物理限制：\n",
    "\n",
    "### B. 性能瓶颈根源\n",
    "1.  **Python GIL (Global Interpreter Lock)**：这是最核心的原因。标准的 Python (CPython) 执行过程中，GIL 确保同一时刻只有一个 CPU 核心在执行字节码。即便拥有多核硬件，单线程程序也无法实现真正的物理并行。\n",
    "\n",
    "2.  **I/O 与内存阻塞**：计算过程中的频繁磁盘读写（I/O Wait）以及由于数据量庞大触发的垃圾回收 (GC) 或虚拟内存交换 (Swapping)，会导致 CPU 进入“空等”状态。\n",
    "3.  **并行化缺失**：交通分配是典型的 **“尴尬并行” (Embarrassingly Parallel)** 任务。\n",
    "\n",
    "### C. 优化路径：多进程并发\n",
    "为了突破性能上限，必须引入 `multiprocessing` 模块：\n",
    "* **实现逻辑**：将起点列表 (Origins) 拆分为多个分片 (Chunks)，分配给不同的进程执行。\n",
    "* **预期效果**：这将瞬间打破 GIL 限制，使 CPU 调用率提升至 **100%**，极大加速 Block 4 阶段的高频率迭代过程。\n",
    "\n",
    "---\n",
    "\n",
    "## 4. 总结与下一步建议\n",
    "\n",
    "目前 Block 3 已经建立了一个稳健的流量底座。但由于 **用户均衡分配 (User Equilibrium)** 需要进行多次 AON 迭代，目前的单线程效率将成为大规模仿真的瓶颈。\n",
    "\n",
    "**下一步您可以：**\n",
    "* **引入多进程并行化**：在 Block 4 的迭代引擎中部署并发搜索逻辑。\n",
    "* **执行大桥倒塌仿真**：移除 `G.remove_edge()` 对应的边，并在残损网络上重新运行该分配逻辑。\n",
    "* **计算系统延时**：量化灾前与灾后全网总旅行时间 ($TSTT$) 的变动量。\n",
    "\n",
    "**您是否需要我为您提供一份关于“如何在 Python 中利用 `multiprocessing` 加速交通分配”的参考代码块？**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93c7d2b1-688f-41b9-b53e-8013e8afc3ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'try' statement on line 139 (2159636593.py, line 140)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[6], line 140\u001b[1;36m\u001b[0m\n\u001b[1;33m    edges.to_csv(output_path, index=False)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after 'try' statement on line 139\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.csgraph import dijkstra\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "# ==========================================\n",
    "# 1. 配置与路径设定\n",
    "# ==========================================\n",
    "DATA_PATH = r\"D:\\PyCode\\论文复现与改进\\2025-D\\2507692\\论文复现与优化\\2025_Problem_D_Data\"\n",
    "\n",
    "ALPHA = 0.15\n",
    "BETA = 4\n",
    "MAX_ITER = 500  # 建议增加到 30 次以观察收敛趋势\n",
    "TOLERANCE = 1e-4\n",
    "\n",
    "# ==========================================\n",
    "# 2. 核心函数\n",
    "# ==========================================\n",
    "\n",
    "def bpr_time(t0, capacity, flow):\n",
    "    return t0 * (1 + ALPHA * np.power((flow / capacity), BETA))\n",
    "\n",
    "def beckmann_obj(alpha, current_flow, target_flow, t0, capacity):\n",
    "    # 计算当前混合流量：x = x_old + alpha * (y - x_old)\n",
    "    x = current_flow + alpha * (target_flow - current_flow)\n",
    "    # Beckmann 目标函数：各路段 BPR 积分之和\n",
    "    val = np.sum(t0 * (x + (ALPHA / (BETA + 1)) * (np.power(x, BETA + 1) / np.power(capacity, BETA))))\n",
    "    return val\n",
    "\n",
    "def run_aon_step(od_grouped, n_nodes, graph_data, edge_map):\n",
    "    \"\"\"\n",
    "    全有全无分配 (AON) 核心逻辑\n",
    "    \"\"\"\n",
    "    row, col, weights = graph_data\n",
    "    graph = csr_matrix((weights, (row, col)), shape=(n_nodes, n_nodes))\n",
    "    target_flow = np.zeros(len(weights))\n",
    "    \n",
    "    # 针对每个唯一起点进行 SSSP\n",
    "    for o_idx, group in od_grouped:\n",
    "        dist, predecessors = dijkstra(graph, indices=o_idx, return_predecessors=True)\n",
    "        \n",
    "        d_indices = group['d_idx'].values\n",
    "        demands = group['demand_qij'].values\n",
    "        \n",
    "        for d_idx, demand in zip(d_indices, demands):\n",
    "            curr = d_idx\n",
    "            # 回溯路径\n",
    "            while curr != o_idx and curr != -9999:\n",
    "                prev = predecessors[curr]\n",
    "                if prev == -9999: break\n",
    "                \n",
    "                # 快速定位边索引\n",
    "                if (prev, curr) in edge_map:\n",
    "                    target_flow[edge_map[(prev, curr)]] += demand\n",
    "                curr = prev\n",
    "    return target_flow\n",
    "\n",
    "# ==========================================\n",
    "# 3. 主程序\n",
    "# ==========================================\n",
    "\n",
    "def run_ue_assignment():\n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] 加载数据中...\")\n",
    "    \n",
    "    edges = pd.read_csv(os.path.join(DATA_PATH, 'edges_with_aon_flow.csv'))\n",
    "    od = pd.read_csv(os.path.join(DATA_PATH, 'od_demand_matrix_calibrated.csv'))\n",
    "\n",
    "    # 节点与边映射\n",
    "    all_nodes = np.unique(np.concatenate([edges['u'], edges['v'], od['origin'], od['destination']]))\n",
    "    node_to_idx = {node: i for i, node in enumerate(all_nodes)}\n",
    "    n_nodes = len(all_nodes)\n",
    "    \n",
    "    edges['u_idx'] = edges['u'].map(node_to_idx)\n",
    "    edges['v_idx'] = edges['v'].map(node_to_idx)\n",
    "    od['o_idx'] = od['origin'].map(node_to_idx)\n",
    "    od['d_idx'] = od['destination'].map(node_to_idx)\n",
    "    \n",
    "    edge_map = {(r.u_idx, r.v_idx): i for i, r in edges.iterrows()}\n",
    "    t0 = edges['free_flow_time'].values\n",
    "    capacity = edges['capacity'].values\n",
    "    od_grouped = list(od.groupby('o_idx'))\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # Step 0: 初始化 (第一次 AON 分配)\n",
    "    # ------------------------------------------\n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] Step 0: 执行初始 AON 分配 (基于自由流时间)...\")\n",
    "    graph_data_0 = (edges['u_idx'].values, edges['v_idx'].values, t0)\n",
    "    current_flow = run_aon_step(od_grouped, n_nodes, graph_data_0, edge_map)\n",
    "    \n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] 初始分配完成。开始 Frank-Wolfe 迭代...\")\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # 迭代计算\n",
    "    # ------------------------------------------\n",
    "    for i in range(MAX_ITER):\n",
    "        iter_start = time.time()\n",
    "        \n",
    "        # 1. 更新当前阻抗\n",
    "        current_weights = bpr_time(t0, capacity, current_flow)\n",
    "        \n",
    "        # 2. 寻找下降方向 (再次 AON)\n",
    "        graph_data = (edges['u_idx'].values, edges['v_idx'].values, current_weights)\n",
    "        target_flow = run_aon_step(od_grouped, n_nodes, graph_data, edge_map)\n",
    "        \n",
    "        # 3. 一维搜索寻找最佳步长 alpha\n",
    "        # 搜索范围为 [0, 1]\n",
    "        res = minimize_scalar(beckmann_obj, bounds=(0, 1), \n",
    "                              args=(current_flow, target_flow, t0, capacity), \n",
    "                              method='bounded')\n",
    "        alpha = res.x\n",
    "        \n",
    "        # 4. 更新流量\n",
    "        new_flow = current_flow + alpha * (target_flow - current_flow)\n",
    "        \n",
    "        # 5. 计算收敛指标 (Relative Gap)\n",
    "        # Gap = sum(cost * (target - current)) / sum(cost * current)\n",
    "        num = np.sum(current_weights * (target_flow - current_flow))\n",
    "        den = np.sum(current_weights * current_flow)\n",
    "        gap = abs(num / den) if den != 0 else 1\n",
    "        \n",
    "        duration = time.time() - iter_start\n",
    "        print(f\"迭代 {i+1:02d} | 步长: {alpha:.4f} | Gap: {gap:.6e} | 耗时: {duration:.1f}s\")\n",
    "        \n",
    "        current_flow = new_flow\n",
    "        \n",
    "        if gap < TOLERANCE:\n",
    "            print(f\"[{time.strftime('%H:%M:%S')}] 满足收敛精度 ({TOLERANCE})。\")\n",
    "            break\n",
    "\n",
    "    # 保存\n",
    "    edges['ue_flow'] = current_flow\n",
    "    edges['ue_travel_time'] = bpr_time(t0, capacity, current_flow)\n",
    "    edges['v_c_ratio'] = current_flow / capacity\n",
    "    \n",
    "    output_path = os.path.join(DATA_PATH, 'edges_with_ue_flow.csv')\n",
    "    try:\n",
    "        edges.to_csv(output_path, index=False)\n",
    "        print(f\"成功保存至: {output_path}\")\n",
    "    except PermissionError:\n",
    "        # 如果文件被 Excel 占用，自动加上时间戳保存，防止数据丢失\n",
    "        timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        alt_path = output_path.replace(\".csv\", f\"_{timestamp}.csv\")\n",
    "        edges.to_csv(alt_path, index=False)\n",
    "        print(f\"警告：原文件被占用，结果已另存为: {alt_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_ue_assignment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0309d4-24a2-40d9-af03-ff6bf88e76ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
