{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "186e6574-22a1-4a3b-9beb-de96a55ed08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… [Project BioShield] æ•°æ®é›†ç”Ÿæˆå®Œæ¯• (Scale: 1000 Nodes, 7 Days)\n",
      "   Peak Daily Waste: 13914.0\n",
      "   Total Hub Cap   : 35424 (Safe)\n",
      "   Total Plant Cap : 18430 (Tight but Feasible)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "def generate_bioshield_data():\n",
    "    np.random.seed(2026)\n",
    "    \n",
    "    # === 1. å‚æ•°è®¾å®š (Scale & Time) ===\n",
    "    NUM_CLINICS = 1000     # è§„æ¨¡ï¼šå¤§\n",
    "    NUM_HUBS = 20          # å€™é€‰æ¢çº½\n",
    "    NUM_PLANTS = 5         # æœ€ç»ˆå¤„ç†å‚\n",
    "    DAYS = 7               # åŠ¨æ€æ—¶é—´è½´\n",
    "    \n",
    "    # è®¾å®šä¸­å¿ƒç‚¹ (City Centers) ç”¨äºç”Ÿæˆèšç±»åˆ†å¸ƒ\n",
    "    centers = [(20,20), (80,80), (20,80), (80,20), (50,50)]\n",
    "    \n",
    "    # === 2. ç”Ÿæˆè¯Šæ‰€ (Clinics - Sources) ===\n",
    "    # ä½¿ç”¨é«˜æ–¯æ··åˆç”Ÿæˆèšç±»åˆ†å¸ƒ (æ¨¡æ‹ŸåŸå¸‚äººå£èšé›†)\n",
    "    coords, cluster_ids = make_blobs(n_samples=NUM_CLINICS, centers=centers, cluster_std=10, random_state=42)\n",
    "    # é™åˆ¶åæ ‡åœ¨ 0-100\n",
    "    coords = np.clip(coords, 0, 100)\n",
    "    \n",
    "    df_clinics = pd.DataFrame({\n",
    "        'Clinic_ID': [f\"C{i+1:04d}\" for i in range(NUM_CLINICS)],\n",
    "        'X': coords[:, 0],\n",
    "        'Y': coords[:, 1],\n",
    "        'Max_Storage': np.random.randint(50, 150, NUM_CLINICS) # åº“å®¹ä¸Šé™\n",
    "    })\n",
    "    \n",
    "    # === 3. ç”Ÿæˆ 7 å¤©çš„åŠ¨æ€åºŸç‰©äº§ç”Ÿé‡ (Time Series) ===\n",
    "    # æ¨¡æ‹Ÿâ€œçˆ†å‘æœŸâ€ï¼šç¬¬3-5å¤©äº§ç”Ÿé‡æ¿€å¢\n",
    "    waste_data = []\n",
    "    for t in range(1, DAYS + 1):\n",
    "        surge_factor = 1.5 if 3 <= t <= 5 else 1.0\n",
    "        # åŸºç¡€äº§é‡ + éšæœºæ³¢åŠ¨ + æ¿€å¢å› å­\n",
    "        daily_waste = np.random.randint(5, 15, NUM_CLINICS) * surge_factor * np.random.uniform(0.8, 1.2, NUM_CLINICS)\n",
    "        \n",
    "        tmp_df = df_clinics[['Clinic_ID']].copy()\n",
    "        tmp_df['Day'] = t\n",
    "        tmp_df['New_Waste'] = daily_waste.astype(int)\n",
    "        waste_data.append(tmp_df)\n",
    "    \n",
    "    df_waste_ts = pd.concat(waste_data)\n",
    "    \n",
    "    # === 4. ç”Ÿæˆä¸­è½¬ç«™ (Hubs) ===\n",
    "    # Hubs ä½äºèšç±»ä¸­å¿ƒé™„è¿‘\n",
    "    hub_coords = np.array(centers * 4) + np.random.normal(0, 5, (20, 2)) # 20ä¸ªHub\n",
    "    hub_coords = np.clip(hub_coords, 0, 100)\n",
    "    \n",
    "    # [God-Mode Audit]: ç¡®ä¿ Hub æ€»å®¹é‡ > æœ€å¤§æ—¥äº§åºŸé‡çš„ 2å€ (å†—ä½™è®¾è®¡)\n",
    "    max_daily_waste = df_waste_ts.groupby('Day')['New_Waste'].sum().max()\n",
    "    req_hub_cap = (max_daily_waste * 2.0) / NUM_HUBS\n",
    "    \n",
    "    df_hubs = pd.DataFrame({\n",
    "        'Hub_ID': [f\"H{i+1:02d}\" for i in range(NUM_HUBS)],\n",
    "        'X': hub_coords[:, 0],\n",
    "        'Y': hub_coords[:, 1],\n",
    "        'Capacity': np.random.randint(int(req_hub_cap), int(req_hub_cap*1.5), NUM_HUBS),\n",
    "        'Fixed_Cost': np.random.randint(500, 1000, NUM_HUBS)\n",
    "    })\n",
    "    \n",
    "    # === 5. ç”Ÿæˆç„šçƒ§å‚ (Plants) ===\n",
    "    # ä½äºåœ°å›¾è¾¹ç¼˜\n",
    "    plant_coords = [(0,0), (100,100), (0,100), (100,0), (50,100)]\n",
    "    \n",
    "    # [God-Mode Audit]: ç¡®ä¿ Plant æ€»å®¹é‡ > æœ€å¤§æ—¥äº§åºŸé‡çš„ 1.2å€ (ç¨æ˜¾ç´§å¼ ï¼Œæµ‹è¯•ç®—æ³•)\n",
    "    req_plant_cap = (max_daily_waste * 1.2) / NUM_PLANTS\n",
    "    \n",
    "    df_plants = pd.DataFrame({\n",
    "        'Plant_ID': [f\"P{i+1:02d}\" for i in range(NUM_PLANTS)],\n",
    "        'X': [p[0] for p in plant_coords],\n",
    "        'Y': [p[1] for p in plant_coords],\n",
    "        'Daily_Capacity': np.random.randint(int(req_plant_cap), int(req_plant_cap*1.2), NUM_PLANTS)\n",
    "    })\n",
    "    \n",
    "    # ä¿å­˜\n",
    "    df_clinics.to_csv('Clinics.csv', index=False)\n",
    "    df_hubs.to_csv('Hubs.csv', index=False)\n",
    "    df_plants.to_csv('Plants.csv', index=False)\n",
    "    df_waste_ts.to_csv('Waste_TimeSeries.csv', index=False)\n",
    "    \n",
    "    print(f\"âœ… [Project BioShield] æ•°æ®é›†ç”Ÿæˆå®Œæ¯• (Scale: 1000 Nodes, 7 Days)\")\n",
    "    print(f\"   Peak Daily Waste: {max_daily_waste:.1f}\")\n",
    "    print(f\"   Total Hub Cap   : {df_hubs['Capacity'].sum()} (Safe)\")\n",
    "    print(f\"   Total Plant Cap : {df_plants['Daily_Capacity'].sum()} (Tight but Feasible)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_bioshield_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccf942bb-13df-4760-aae0-7b880535a9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pulp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import warnings\n",
    "import shutil \n",
    "\n",
    "# --- å…¨å±€ç»˜å›¾é…ç½® (Global Plotting Config) ---\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.rcParams['font.sans-serif'] = ['Arial', 'SimHei']  # å…¼å®¹ä¸­è‹±æ–‡æ˜¾ç¤º\n",
    "\n",
    "plt.rcParams['axes.unicode_minus'] = False             # è§£å†³è´Ÿå·æ˜¾ç¤ºä¸ºæ–¹å—çš„é—®é¢˜\n",
    "try:\n",
    "    import scienceplots\n",
    "    plt.style.use(['science', 'no-latex'])             # å°è¯•åŠ è½½ç§‘ç ”ç»˜å›¾é£æ ¼\n",
    "except ImportError:\n",
    "    sns.set_theme(style=\"whitegrid\", palette=\"deep\", context=\"paper\") # é™çº§æ–¹æ¡ˆ\n",
    "\n",
    "class IP_Solver_Capsule:\n",
    "    \"\"\"\n",
    "    [MCM IP Solver V7.3 - Platinum Documentation]\n",
    "    \n",
    "    ä¸€ä¸ªé«˜åº¦å°è£…çš„æ··åˆæ•´æ•°è§„åˆ’ (MIP) æ±‚è§£å™¨æ¡†æ¶ï¼Œä¸“ä¸ºæ•°å­¦å»ºæ¨¡ç«èµ›è®¾è®¡ã€‚\n",
    "    é›†æˆäº†å»ºæ¨¡ã€æ±‚è§£ã€é²æ£’æ€§åˆ†æã€å¯è§†åŒ–å’Œè‡ªåŠ¨å½’æ¡£åŠŸèƒ½ã€‚\n",
    "    \n",
    "    Attributes:\n",
    "        prob (pulp.LpProblem): åº•å±‚ PuLP é—®é¢˜å¯¹è±¡\n",
    "        matrix_vars (dict): å­˜å‚¨æ‰€æœ‰çŸ©é˜µ/å­—å…¸å˜é‡çš„å¼•ç”¨\n",
    "        binary_vars (list): è‡ªåŠ¨è¿½è¸ªæ‰€æœ‰äºŒå…ƒå˜é‡ï¼Œç”¨äºç¨³å®šæ€§åˆ†æ\n",
    "        output_dir (str): ç»“æœè¾“å‡ºçš„æ ¹ç›®å½•\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name=\"MIP_Model\", sense='max'):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–æ±‚è§£å™¨ç¯å¢ƒã€‚\n",
    "        :param name: æ¨¡å‹åç§° (ç”¨äºç”Ÿæˆæ–‡ä»¶å¤¹å’Œæ—¥å¿—)\n",
    "        :param sense: ä¼˜åŒ–æ–¹å‘ ('max' æˆ– 'min')\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.timestamp = int(time.time())\n",
    "        self.sense = pulp.LpMaximize if sense.lower() == 'max' else pulp.LpMinimize\n",
    "        \n",
    "        # åˆå§‹åŒ– PuLP é—®é¢˜å¯¹è±¡\n",
    "        self.prob = pulp.LpProblem(self.name, self.sense)\n",
    "        \n",
    "        # å†…éƒ¨å˜é‡ä»“åº“\n",
    "        self.matrix_vars = {}   # {name: var_dict}\n",
    "        self.binary_vars = []   # [var_obj, ...]\n",
    "        self.aux_vars = {}      # è¾…åŠ©å˜é‡ (å¦‚ TSP çš„ u å˜é‡)\n",
    "        \n",
    "        # åˆ›å»ºè¾“å‡ºç›®å½•\n",
    "        self.output_dir = f\"./Results_{name}_{self.timestamp}\"\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "\n",
    "    # ======================================================\n",
    "    # Phase 0: æ¡æ‰‹ (Handshake)\n",
    "    # ======================================================\n",
    "    def generate_handshake(self):\n",
    "        \"\"\"ç”Ÿæˆæ¡æ‰‹åè®® Promptï¼Œç”¨äºå¼•å¯¼ AI åŠ©æ‰‹è¿›å…¥ä¸Šä¸‹æ–‡\"\"\"\n",
    "        print(f\"\\nğŸ¤ === å¤åˆ¶ä»¥ä¸‹ Prompt å‘é€ç»™ AI (V7.3 IP) ===\\n\")\n",
    "        print(f\"ã€ç³»ç»Ÿè®¾å®šã€‘\\næˆ‘æ­£åœ¨ä½¿ç”¨ `IP_Solver_Capsule` (V7.3 - Documented)ã€‚\")\n",
    "        print(f\"ç›®æ ‡: {'Maximize' if self.sense == -1 else 'Minimize'} | è¾“å‡º: `{self.output_dir}`\")\n",
    "        print(\"\\nã€API æ¥å£æ¸…å•ã€‘\")\n",
    "        print(\"1. å˜é‡å®šä¹‰: add_var_matrix(rows, cols) / add_vars_dict(keys)\")\n",
    "        print(\"2. çº¦æŸæ„å»º: add_logic_constraint(...) / add_mandatory_assignment(...)\")\n",
    "        print(\"3. æ±‚è§£æ‰§è¡Œ: solve(time_limit=300)\")\n",
    "        print(\"4. æ·±åº¦åˆ†æ: analyze_relaxation_gap() / analyze_binary_stability()\")\n",
    "        print(\"5. ç»“æœå¯è§†åŒ–: visualize_routing(...)\")\n",
    "        print(\"6. å…¨é‡å½’æ¡£: archive_project(dataframes, figures)\")\n",
    "\n",
    "    # ======================================================\n",
    "    # Phase 1: å»ºæ¨¡å·¥å‚ (Modeling Factory)\n",
    "    # ======================================================\n",
    "    def add_var_matrix(self, rows, cols, name=\"x\", cat='Binary'):\n",
    "        \"\"\"\n",
    "        åˆ›å»ºä¸€ä¸ªäºŒç»´å˜é‡çŸ©é˜µ (Nested Dictionary)ã€‚\n",
    "        :param rows: è¡Œç´¢å¼•åˆ—è¡¨ (å¦‚ Site IDs)\n",
    "        :param cols: åˆ—ç´¢å¼•åˆ—è¡¨ (å¦‚ Demand IDs)\n",
    "        :param cat: å˜é‡ç±»å‹ ('Binary', 'Integer', 'Continuous')\n",
    "        :return: å˜é‡å­—å…¸ x[row][col]\n",
    "        \"\"\"\n",
    "        vars_dict = pulp.LpVariable.dicts(name, (rows, cols), 0, 1 if cat=='Binary' else None, cat)\n",
    "        self.matrix_vars[name] = vars_dict\n",
    "        \n",
    "        # è‡ªåŠ¨æ³¨å†Œ Binary å˜é‡ç”¨äºåç»­åˆ†æ\n",
    "        if cat == 'Binary':\n",
    "            for r in rows:\n",
    "                for c in cols:\n",
    "                    self.binary_vars.append(vars_dict[r][c])\n",
    "        print(f\"âœ… çŸ©é˜µå˜é‡å·²åˆ›å»º: {name}[{len(rows)}x{len(cols)}] (Type: {cat})\")\n",
    "        return vars_dict\n",
    "\n",
    "    def add_vars_dict(self, keys, name, cat, lowBound=None, upBound=None):\n",
    "        \"\"\"\n",
    "        åˆ›å»ºä¸€ä¸ªä¸€ç»´å­—å…¸å˜é‡ (Flat Dictionary)ã€‚\n",
    "        :param keys: ç´¢å¼•åˆ—è¡¨ (å¯ä»¥æ˜¯ Tuple åˆ—è¡¨ï¼Œå¦‚ [(d1,s1), (d2,s2)])\n",
    "        :return: å˜é‡å­—å…¸ x[key]\n",
    "        \"\"\"\n",
    "        vars_dict = pulp.LpVariable.dicts(name, keys, lowBound=lowBound, upBound=upBound, cat=cat)\n",
    "        self.matrix_vars[name] = vars_dict\n",
    "        \n",
    "        if cat == 'Binary':\n",
    "            for k in keys:\n",
    "                self.binary_vars.append(vars_dict[k])\n",
    "        \n",
    "        print(f\"âœ… å­—å…¸å˜é‡å·²åˆ›å»º: {name} [Size: {len(keys)}] (Type: {cat})\")\n",
    "        return vars_dict\n",
    "\n",
    "    def add_logic_constraint(self, bin_var, target_var, logic_type='active_if_1', M=1e5):\n",
    "        \"\"\"\n",
    "        æ·»åŠ  Big-M é€»è¾‘çº¦æŸã€‚\n",
    "        :param bin_var: æ§åˆ¶å˜é‡ (0/1)\n",
    "        :param target_var: å—æ§å˜é‡ (Continuous/Integer)\n",
    "        :param logic_type: \n",
    "            - 'active_if_1': è‹¥ bin=0ï¼Œåˆ™ target å¿…é¡»ä¸º 0 (target <= M * bin)\n",
    "            - 'forced_cost': è‹¥ bin=1ï¼Œåˆ™ target å¿…é¡» > 0 (target >= bin) [ç®€æ˜“ç‰ˆ]\n",
    "            - 'binary_implication': è‹¥ bin=1ï¼Œåˆ™ target å¿…é¡»ä¸º 1 (bin <= target)\n",
    "        \"\"\"\n",
    "        idx = len(self.prob.constraints)\n",
    "        if logic_type == 'active_if_1': \n",
    "            self.prob += (target_var <= M * bin_var), f\"Logic_Active_{idx}\"\n",
    "        elif logic_type == 'forced_cost': \n",
    "            self.prob += (target_var >= M * bin_var), f\"Logic_FixedCost_{idx}\"\n",
    "        elif logic_type == 'binary_implication':\n",
    "            self.prob += (bin_var <= target_var), f\"Logic_Imp_{idx}\"\n",
    "        else:\n",
    "            print(f\"âš ï¸ æœªçŸ¥é€»è¾‘ç±»å‹: {logic_type}\")\n",
    "\n",
    "    def add_mandatory_assignment(self, demand_ids, site_ids, assignment_var_dict, name_prefix=\"MustServe\"):\n",
    "        \"\"\"\n",
    "        [é²æ£’æ€§å¢å¼º] æ·»åŠ å¼ºåˆ¶æŒ‡æ´¾çº¦æŸï¼šsum(y_ij) == 1ã€‚\n",
    "        è§£å†³ 'Lazy Solver' é—®é¢˜ï¼ˆå³æ±‚è§£å™¨ä¸ºäº†çœé’±å®Œå…¨ä¸å»ºç«™ï¼‰ã€‚\n",
    "        \n",
    "        * ç‰¹æ€§: è‡ªåŠ¨è¯†åˆ«åµŒå¥—å­—å…¸ (x[i][j]) æˆ– æ‰å¹³å­—å…¸ (x[(i,j)])\n",
    "        \"\"\"\n",
    "        count = 0\n",
    "        for d in demand_ids:\n",
    "            relevant_vars = []\n",
    "            for s in site_ids:\n",
    "                # å…¼å®¹æ€§æ£€æŸ¥: å°è¯•ä¸¤ç§å¸¸è§çš„å˜é‡è®¿é—®æ–¹å¼\n",
    "                try:\n",
    "                    # å°è¯• Flat Key: x[(d,s)]\n",
    "                    v = assignment_var_dict.get((d, s))\n",
    "                    if v is None:\n",
    "                         # å°è¯• Nested Key: x[d][s]\n",
    "                         if d in assignment_var_dict and s in assignment_var_dict[d]:\n",
    "                             v = assignment_var_dict[d][s]\n",
    "                    \n",
    "                    if v is not None:\n",
    "                        relevant_vars.append(v)\n",
    "                except: pass\n",
    "            \n",
    "            if relevant_vars:\n",
    "                self.prob += (pulp.lpSum(relevant_vars) == 1, f\"{name_prefix}_{d}\")\n",
    "                count += 1\n",
    "        print(f\"âœ… å·²æ·»åŠ å¼ºåˆ¶æŒ‡æ´¾çº¦æŸ: è¦†ç›– {count} ä¸ªéœ€æ±‚èŠ‚ç‚¹\")\n",
    "\n",
    "    def add_TSP_subtour_elimination(self, x_vars, cities):\n",
    "        \"\"\"\n",
    "        è‡ªåŠ¨ç”Ÿæˆ TSP å­å›è·¯æ¶ˆé™¤çº¦æŸ (MTZ Formulation)ã€‚\n",
    "        u_i - u_j + n * x_ij <= n - 1\n",
    "        \"\"\"\n",
    "        n = len(cities)\n",
    "        # åˆ›å»ºè¾…åŠ©å˜é‡ u_i (ä»£è¡¨è®¿é—®æ¬¡åº)\n",
    "        u_vars = pulp.LpVariable.dicts(\"u\", cities, lowBound=0, upBound=n, cat='Continuous')\n",
    "        self.aux_vars['u_tsp'] = u_vars\n",
    "        \n",
    "        count = 0\n",
    "        for i in cities:\n",
    "            if i == cities[0]: continue # èµ·ç‚¹ä¸éœ€è¦çº¦æŸ\n",
    "            for j in cities:\n",
    "                if j == cities[0] or i == j: continue\n",
    "                try:\n",
    "                    # è¯»å– x_ij å˜é‡\n",
    "                    x_val = None\n",
    "                    if i in x_vars and j in x_vars[i]: x_val = x_vars[i][j] # Nested\n",
    "                    elif (i,j) in x_vars: x_val = x_vars[(i,j)] # Flat\n",
    "                    \n",
    "                    if x_val is not None:\n",
    "                        self.prob += (u_vars[i] - u_vars[j] + n * x_val <= n - 1), f\"Subtour_{i}_{j}\"\n",
    "                        count += 1\n",
    "                except: pass\n",
    "        print(f\"âœ… å·²è‡ªåŠ¨ç”Ÿæˆ TSP MTZ çº¦æŸ: {count} æ¡\")\n",
    "\n",
    "    # ======================================================\n",
    "    # Phase 2: æ±‚è§£ (Solver)\n",
    "    # ======================================================\n",
    "    def solve(self, solver_name='CBC', time_limit=300, gap_rel=0.05):\n",
    "        \"\"\"\n",
    "        æ‰§è¡Œæ±‚è§£ã€‚\n",
    "        :param time_limit: æœ€å¤§è¿è¡Œæ—¶é—´ (ç§’)\n",
    "        :param gap_rel: ç›¸å¯¹ Gap å®¹å¿åº¦ (0.05 = 5%)\n",
    "        \"\"\"\n",
    "        print(f\"\\nğŸš€ å¯åŠ¨æ±‚è§£å™¨ ({solver_name})...\")\n",
    "        # msg=0 å…³é—­åº•å±‚æ±‚è§£å™¨åˆ·å±ï¼Œä½¿ç”¨ clean output\n",
    "        solver = pulp.PULP_CBC_CMD(timeLimit=time_limit, gapRel=gap_rel, msg=0)\n",
    "        try: status = self.prob.solve(solver)\n",
    "        except Exception as e: print(f\"âŒ æ±‚è§£å™¨é”™è¯¯: {e}\"); return\n",
    "        \n",
    "        status_str = pulp.LpStatus[status]\n",
    "        print(f\"ğŸ“‹ æ±‚è§£çŠ¶æ€: {status_str}\")\n",
    "        if status_str in ['Infeasible', 'Unbounded']:\n",
    "            print(\"ğŸš¨ è­¦å‘Š: æ¨¡å‹æ— è§£! å»ºè®®æ£€æŸ¥äº’æ–¥çº¦æŸæˆ– Big-M ç³»æ•°ã€‚\")\n",
    "        else:\n",
    "            print(f\"ğŸ’ æœ€ä¼˜ç›®æ ‡å€¼: {pulp.value(self.prob.objective)}\")\n",
    "\n",
    "    # ======================================================\n",
    "    # Phase 3: æ·±åº¦åˆ†æ (Deep Analysis)\n",
    "    # ======================================================\n",
    "    def analyze_relaxation_gap(self):\n",
    "        \"\"\"\n",
    "        [MIP ç‰¹æ€§] è®¡ç®—æ•´æ•°ä»£ä»·ã€‚\n",
    "        å±•ç¤º 'è¦æ±‚å˜é‡å¿…é¡»æ˜¯æ•´æ•°' è¿™ä¸€é™åˆ¶å¯¼è‡´æˆæœ¬å¢åŠ äº†å¤šå°‘ã€‚\n",
    "        \"\"\"\n",
    "        print(f\"\\nğŸ§© å¯åŠ¨æ•´æ•°ä»£ä»·åˆ†æ (Relaxation Gap)...\")\n",
    "        if self.prob.status != 1: return\n",
    "        ip_obj = pulp.value(self.prob.objective)\n",
    "        print(f\"   -> IP (æ•´æ•°) ç›®æ ‡å€¼: {ip_obj}\")\n",
    "        print(\"   -> (æ³¨: ä¸ºä¿æŠ¤æ¨¡å‹çŠ¶æ€ï¼Œè¯·æ‰‹åŠ¨ç§»é™¤ Integer å±æ€§åå¯¹æ¯” LP è§£)\")\n",
    "\n",
    "    def analyze_binary_stability(self, perturb_range=0.1, runs=10):\n",
    "        \"\"\"\n",
    "        [å†³ç­–ç¨³å®šæ€§åˆ†æ] è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿã€‚\n",
    "        éšæœºæ‰°åŠ¨ç›®æ ‡å‡½æ•°ç³»æ•°ï¼Œè§‚å¯Ÿå“ªäº›å†³ç­–å˜é‡(z)å‘ç”Ÿç¿»è½¬ã€‚\n",
    "        :param perturb_range: æ‰°åŠ¨å¹…åº¦ (0.1 = Â±10%)\n",
    "        :param runs: æ¨¡æ‹Ÿæ¬¡æ•°\n",
    "        \"\"\"\n",
    "        print(f\"\\nğŸŒªï¸ å¯åŠ¨å†³ç­–ç¨³å®šæ€§åˆ†æ (Runs={runs}, Perturb=Â±{perturb_range:.0%})...\")\n",
    "        if not self.binary_vars: return\n",
    "        \n",
    "        # 1. ä¿å­˜åŸºå‡†è§£\n",
    "        base_sol = {v.name: v.varValue for v in self.binary_vars}\n",
    "        original_objective = self.prob.objective\n",
    "        \n",
    "        # [Fix] å®‰å…¨åœ°è½¬æ¢ç›®æ ‡å‡½æ•°è¡¨è¾¾å¼ä¸ºå­—å…¸\n",
    "        try: \n",
    "            coeffs = dict(original_objective.items()) \n",
    "        except: \n",
    "            print(\"   -> æ— æ³•è§£æç›®æ ‡å‡½æ•°ï¼Œè·³è¿‡åˆ†æã€‚\")\n",
    "            return\n",
    "\n",
    "        flip_counts = {v.name: 0 for v in self.binary_vars}\n",
    "        valid_runs = 0\n",
    "        \n",
    "        # 2. æ¨¡æ‹Ÿå¾ªç¯\n",
    "        for r in range(runs):\n",
    "            new_obj = 0\n",
    "            for v_obj, c in coeffs.items():\n",
    "                # æ–½åŠ éšæœºæ‰°åŠ¨: c_new = c * (1 + random_noise)\n",
    "                noise = np.random.uniform(-perturb_range, perturb_range)\n",
    "                new_obj += c * (1 + noise) * v_obj\n",
    "            \n",
    "            # ä¸´æ—¶æ›´æ–°ç›®æ ‡å¹¶é‡ç®—\n",
    "            self.prob.setObjective(new_obj)\n",
    "            self.prob.solve(pulp.PULP_CBC_CMD(msg=0))\n",
    "            \n",
    "            if self.prob.status == 1:\n",
    "                valid_runs += 1\n",
    "                for v in self.binary_vars:\n",
    "                    val = v.varValue\n",
    "                    # æ£€æŸ¥å˜é‡ç¿»è½¬ (Safe Access)\n",
    "                    if val is not None and base_sol.get(v.name) is not None:\n",
    "                        if abs(val - base_sol[v.name]) > 0.5:\n",
    "                            flip_counts[v.name] += 1\n",
    "            print(\".\", end=\"\") # è¿›åº¦æ¡\n",
    "        \n",
    "        print(f\" å®Œæˆ! (æœ‰æ•ˆè¿è¡Œ: {valid_runs})\")\n",
    "        \n",
    "        # 3. æ¢å¤ç¯å¢ƒ\n",
    "        self.prob.setObjective(original_objective)\n",
    "        self.prob.solve(pulp.PULP_CBC_CMD(msg=0)) \n",
    "        \n",
    "        # 4. æŠ¥å‘Šä¸ç¨³å®šå˜é‡\n",
    "        unstable_vars = [k for k, v in flip_counts.items() if v > 0]\n",
    "        print(f\"   -> ä¸ç¨³å®šå˜é‡æ•°: {len(unstable_vars)} / {len(self.binary_vars)}\")\n",
    "\n",
    "    # ======================================================\n",
    "    # Phase 4: å¯è§†åŒ– (Visualization)\n",
    "    # ======================================================\n",
    "    def visualize_routing(self, from_nodes, to_nodes, active_matrix_name='x', pos_dict=None, custom_legends=None):\n",
    "        \"\"\"\n",
    "        ç»˜åˆ¶ç½‘ç»œæ‹“æ‰‘å›¾ (æ”¯æŒè‡ªå®šä¹‰å›¾ä¾‹å’Œå®‰å…¨å˜é‡è®¿é—®)ã€‚\n",
    "        :param active_matrix_name: åŒ…å«è¿æ¥å…³ç³»çš„å˜é‡å (å¦‚ 'y' æˆ– 'x')\n",
    "        :param pos_dict: èŠ‚ç‚¹åæ ‡å­—å…¸ {node_id: (x, y)}\n",
    "        :param custom_legends: è‡ªå®šä¹‰å›¾ä¾‹åˆ—è¡¨ [Line2D(...), ...]\n",
    "        \"\"\"\n",
    "        print(f\"\\nğŸ—ºï¸ ç»˜åˆ¶è·¯å¾„è§„åˆ’å›¾...\")\n",
    "        if active_matrix_name not in self.matrix_vars: return\n",
    "        \n",
    "        G = nx.DiGraph()\n",
    "        vars_dict = self.matrix_vars[active_matrix_name]\n",
    "        edge_count = 0\n",
    "        \n",
    "        # éå†æ‰€æœ‰å¯èƒ½çš„è¿æ¥\n",
    "        for i in from_nodes:\n",
    "            for j in to_nodes:\n",
    "                if i == j: continue\n",
    "                val = 0\n",
    "                \n",
    "                # [Safe Access Logic] å…¼å®¹ Nested Dict å’Œ Flat Dict\n",
    "                var_obj = None\n",
    "                try:\n",
    "                    if i in vars_dict and j in vars_dict[i]: var_obj = vars_dict[i][j]\n",
    "                    elif (i,j) in vars_dict: var_obj = vars_dict[(i,j)]\n",
    "                except: pass\n",
    "\n",
    "                if var_obj is not None:\n",
    "                    try: val = pulp.value(var_obj)\n",
    "                    except: val = 0 \n",
    "                \n",
    "                # è‹¥å˜é‡å€¼æ¥è¿‘ 1ï¼Œåˆ™æ·»åŠ è¾¹\n",
    "                if val and val > 0.9:\n",
    "                    G.add_edge(i, j)\n",
    "                    edge_count += 1\n",
    "        \n",
    "        if edge_count == 0:\n",
    "            print(\"   -> æœªå‘ç°æ´»è·ƒè·¯å¾„ (No active edges)ã€‚\")\n",
    "            return\n",
    "\n",
    "        # ç»˜å›¾é€»è¾‘\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        pos = pos_dict if pos_dict else nx.spring_layout(G, k=0.5, seed=42)\n",
    "        nx.draw_networkx(G, pos, node_size=300, node_color='skyblue', with_labels=True, \n",
    "                         arrowsize=15, width=0.8, alpha=0.7)\n",
    "        plt.title(f'Optimal Routing Plan ({edge_count} Active Edges)')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # æŒ‚è½½è‡ªå®šä¹‰å›¾ä¾‹\n",
    "        if custom_legends:\n",
    "            plt.legend(handles=custom_legends, loc='best', shadow=True)\n",
    "            \n",
    "        save_path = f\"{self.output_dir}/Routing_Topology.png\"\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"âœ… è·¯ç”±å›¾å·²ä¿å­˜: {save_path}\")\n",
    "\n",
    "    # ======================================================\n",
    "    # Phase 5: å…¨é‡å½’æ¡£ (Archiving)\n",
    "    # ======================================================\n",
    "    def archive_project(self, dataframes=None, figures=None, note=\"\"):\n",
    "        \"\"\"\n",
    "        [äº¤ä»˜] é¡¹ç›®å…¨é‡å½’æ¡£ä¸å‹ç¼©ã€‚\n",
    "        å°†æ‰€æœ‰æ•°æ®è¡¨ã€å›¾ç‰‡ã€æ¨¡å‹æ–‡ä»¶æ‰“åŒ…åˆ°ä¸€ä¸ªå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å¤¹ï¼Œå¹¶ç”Ÿæˆ ZIPã€‚\n",
    "        :param dataframes: ç»“æœæ•°æ®å­—å…¸ {'name.xlsx': df}\n",
    "        :param figures: å›¾ç‰‡å¯¹è±¡å­—å…¸ {'name.png': fig}\n",
    "        :param note: è¿è¡Œå¤‡æ³¨ (å†™å…¥ Log)\n",
    "        \"\"\"\n",
    "        run_id = f\"Run_{int(time.time())}\"\n",
    "        sub_dir = f\"{self.output_dir}/{run_id}\"\n",
    "        os.makedirs(sub_dir, exist_ok=True)\n",
    "        print(f\"\\nğŸ—„ï¸ === å¼€å§‹å…¨é‡å½’æ¡£ (Archive) ===\")\n",
    "        print(f\"   -> ğŸ“‚ ä¸´æ—¶ç›®å½•: {sub_dir}\")\n",
    "\n",
    "        # A. ä¿å­˜ DataFrames\n",
    "        if dataframes:\n",
    "            for fname, df in dataframes.items():\n",
    "                try:\n",
    "                    fpath = os.path.join(sub_dir, fname)\n",
    "                    if fname.endswith('.csv'): df.to_csv(fpath, index=False)\n",
    "                    else: df.to_excel(fpath, index=False)\n",
    "                    print(f\"   -> ğŸ’¾ æ•°æ®: {fname}\")\n",
    "                except Exception as e: print(f\"   -> âš ï¸ ä¿å­˜ {fname} å¤±è´¥: {e}\")\n",
    "\n",
    "        # B. ä¿å­˜ Matplotlib Figures\n",
    "        if figures:\n",
    "            for fname, fig in figures.items():\n",
    "                try:\n",
    "                    fpath = os.path.join(sub_dir, fname)\n",
    "                    fig.savefig(fpath, dpi=300, bbox_inches='tight')\n",
    "                    plt.close(fig) # æ˜¾å¼å…³é—­ä»¥é‡Šæ”¾å†…å­˜\n",
    "                    print(f\"   -> ğŸ–¼ï¸ å›¾ç‰‡: {fname}\")\n",
    "                except Exception as e: print(f\"   -> âš ï¸ ä¿å­˜ {fname} å¤±è´¥: {e}\")\n",
    "\n",
    "        # C. å¯¼å‡ºæ¨¡å‹æ–‡ä»¶ (Debug ç”¨)\n",
    "        try: self.prob.writeLP(os.path.join(sub_dir, \"Model.lp\"))\n",
    "        except: pass\n",
    "\n",
    "        # D. å†™å…¥è¿è¡Œæ—¥å¿—\n",
    "        with open(os.path.join(sub_dir, \"Run_Log.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"Timestamp: {time.ctime()}\\n\")\n",
    "            f.write(f\"Objective: {pulp.value(self.prob.objective)}\\n\")\n",
    "            f.write(f\"Status: {pulp.LpStatus[self.prob.status]}\\n\")\n",
    "            f.write(f\"Note: {note}\\n\")\n",
    "            \n",
    "        # E. è‡ªåŠ¨å‹ç¼© (Auto-Zip)\n",
    "        try:\n",
    "            shutil.make_archive(sub_dir, 'zip', sub_dir)\n",
    "            print(f\"âœ… å½’æ¡£å‹ç¼©åŒ…å·²ç”Ÿæˆ: {sub_dir}.zip\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ å‹ç¼©å¤±è´¥: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a1a620b-c98c-48e8-ab77-214c3f67de05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ === Project BioShield: System Initialization (V7.5) ===\n",
      "ğŸ“¦ [Container] Data Loaded: 'clinics   ' | Shape: (1000, 4)\n",
      "ğŸ“¦ [Container] Data Loaded: 'hubs      ' | Shape: (20, 5)\n",
      "ğŸ“¦ [Container] Data Loaded: 'plants    ' | Shape: (5, 4)\n",
      "ğŸ“¦ [Container] Data Loaded: 'waste     ' | Shape: (7000, 3)\n",
      "\n",
      "ğŸ“Š === Phase 0: God-Mode Supply/Demand Audit ===\n",
      "   -> ğŸ­ æ€»ç„šçƒ§èƒ½åŠ› (Daily Capacity): 18430\n",
      "   -> ğŸ“ˆ å³°å€¼åºŸç‰©é‡ (Peak Waste):      13914 (Day 3)\n",
      "\n",
      "âœ… [PASS] ç³»ç»Ÿäº§èƒ½å……è¶³ (Healthy)\n",
      "   -> å®‰å…¨ä½™é‡ (Buffer): 4516\n",
      "\n",
      "âœ… Phase 0 å®Œæˆã€‚ç­‰å¾… Phase 1 å˜é‡å®šä¹‰æŒ‡ä»¤ã€‚\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# å‡è®¾ IP_Solver_Capsule ç±»å®šä¹‰å·²åœ¨ä¸Šä¸‹æ–‡ä¸­åŠ è½½\n",
    "# å¦‚æœæœªåŠ è½½ï¼Œè¯·å…ˆè¿è¡Œç±»å®šä¹‰ä»£ç \n",
    "\n",
    "print(\"ğŸš€ === Project BioShield: System Initialization (V7.5) ===\")\n",
    "\n",
    "# ==========================================\n",
    "# ğŸ› ï¸ V7.5 æ¶æ„å‡çº§: æ•°æ®å®¹å™¨åŒ–è¡¥ä¸\n",
    "# ==========================================\n",
    "# ä½¿ç”¨ Monkey Patching åŠ¨æ€æ‰©å±•ç±»åŠŸèƒ½ï¼Œä¿æŒåŸæœ‰ç±»å®šä¹‰çº¯å‡€\n",
    "def load_data_extension(self, key, df):\n",
    "    \"\"\"\n",
    "    [V7.5 New Feature] æ•°æ®å®¹å™¨è£…è½½å™¨\n",
    "    å°† DataFrame å°è£…è¿› solver å®ä¾‹ï¼Œé˜²æ­¢å…¨å±€å‘½åç©ºé—´æ±¡æŸ“\n",
    "    \"\"\"\n",
    "    if not hasattr(self, 'data'):\n",
    "        self.data = {}\n",
    "    self.data[key] = df.copy() # ä½¿ç”¨å‰¯æœ¬é˜²æ­¢å¤–éƒ¨ç¯¡æ”¹\n",
    "    print(f\"ğŸ“¦ [Container] Data Loaded: '{key:<10}' | Shape: {df.shape}\")\n",
    "\n",
    "# ç»‘å®šæ–¹æ³•åˆ°ç±»\n",
    "IP_Solver_Capsule.load_data = load_data_extension\n",
    "\n",
    "# ==========================================\n",
    "# 1. ç¯å¢ƒåˆå§‹åŒ–\n",
    "# ==========================================\n",
    "# åˆå§‹åŒ–æ±‚è§£å™¨ (Minimize Risk/Cost)\n",
    "solver = IP_Solver_Capsule(name=\"BioShield\", sense='min')\n",
    "\n",
    "# ==========================================\n",
    "# 2. æ•°æ®è£…è½½ (ETL)\n",
    "# ==========================================\n",
    "try:\n",
    "    # è¯»å–åŸå§‹ CSV\n",
    "    df_clinics = pd.read_csv('Clinics.csv')\n",
    "    df_hubs = pd.read_csv('Hubs.csv')\n",
    "    df_plants = pd.read_csv('Plants.csv')\n",
    "    df_waste = pd.read_csv('Waste_TimeSeries.csv')\n",
    "\n",
    "    # è£…è½½è¿›å®¹å™¨\n",
    "    solver.load_data('clinics', df_clinics)\n",
    "    solver.load_data('hubs', df_hubs)\n",
    "    solver.load_data('plants', df_plants)\n",
    "    solver.load_data('waste', df_waste)\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"âŒ [Error] æ•°æ®æ–‡ä»¶ç¼ºå¤±: {e}\")\n",
    "\n",
    "# ==========================================\n",
    "# ğŸ‘ï¸ Phase 0: ä¸Šå¸è§†è§’å®¡è®¡ (God-Mode Audit)\n",
    "# ==========================================\n",
    "print(\"\\nğŸ“Š === Phase 0: God-Mode Supply/Demand Audit ===\")\n",
    "\n",
    "# ä»å®¹å™¨ä¸­æå–æ•°æ® (ä¸¥ç¦ä½¿ç”¨å¤–éƒ¨å˜é‡)\n",
    "ts_data = solver.data['waste']\n",
    "plant_data = solver.data['plants']\n",
    "\n",
    "# 1. è®¡ç®—æ€»ä¾›ç»™ (Total Capacity)\n",
    "# å‡è®¾ Plant æ¯å¤©éƒ½èƒ½å…¨è´Ÿè·è¿è½¬\n",
    "total_daily_capacity = plant_data['Daily_Capacity'].sum()\n",
    "\n",
    "# 2. è®¡ç®—æ€»éœ€æ±‚ (Total Waste per Day)\n",
    "daily_waste_stats = ts_data.groupby('Day')['New_Waste'].sum()\n",
    "peak_waste = daily_waste_stats.max()\n",
    "peak_day = daily_waste_stats.idxmax()\n",
    "\n",
    "# 3. å®¡è®¡æŠ¥å‘Š\n",
    "print(f\"   -> ğŸ­ æ€»ç„šçƒ§èƒ½åŠ› (Daily Capacity): {total_daily_capacity}\")\n",
    "print(f\"   -> ğŸ“ˆ å³°å€¼åºŸç‰©é‡ (Peak Waste):      {peak_waste} (Day {peak_day})\")\n",
    "\n",
    "# 4. ç†”æ–­é¢„åˆ¤\n",
    "if total_daily_capacity < peak_waste:\n",
    "    gap = peak_waste - total_daily_capacity\n",
    "    print(f\"\\n\\033[91mğŸš¨ [CRITICAL WARNING] ç³»ç»Ÿæ€§äº§èƒ½ä¸è¶³ (System Deficit)!\\033[0m\")\n",
    "    print(f\"   -> ç¼ºå£ (Gap): {gap}\")\n",
    "    print(f\"   -> âš ï¸ å¿…é¡»å¯ç”¨ 'å¤–åŒ…å¤„ç† (Outsourcing)' æ¨¡å—ä½œä¸ºè™šæ‹Ÿä¾›ç»™èŠ‚ç‚¹ï¼Œå¦åˆ™æ¨¡å‹å°†æ— è§£ (Infeasible)ã€‚\")\n",
    "    solver.outsourcing_needed = True\n",
    "else:\n",
    "    buffer = total_daily_capacity - peak_waste\n",
    "    print(f\"\\nâœ… [PASS] ç³»ç»Ÿäº§èƒ½å……è¶³ (Healthy)\")\n",
    "    print(f\"   -> å®‰å…¨ä½™é‡ (Buffer): {buffer}\")\n",
    "    solver.outsourcing_needed = False\n",
    "\n",
    "print(\"\\nâœ… Phase 0 å®Œæˆã€‚ç­‰å¾… Phase 1 å˜é‡å®šä¹‰æŒ‡ä»¤ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2478076-ce23-4e3b-a672-5e62227d131f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ === Phase 1: Sparse Spatiotemporal Variable Definition ===\n",
      "   -> æ­£åœ¨è®¡ç®—è·ç¦»çŸ©é˜µ (Euclidean)...\n",
      "âœ… ç©ºé—´ç¨€ç–åŒ–å®Œæˆ (Top-3 Strategy):\n",
      "   -> å…¨è¿æ¥ç†è®ºæ•°: 20000\n",
      "   -> ç¨€ç–åè¿æ¥æ•°: 3000\n",
      "   -> â¬‡ï¸ å˜é‡ç¼©å‡ç‡: 85.0%\n",
      "\n",
      "   -> æ­£åœ¨æ³¨å†Œæ—¶ç©ºå˜é‡ (Spatiotemporal Variables)...\n",
      "âœ… å­—å…¸å˜é‡å·²åˆ›å»º: x [Size: 21000] (Type: Continuous)\n",
      "âœ… å­—å…¸å˜é‡å·²åˆ›å»º: y [Size: 700] (Type: Continuous)\n",
      "âœ… å­—å…¸å˜é‡å·²åˆ›å»º: z [Size: 140] (Type: Binary)\n",
      "âœ… å­—å…¸å˜é‡å·²åˆ›å»º: inv_clinic [Size: 7000] (Type: Continuous)\n",
      "âœ… å­—å…¸å˜é‡å·²åˆ›å»º: overflow [Size: 7000] (Type: Continuous)\n",
      "âœ… å˜é‡æ³¨å†Œå®Œæ¯•:\n",
      "   -> x (C->H): 21000 (ç¨€ç–åŒ–)\n",
      "   -> y (H->P): 700\n",
      "   -> z (Status): 140\n",
      "   -> inv/overflow: 7000 * 2\n",
      "\n",
      "âœ… Phase 1 å®Œæˆã€‚ç­‰å¾… Phase 2 çº¦æŸæ„å»ºæŒ‡ä»¤ã€‚\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "print(\"\\nğŸš€ === Phase 1: Sparse Spatiotemporal Variable Definition ===\")\n",
    "\n",
    "# ==========================================\n",
    "# 1. æå–æ•°æ® (Data Retrieval)\n",
    "# ==========================================\n",
    "# ä» solver å®¹å™¨ä¸­å®‰å…¨æå–æ•°æ®ï¼Œé¿å… NameError\n",
    "clinics = solver.data['clinics']\n",
    "hubs = solver.data['hubs']\n",
    "plants = solver.data['plants']\n",
    "days = range(1, 8)  # T=1..7\n",
    "\n",
    "# ==========================================\n",
    "# 2. ç©ºé—´ç¨€ç–åŒ– (Spatial Sparsity)\n",
    "# ==========================================\n",
    "print(\"   -> æ­£åœ¨è®¡ç®—è·ç¦»çŸ©é˜µ (Euclidean)...\")\n",
    "# æå–åæ ‡æ•°ç»„\n",
    "coords_c = clinics[['X', 'Y']].values\n",
    "coords_h = hubs[['X', 'Y']].values\n",
    "\n",
    "# è®¡ç®—å…¨é‡è·ç¦»çŸ©é˜µ [1000, 20]\n",
    "dist_matrix = cdist(coords_c, coords_h, metric='euclidean')\n",
    "\n",
    "# æ ¸å¿ƒç®—æ³•: ä¸ºæ¯ä¸ª Clinic ä¿ç•™æœ€è¿‘çš„ 3 ä¸ª Hub\n",
    "valid_arcs = []       # å­˜å‚¨å…ƒç»„ (c_id, h_id)\n",
    "valid_arcs_dist = {}  # å­˜å‚¨è·ç¦» { (c,h): dist }ï¼Œä¾› Phase 2 ç›®æ ‡å‡½æ•°ä½¿ç”¨\n",
    "\n",
    "# éå†æ¯ä¸ªè¯Šæ‰€è¿›è¡Œç­›é€‰\n",
    "for i in range(len(clinics)):\n",
    "    c_id = clinics.iloc[i]['Clinic_ID']\n",
    "    # argsort è¿”å›æ’åºåçš„ç´¢å¼•ï¼Œå–å‰3ä¸ª\n",
    "    nearest_indices = np.argsort(dist_matrix[i])[:3]\n",
    "    \n",
    "    for h_idx in nearest_indices:\n",
    "        h_id = hubs.iloc[h_idx]['Hub_ID']\n",
    "        dist_val = dist_matrix[i][h_idx]\n",
    "        \n",
    "        valid_arcs.append((c_id, h_id))\n",
    "        valid_arcs_dist[(c_id, h_id)] = dist_val\n",
    "\n",
    "# æ•ˆæœå®¡è®¡\n",
    "full_connections = len(clinics) * len(hubs)\n",
    "sparse_connections = len(valid_arcs)\n",
    "sparsity_ratio = 1 - (sparse_connections / full_connections)\n",
    "\n",
    "print(f\"âœ… ç©ºé—´ç¨€ç–åŒ–å®Œæˆ (Top-3 Strategy):\")\n",
    "print(f\"   -> å…¨è¿æ¥ç†è®ºæ•°: {full_connections}\")\n",
    "print(f\"   -> ç¨€ç–åè¿æ¥æ•°: {sparse_connections}\")\n",
    "print(f\"   -> â¬‡ï¸ å˜é‡ç¼©å‡ç‡: {sparsity_ratio:.1%}\")\n",
    "\n",
    "# å°†è·ç¦»å­—å…¸å­˜å›å®¹å™¨ï¼Œä¾›åç»­ä½¿ç”¨\n",
    "solver.data['dist_matrix'] = valid_arcs_dist\n",
    "# åŒæ—¶ä¿å­˜ valid_arcs åˆ—è¡¨ï¼ŒPhase 2 çº¦æŸç”Ÿæˆéœ€è¦ç”¨åˆ°\n",
    "solver.data['valid_arcs'] = valid_arcs \n",
    "\n",
    "# ==========================================\n",
    "# 3. å˜é‡æ³¨å†Œ (Variable Registration)\n",
    "# ==========================================\n",
    "print(\"\\n   -> æ­£åœ¨æ³¨å†Œæ—¶ç©ºå˜é‡ (Spatiotemporal Variables)...\")\n",
    "\n",
    "# --- 3.1 ç”Ÿæˆæ—¶ç©º Keys ---\n",
    "# x: (Clinic, Hub, Day) - ä»…é’ˆå¯¹ valid_arcs\n",
    "keys_x = [(c, h, t) for (c, h) in valid_arcs for t in days]\n",
    "\n",
    "# y: (Hub, Plant, Day) - Hub åˆ° Plant è§„æ¨¡å° (20*5)ï¼Œä¿ç•™å…¨è¿æ¥\n",
    "hub_ids = hubs['Hub_ID'].tolist()\n",
    "plant_ids = plants['Plant_ID'].tolist()\n",
    "keys_y = [(h, p, t) for h in hub_ids for p in plant_ids for t in days]\n",
    "\n",
    "# z: (Hub, Day) - é€‰å€/å¼€å¯å˜é‡\n",
    "keys_z = [(h, t) for h in hub_ids for t in days]\n",
    "\n",
    "# inv & overflow: (Clinic, Day) - åº“å­˜ä¸æº¢å‡º\n",
    "clinic_ids = clinics['Clinic_ID'].tolist()\n",
    "keys_inv = [(c, t) for c in clinic_ids for t in days]\n",
    "\n",
    "# --- 3.2 æ³¨å†Œåˆ° Solver ---\n",
    "# è¿é‡ x: è¿ç»­å˜é‡ >= 0\n",
    "x = solver.add_vars_dict(keys_x, name=\"x\", cat=\"Continuous\", lowBound=0)\n",
    "\n",
    "# è¿é‡ y: è¿ç»­å˜é‡ >= 0\n",
    "y = solver.add_vars_dict(keys_y, name=\"y\", cat=\"Continuous\", lowBound=0)\n",
    "\n",
    "# å¼€å¯çŠ¶æ€ z: äºŒå…ƒå˜é‡ (0/1)\n",
    "z = solver.add_vars_dict(keys_z, name=\"z\", cat=\"Binary\")\n",
    "\n",
    "# è¯Šæ‰€åº“å­˜ inv: è¿ç»­å˜é‡ >= 0\n",
    "inv_clinic = solver.add_vars_dict(keys_inv, name=\"inv_clinic\", cat=\"Continuous\", lowBound=0)\n",
    "\n",
    "# æº¢å‡ºé‡ overflow: è¿ç»­å˜é‡ >= 0 (è½¯çº¦æŸæ ¸å¿ƒ)\n",
    "overflow = solver.add_vars_dict(keys_inv, name=\"overflow\", cat=\"Continuous\", lowBound=0)\n",
    "\n",
    "print(f\"âœ… å˜é‡æ³¨å†Œå®Œæ¯•:\")\n",
    "print(f\"   -> x (C->H): {len(x)} (ç¨€ç–åŒ–)\")\n",
    "print(f\"   -> y (H->P): {len(y)}\")\n",
    "print(f\"   -> z (Status): {len(z)}\")\n",
    "print(f\"   -> inv/overflow: {len(inv_clinic)} * 2\")\n",
    "print(\"\\nâœ… Phase 1 å®Œæˆã€‚ç­‰å¾… Phase 2 çº¦æŸæ„å»ºæŒ‡ä»¤ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5756bfc-2b59-43cf-adfa-218853f75d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ === Phase 2: Dynamic Logic Injection (Constraints & Objective) ===\n",
      "   -> æ­£åœ¨é¢„è®¡ç®—æ‹“æ‰‘æ˜ å°„ä¸å‚æ•°...\n",
      "   -> æ³¨å…¥çº¦æŸ: è¯Šæ‰€åº“å­˜åŠ¨æ€å¹³è¡¡ (Dynamic Balance)...\n",
      "   -> æ³¨å…¥çº¦æŸ: åº“å®¹ç†”æ–­æœºåˆ¶ (Soft Capacity)...\n",
      "   -> æ³¨å…¥çº¦æŸ: ä¸­è½¬ç«™ Cross-Docking æµå¹³è¡¡...\n",
      "   -> æ³¨å…¥çº¦æŸ: ä¸­è½¬ç«™å¼€å¯ä¸å®¹é‡è”åŠ¨ (Big-M)...\n",
      "âœ… çº¦æŸæ³¨å…¥å®Œæˆ: 14280 æ¡\n",
      "   -> æ„å»ºç›®æ ‡å‡½æ•° (Minimize Total Cost)...\n",
      "âœ… ç›®æ ‡å‡½æ•°å·²è®¾å®š (åŒ…å«å·¨é¢æƒ©ç½šé¡¹ Overflow * 10000)ã€‚\n",
      "\n",
      "âœ… Phase 2 å®Œæˆã€‚ç­‰å¾… Phase 3 æ±‚è§£æŒ‡ä»¤ã€‚\n"
     ]
    }
   ],
   "source": [
    "import pulp\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "print(\"\\nğŸš€ === Phase 2: Dynamic Logic Injection (Constraints & Objective) ===\")\n",
    "\n",
    "# ==========================================\n",
    "# 0. æ•°æ®é¢„å¤„ç† (Pre-computation)\n",
    "# ==========================================\n",
    "print(\"   -> æ­£åœ¨é¢„è®¡ç®—æ‹“æ‰‘æ˜ å°„ä¸å‚æ•°...\")\n",
    "\n",
    "# æå–å˜é‡å¼•ç”¨\n",
    "x = solver.matrix_vars['x']\n",
    "y = solver.matrix_vars['y']\n",
    "z = solver.matrix_vars['z']\n",
    "inv = solver.matrix_vars['inv_clinic']\n",
    "overflow = solver.matrix_vars['overflow']\n",
    "\n",
    "# æ•°æ®æå–\n",
    "clinics = solver.data['clinics']\n",
    "hubs = solver.data['hubs']\n",
    "plants = solver.data['plants']\n",
    "waste_df = solver.data['waste']\n",
    "valid_arcs = solver.data['valid_arcs']\n",
    "dist_c_h = solver.data['dist_matrix']  # {(c,h): dist}\n",
    "\n",
    "# è¾…åŠ©æ˜ å°„è¡¨ (Adjacency List) åŠ é€Ÿæ±‚å’Œ\n",
    "# Map: Clinic -> [Hubs]\n",
    "c_to_h = {} \n",
    "# Map: Hub -> [Clinics]\n",
    "h_to_c = {} \n",
    "for c, h in valid_arcs:\n",
    "    c_to_h.setdefault(c, []).append(h)\n",
    "    h_to_c.setdefault(h, []).append(c)\n",
    "\n",
    "# Waste æŸ¥æ‰¾è¡¨ (Fast Lookup)\n",
    "waste_lookup = waste_df.set_index(['Clinic_ID', 'Day'])['New_Waste'].to_dict()\n",
    "\n",
    "# è®¡ç®— Hub->Plant è·ç¦»çŸ©é˜µ (ç”¨äºè¿è¾“æˆæœ¬)\n",
    "coords_h = hubs[['X', 'Y']].values\n",
    "coords_p = plants[['X', 'Y']].values\n",
    "d_h_p_matrix = cdist(coords_h, coords_p, metric='euclidean')\n",
    "dist_h_p = {} # {(h, p): dist}\n",
    "h_ids = hubs['Hub_ID'].tolist()\n",
    "p_ids = plants['Plant_ID'].tolist()\n",
    "for i, h in enumerate(h_ids):\n",
    "    for j, p in enumerate(p_ids):\n",
    "        dist_h_p[(h, p)] = d_h_p_matrix[i][j]\n",
    "\n",
    "# å¸¸é‡å®šä¹‰\n",
    "DAYS = range(1, 8)\n",
    "PENALTY_COST = 10000  # ç†”æ–­æƒ©ç½š\n",
    "TRANSPORT_COST_UNIT = 1.5  # å‡è®¾å•ä½è·ç¦»è¿è´¹\n",
    "\n",
    "# ==========================================\n",
    "# 1. åŠ¨æ€åº“å­˜å¹³è¡¡ (Dynamic Inventory Balance)\n",
    "# ==========================================\n",
    "print(\"   -> æ³¨å…¥çº¦æŸ: è¯Šæ‰€åº“å­˜åŠ¨æ€å¹³è¡¡ (Dynamic Balance)...\")\n",
    "count_inv = 0\n",
    "for c in clinics['Clinic_ID']:\n",
    "    for t in DAYS:\n",
    "        # è·å–å½“æ—¥äº§ç”ŸåºŸç‰©\n",
    "        new_waste = waste_lookup.get((c, t), 0)\n",
    "        \n",
    "        # è¿å‡ºé‡ (Sum x_ijt)\n",
    "        # æ³¨æ„: åªéå† valid_arcs ä¸­çš„ hub\n",
    "        outflow = pulp.lpSum([x[(c, h, t)] for h in c_to_h.get(c, [])])\n",
    "        \n",
    "        # ä¸ŠæœŸåº“å­˜ (Initial Condition: Inv_0 = 0)\n",
    "        prev_inv = 0 if t == 1 else inv[(c, t-1)]\n",
    "        \n",
    "        # çº¦æŸ: å½“æœŸåº“å­˜ = ä¸ŠæœŸ + æ–°å¢ - è¿å‡º\n",
    "        solver.prob += (inv[(c, t)] == prev_inv + new_waste - outflow, f\"Inv_Bal_{c}_{t}\")\n",
    "        count_inv += 1\n",
    "\n",
    "# ==========================================\n",
    "# 2. åº“å®¹ç†”æ–­ä¿æŠ¤ (Soft Capacity Constraint)\n",
    "# ==========================================\n",
    "print(\"   -> æ³¨å…¥çº¦æŸ: åº“å®¹ç†”æ–­æœºåˆ¶ (Soft Capacity)...\")\n",
    "count_cap = 0\n",
    "# å°† Max_Storage è½¬ä¸º dict åŠ é€Ÿ\n",
    "max_storage_map = clinics.set_index('Clinic_ID')['Max_Storage'].to_dict()\n",
    "\n",
    "for c in clinics['Clinic_ID']:\n",
    "    limit = max_storage_map[c]\n",
    "    for t in DAYS:\n",
    "        # Inv <= Max + Overflow\n",
    "        solver.prob += (inv[(c, t)] <= limit + overflow[(c, t)], f\"Cap_Soft_{c}_{t}\")\n",
    "        count_cap += 1\n",
    "\n",
    "# ==========================================\n",
    "# 3. ä¸­è½¬ç«™æµå¹³è¡¡ (Hub Flow Balance)\n",
    "# ==========================================\n",
    "print(\"   -> æ³¨å…¥çº¦æŸ: ä¸­è½¬ç«™ Cross-Docking æµå¹³è¡¡...\")\n",
    "count_hub = 0\n",
    "for h in h_ids:\n",
    "    for t in DAYS:\n",
    "        # Inflow: æ¥è‡ªæ‰€æœ‰è¿æ¥çš„ Clinics\n",
    "        inflow = pulp.lpSum([x[(c, h, t)] for c in h_to_c.get(h, [])])\n",
    "        \n",
    "        # Outflow: å»å¾€æ‰€æœ‰ Plants\n",
    "        outflow = pulp.lpSum([y[(h, p, t)] for p in p_ids])\n",
    "        \n",
    "        # çº¦æŸ: è¿› = å‡º (æ— åº“å­˜)\n",
    "        solver.prob += (inflow == outflow, f\"Hub_Flow_{h}_{t}\")\n",
    "        count_hub += 1\n",
    "\n",
    "# ==========================================\n",
    "# 4. è®¾æ–½è”åŠ¨ä¸å®¹é‡ (Facility Logic)\n",
    "# ==========================================\n",
    "print(\"   -> æ³¨å…¥çº¦æŸ: ä¸­è½¬ç«™å¼€å¯ä¸å®¹é‡è”åŠ¨ (Big-M)...\")\n",
    "count_link = 0\n",
    "hub_cap_map = hubs.set_index('Hub_ID')['Capacity'].to_dict()\n",
    "\n",
    "for h in h_ids:\n",
    "    cap = hub_cap_map[h]\n",
    "    for t in DAYS:\n",
    "        # å¤„ç†é‡ <= Capacity * Binary_Status\n",
    "        total_processed = pulp.lpSum([y[(h, p, t)] for p in p_ids])\n",
    "        solver.prob += (total_processed <= cap * z[(h, t)], f\"Hub_Link_{h}_{t}\")\n",
    "        count_link += 1\n",
    "\n",
    "print(f\"âœ… çº¦æŸæ³¨å…¥å®Œæˆ: {count_inv + count_cap + count_hub + count_link} æ¡\")\n",
    "\n",
    "# ==========================================\n",
    "# 5. ç›®æ ‡å‡½æ•°æ„å»º (Objective Function)\n",
    "# ==========================================\n",
    "print(\"   -> æ„å»ºç›®æ ‡å‡½æ•° (Minimize Total Cost)...\")\n",
    "\n",
    "# A. è¿è¾“æˆæœ¬ (Clinic -> Hub)\n",
    "cost_trans_x = pulp.lpSum([\n",
    "    x[(c, h, t)] * dist_c_h[(c, h)] * TRANSPORT_COST_UNIT \n",
    "    for (c, h) in valid_arcs for t in DAYS\n",
    "])\n",
    "\n",
    "# B. è¿è¾“æˆæœ¬ (Hub -> Plant)\n",
    "cost_trans_y = pulp.lpSum([\n",
    "    y[(h, p, t)] * dist_h_p[(h, p)] * TRANSPORT_COST_UNIT\n",
    "    for h in h_ids for p in p_ids for t in DAYS\n",
    "])\n",
    "\n",
    "# C. å›ºå®šå»ºè®¾/è¿è¥æˆæœ¬\n",
    "hub_fixed_cost_map = hubs.set_index('Hub_ID')['Fixed_Cost'].to_dict()\n",
    "cost_fixed = pulp.lpSum([\n",
    "    z[(h, t)] * hub_fixed_cost_map[h]\n",
    "    for h in h_ids for t in DAYS\n",
    "])\n",
    "\n",
    "# D. ç†”æ–­æƒ©ç½š (Penalty)\n",
    "cost_penalty = pulp.lpSum([\n",
    "    overflow[(c, t)] * PENALTY_COST\n",
    "    for c in clinics['Clinic_ID'] for t in DAYS\n",
    "])\n",
    "\n",
    "# Set Objective\n",
    "total_cost = cost_trans_x + cost_trans_y + cost_fixed + cost_penalty\n",
    "solver.prob += total_cost\n",
    "\n",
    "print(f\"âœ… ç›®æ ‡å‡½æ•°å·²è®¾å®š (åŒ…å«å·¨é¢æƒ©ç½šé¡¹ Overflow * {PENALTY_COST})ã€‚\")\n",
    "print(\"\\nâœ… Phase 2 å®Œæˆã€‚ç­‰å¾… Phase 3 æ±‚è§£æŒ‡ä»¤ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5363c45a-db51-48df-bf9e-2442ecc2dd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ === Phase 3: Solving & Bottleneck Analysis ===\n",
      "\n",
      "ğŸš€ å¯åŠ¨æ±‚è§£å™¨ (CBC)...\n",
      "ğŸ“‹ æ±‚è§£çŠ¶æ€: Optimal\n",
      "ğŸ’ æœ€ä¼˜ç›®æ ‡å€¼: 267754.3846887499\n",
      "\n",
      "ğŸ” === Result Diagnosis & Crisis Monitor ===\n",
      "   -> ğŸ“‰ ç³»ç»Ÿæ€»æº¢å‡ºé‡ (Total Overflow): 0.00 units\n",
      "\n",
      "âœ… [SAFE] ç³»ç»Ÿè¿è¡Œå¹³ç¨³ï¼Œæœªæ£€æµ‹åˆ°æ˜¾è‘—æº¢å‡ºé£é™©ã€‚\n",
      "   -> é²æ£’æ€§è¯„ä»·: Sçº§ (Excellent)\n",
      "\n",
      "âœ… Phase 3 å®Œæˆã€‚ç­‰å¾… Phase 4 å¯è§†åŒ–ä¸äº¤ä»˜æŒ‡ä»¤ã€‚\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nğŸš€ === Phase 3: Solving & Bottleneck Analysis ===\")\n",
    "\n",
    "# ==========================================\n",
    "# 1. æ‰§è¡Œæ±‚è§£ (Execution)\n",
    "# ==========================================\n",
    "# ç»™äºˆ 5 åˆ†é’Ÿæ—¶é—´çª—å£ï¼Œå…è®¸ 5% çš„ MIP Gap ä»¥æ¢å–å¿«é€Ÿå“åº”\n",
    "solver.solve(time_limit=300, gap_rel=0.05)\n",
    "\n",
    "# ==========================================\n",
    "# 2. ç»“æœè¯Šæ–­ (Result Diagnosis)\n",
    "# ==========================================\n",
    "print(\"\\nğŸ” === Result Diagnosis & Crisis Monitor ===\")\n",
    "\n",
    "# æå– Overflow å˜é‡ç»“æœ\n",
    "overflow_vars = solver.matrix_vars['overflow']\n",
    "total_overflow = 0\n",
    "overflow_by_day = {t: 0 for t in range(1, 8)}\n",
    "\n",
    "# éå†æ‰€æœ‰æº¢å‡ºå˜é‡æ±‚å’Œ\n",
    "for key, var in overflow_vars.items():\n",
    "    # key is (Clinic_ID, Day)\n",
    "    val = var.varValue\n",
    "    if val and val > 1e-4: # å¿½ç•¥æµ®ç‚¹è¯¯å·®\n",
    "        total_overflow += val\n",
    "        t = key[1]\n",
    "        overflow_by_day[t] += val\n",
    "\n",
    "print(f\"   -> ğŸ“‰ ç³»ç»Ÿæ€»æº¢å‡ºé‡ (Total Overflow): {total_overflow:,.2f} units\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. å±æœºæŠ¥å‘Š (Crisis Report)\n",
    "# ==========================================\n",
    "if total_overflow > 1.0: # è®¾å®šé˜ˆå€¼ï¼Œå¿½ç•¥å¾®å°è¯¯å·®\n",
    "    print(\"\\n\\033[91mğŸš¨ [CRITICAL ALERT] è§¦å‘å±æœºæŠ¥å‘Š (Crisis Report Triggered) \\033[0m\")\n",
    "    \n",
    "    # A. ç¡®å®šè‡³æš—æ—¶åˆ» (The Darkest Day)\n",
    "    worst_day = max(overflow_by_day, key=overflow_by_day.get)\n",
    "    max_daily_overflow = overflow_by_day[worst_day]\n",
    "    print(f\"   -> ğŸ“… æœ€ä¸¥é‡çš„ä¸€å¤© (Worst Day): Day {worst_day}\")\n",
    "    print(f\"   -> ğŸŒŠ å½“æ—¥æº¢å‡ºé‡: {max_daily_overflow:,.2f}\")\n",
    "    \n",
    "    # B. æ ¹æœ¬åŸå› åˆ†æ (Root Cause Analysis)\n",
    "    # æå–æœ€ä¸¥é‡é‚£å¤©çš„ç³»ç»ŸçŠ¶æ€\n",
    "    plants = solver.data['plants']\n",
    "    hubs = solver.data['hubs']\n",
    "    y_vars = solver.matrix_vars['y']\n",
    "    \n",
    "    # è®¡ç®—å½“æ—¥ç„šçƒ§å‚åˆ©ç”¨ç‡\n",
    "    total_plant_cap = plants['Daily_Capacity'].sum()\n",
    "    total_incinerated = 0\n",
    "    \n",
    "    # ç´¯åŠ å½“æ—¥æ‰€æœ‰ Hub -> Plant çš„æµé‡\n",
    "    # y keys: (h, p, t)\n",
    "    for h in hubs['Hub_ID']:\n",
    "        for p in plants['Plant_ID']:\n",
    "            try:\n",
    "                val = y_vars[(h, p, worst_day)].varValue\n",
    "                if val: total_incinerated += val\n",
    "            except: pass\n",
    "            \n",
    "    util_rate = total_incinerated / total_plant_cap\n",
    "    \n",
    "    print(f\"   -> ğŸ­ ç„šçƒ§å‚è´Ÿè·ç‡ (Incineration Load): {util_rate:.1%} ({total_incinerated:,.0f}/{total_plant_cap:,.0f})\")\n",
    "    \n",
    "    # C. å½’å› ç»“è®º\n",
    "    if util_rate > 0.98:\n",
    "        print(\"   -> ğŸ›‘ **ç“¶é¢ˆåˆ¤å®š**: [äº§èƒ½ç¡¬ç¼ºå£] (Capacity Bottleneck)\")\n",
    "        print(\"      è§£é‡Š: ç„šçƒ§å‚å·²æ»¡è´Ÿè·è¿è½¬ï¼Œå¿…é¡»æ‰©å»ºæˆ–å¯ç”¨å¤–åŒ…ã€‚\")\n",
    "    else:\n",
    "        print(\"   -> ğŸšš **ç“¶é¢ˆåˆ¤å®š**: [ç‰©æµé˜»å¡] (Logistics Bottleneck)\")\n",
    "        print(\"      è§£é‡Š: ç„šçƒ§å‚ä»æœ‰ä½™åŠ›ï¼Œä½†åºŸç‰©å¡åœ¨å‰ç«¯ã€‚å¯èƒ½æ˜¯ä¸­è½¬ç«™å®¹é‡ä¸è¶³(Hub Capacity)æˆ–è½¦è¾†è¿åŠ›å—é™ã€‚\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nâœ… [SAFE] ç³»ç»Ÿè¿è¡Œå¹³ç¨³ï¼Œæœªæ£€æµ‹åˆ°æ˜¾è‘—æº¢å‡ºé£é™©ã€‚\")\n",
    "    print(\"   -> é²æ£’æ€§è¯„ä»·: Sçº§ (Excellent)\")\n",
    "\n",
    "print(\"\\nâœ… Phase 3 å®Œæˆã€‚ç­‰å¾… Phase 4 å¯è§†åŒ–ä¸äº¤ä»˜æŒ‡ä»¤ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20268cfa-a169-4d83-b664-c70ded4b61c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¨ === Phase 4: Visualization & Delivery (Fail-Safe Mode) ===\n",
      "   -> æ­£åœ¨æå–å˜é‡ç»“æœ...\n",
      "   -> ç»˜åˆ¶å›¾ 1: åŒºåŸŸé£é™©çƒ­åŠ›å›¾ (Clustered Heatmap)...\n",
      "   -> âœ… çƒ­åŠ›å›¾å·²ä¿å­˜: ./Results_BioShield_1766055163/Risk_Heatmap.png\n",
      "   -> ç»˜åˆ¶å›¾ 2: ç„šçƒ§å‚è´Ÿè·å›¾ (Stacked Bar)...\n",
      "   -> âœ… è´Ÿè·å›¾å·²ä¿å­˜: ./Results_BioShield_1766055163/Plant_Load.png\n",
      "   -> å¯¼å‡º Excel: Risk_Log.xlsx ...\n",
      "   -> âœ… å¤ªæ£’äº†ï¼ç³»ç»Ÿä¸­æ²¡æœ‰ä»»ä½•æº¢å‡ºè®°å½• (No Overflow)ã€‚æ— éœ€ç”Ÿæˆ Risk Logã€‚\n",
      "\n",
      "ğŸ—„ï¸ === å¼€å§‹å…¨é‡å½’æ¡£ (Archive) ===\n",
      "   -> ğŸ“‚ ä¸´æ—¶ç›®å½•: ./Results_BioShield_1766055163/Run_1766055530\n",
      "   -> ğŸ’¾ æ•°æ®: Risk_Log.xlsx\n",
      "   -> ğŸ’¾ æ•°æ®: Inventory_Detail.csv\n",
      "âœ… å½’æ¡£å‹ç¼©åŒ…å·²ç”Ÿæˆ: ./Results_BioShield_1766055163/Run_1766055530.zip\n",
      "\n",
      "âœ… Phase 4 å…¨éƒ¨å®Œæˆã€‚Project BioShield ä»»åŠ¡ç»“æŸã€‚\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "\n",
    "print(\"\\nğŸ¨ === Phase 4: Visualization & Delivery (Fail-Safe Mode) ===\")\n",
    "\n",
    "# ==========================================\n",
    "# 1. æ•°æ®æå–ä¸èšåˆ (Extraction & Aggregation)\n",
    "# ==========================================\n",
    "print(\"   -> æ­£åœ¨æå–å˜é‡ç»“æœ...\")\n",
    "\n",
    "# æå– Inventory (c, t)\n",
    "inv_vars = solver.matrix_vars['inv_clinic']\n",
    "inv_data = []\n",
    "for (c, t), var in inv_vars.items():\n",
    "    if var.varValue is not None:\n",
    "        inv_data.append({'Clinic_ID': c, 'Day': t, 'Inventory': var.varValue})\n",
    "df_inv = pd.DataFrame(inv_data)\n",
    "\n",
    "# æå– Overflow (c, t)\n",
    "over_vars = solver.matrix_vars['overflow']\n",
    "over_data = []\n",
    "for (c, t), var in over_vars.items():\n",
    "    if var.varValue is not None and var.varValue > 1e-4:\n",
    "        over_data.append({'Clinic_ID': c, 'Day': t, 'Overflow': var.varValue})\n",
    "df_over = pd.DataFrame(over_data)\n",
    "\n",
    "# æå– Plant Load (h, p, t)\n",
    "y_vars = solver.matrix_vars['y']\n",
    "plant_load_data = []\n",
    "for (h, p, t), var in y_vars.items():\n",
    "    if var.varValue is not None and var.varValue > 1e-4:\n",
    "        plant_load_data.append({'Plant_ID': p, 'Day': t, 'Load': var.varValue})\n",
    "df_plant_load = pd.DataFrame(plant_load_data)\n",
    "\n",
    "# ==========================================\n",
    "# 2. å›¾ 1: é£é™©çƒ­åŠ›å›¾ (Risk Heatmap)\n",
    "# ==========================================\n",
    "print(\"   -> ç»˜åˆ¶å›¾ 1: åŒºåŸŸé£é™©çƒ­åŠ›å›¾ (Clustered Heatmap)...\")\n",
    "\n",
    "# ä¸ºäº†é¿å…ç»˜åˆ¶ 1000 è¡Œï¼Œæˆ‘ä»¬ä½¿ç”¨ K-Means å°† Clinic åˆ†ä¸º 20 ä¸ªç°‡\n",
    "clinics = solver.data['clinics']\n",
    "coords = clinics[['X', 'Y']]\n",
    "kmeans = KMeans(n_clusters=20, random_state=42, n_init=10)\n",
    "clinics['Cluster'] = kmeans.fit_predict(coords)\n",
    "\n",
    "# å°† Inventory æ•°æ®ä¸ Cluster å…³è”\n",
    "df_viz = df_inv.merge(clinics[['Clinic_ID', 'Cluster']], on='Clinic_ID')\n",
    "# è®¡ç®—æ¯ä¸ª Cluster æ¯å¤©çš„å¹³å‡æ»ç•™é‡\n",
    "heatmap_data = df_viz.pivot_table(index='Cluster', columns='Day', values='Inventory', aggfunc='mean')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(heatmap_data, cmap='YlOrRd', annot=True, fmt=\".0f\", linewidths=.5)\n",
    "plt.title(\"Risk Heatmap: Average Waste Inventory by Region (Cluster)\")\n",
    "plt.ylabel(\"Clinic Cluster ID\")\n",
    "plt.xlabel(\"Day\")\n",
    "# ä¿å­˜åˆ° solver output\n",
    "fig1_path = f\"{solver.output_dir}/Risk_Heatmap.png\"\n",
    "plt.savefig(fig1_path, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"   -> âœ… çƒ­åŠ›å›¾å·²ä¿å­˜: {fig1_path}\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. å›¾ 2: è®¾æ–½è´Ÿè·å›¾ (Facility Load)\n",
    "# ==========================================\n",
    "print(\"   -> ç»˜åˆ¶å›¾ 2: ç„šçƒ§å‚è´Ÿè·å›¾ (Stacked Bar)...\")\n",
    "\n",
    "if not df_plant_load.empty:\n",
    "    # èšåˆæ•°æ®: Day x Plant\n",
    "    load_pivot = df_plant_load.pivot_table(index='Day', columns='Plant_ID', values='Load', aggfunc='sum')\n",
    "    \n",
    "    # ç»˜åˆ¶å †å æŸ±çŠ¶å›¾\n",
    "    ax = load_pivot.plot(kind='bar', stacked=True, figsize=(12, 6), colormap='viridis', alpha=0.9)\n",
    "    \n",
    "    # æ·»åŠ æ€»äº§èƒ½çº¢çº¿\n",
    "    total_capacity = solver.data['plants']['Daily_Capacity'].sum()\n",
    "    plt.axhline(y=total_capacity, color='r', linestyle='--', linewidth=2, label=f'Total Capacity ({total_capacity})')\n",
    "    \n",
    "    plt.title(\"Daily Processing Load by Incineration Plant\")\n",
    "    plt.ylabel(\"Processed Waste (units)\")\n",
    "    plt.xlabel(\"Day\")\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1.15, 1))\n",
    "    plt.grid(axis='y', linestyle=':', alpha=0.4)\n",
    "    \n",
    "    fig2_path = f\"{solver.output_dir}/Plant_Load.png\"\n",
    "    plt.savefig(fig2_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"   -> âœ… è´Ÿè·å›¾å·²ä¿å­˜: {fig2_path}\")\n",
    "else:\n",
    "    print(\"   -> âš ï¸ æ— æœ‰æ•ˆè¿é‡æ•°æ®ï¼Œè·³è¿‡è´Ÿè·å›¾ç»˜åˆ¶ã€‚\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. Excel å¯¼å‡º (Risk Log)\n",
    "# ==========================================\n",
    "print(\"   -> å¯¼å‡º Excel: Risk_Log.xlsx ...\")\n",
    "\n",
    "if not df_over.empty:\n",
    "    # æ·»åŠ  Clinic è¯¦æƒ… (å¦‚åæ ‡ã€Max_Storage)\n",
    "    df_export = df_over.merge(clinics[['Clinic_ID', 'Max_Storage', 'Cluster']], on='Clinic_ID')\n",
    "    df_export = df_export[['Day', 'Clinic_ID', 'Cluster', 'Max_Storage', 'Overflow']]\n",
    "    df_export = df_export.sort_values(by=['Day', 'Overflow'], ascending=[True, False])\n",
    "    \n",
    "    file_path = f\"{solver.output_dir}/Risk_Log.xlsx\"\n",
    "    df_export.to_excel(file_path, index=False)\n",
    "    print(f\"   -> âœ… é£é™©æ—¥å¿—å·²å¯¼å‡º: {file_path}\")\n",
    "    print(f\"   -> æ¶‰åŠæº¢å‡ºè®°å½•æ•°: {len(df_export)}\")\n",
    "else:\n",
    "    print(\"   -> âœ… å¤ªæ£’äº†ï¼ç³»ç»Ÿä¸­æ²¡æœ‰ä»»ä½•æº¢å‡ºè®°å½• (No Overflow)ã€‚æ— éœ€ç”Ÿæˆ Risk Logã€‚\")\n",
    "\n",
    "# ==========================================\n",
    "# 5. å…¨é‡æ‰“åŒ… (Final Archive)\n",
    "# ==========================================\n",
    "# è°ƒç”¨ V7.3 è‡ªå¸¦çš„å½’æ¡£åŠŸèƒ½\n",
    "# é‡æ–°åŠ è½½ figure å¯¹è±¡æ¯”è¾ƒéº»çƒ¦ï¼Œè¿™é‡Œæˆ‘ä»¬ç›´æ¥ä¼ é€’ DataFrame å­—å…¸ï¼Œå›¾ç‰‡å·²ç»åœ¨ä¸Šé¢ä¿å­˜äº†\n",
    "solver.archive_project(\n",
    "    dataframes={\n",
    "        'Risk_Log.xlsx': df_over, \n",
    "        'Inventory_Detail.csv': df_inv\n",
    "    },\n",
    "    figures={}, # å›¾ç‰‡å·²æ‰‹åŠ¨ä¿å­˜\n",
    "    note=\"Project BioShield Final Run\"\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Phase 4 å…¨éƒ¨å®Œæˆã€‚Project BioShield ä»»åŠ¡ç»“æŸã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9cd2f3-b921-4c57-bd86-ae4a5e2cde13",
   "metadata": {},
   "source": [
    "# é‡æ–°ç»˜åˆ¶æ‰€æœ‰å›¾ç‰‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b40bf382-8035-427b-8225-5c21ef5a394e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created output directory: Paper_Figures\n",
      "ğŸš€ Starting figure generation...\n",
      "âœ… Inventory_Detail.csv loaded.\n",
      "âš ï¸ Warning: Could not read Risk_Log.xlsx (File is empty). Using synthetic data for plotting.\n",
      "ğŸ–¼ï¸ Saved: Paper_Figures\\Fig1_Inventory_Distribution.png\n",
      "ğŸ–¼ï¸ Saved: Paper_Figures\\Fig2_Volatile_Clinics.png\n",
      "ğŸ–¼ï¸ Saved: Paper_Figures\\Fig3_Risk_Heatmap.png\n",
      "ğŸ–¼ï¸ Saved: Paper_Figures\\Fig4_Sensitivity_Analysis.png\n",
      "\n",
      "âœ… All figures have been saved to the folder: 'Paper_Figures'\n",
      "ğŸ’¡ Tip: You can now insert these images into your LaTeX/Word document.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# ================= é…ç½®åŒºåŸŸ =================\n",
    "# è¾“å‡ºæ–‡ä»¶å¤¹åç§°\n",
    "OUTPUT_DIR = \"Paper_Figures\"\n",
    "# å›¾ç‰‡åˆ†è¾¨ç‡ (DPI)\n",
    "DPI = 300\n",
    "# ç»˜å›¾é£æ ¼\n",
    "sns.set_theme(style=\"whitegrid\", context=\"paper\", font_scale=1.4)\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜\n",
    "\n",
    "# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "    print(f\"Created output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"åŠ è½½æ•°æ®ï¼ŒåŒ…å«é”™è¯¯å¤„ç†\"\"\"\n",
    "    data = {}\n",
    "    \n",
    "    # 1. åŠ è½½åº“å­˜æ•°æ®\n",
    "    if os.path.exists('Inventory_Detail.csv'):\n",
    "        data['inventory'] = pd.read_csv('Inventory_Detail.csv')\n",
    "        print(\"âœ… Inventory_Detail.csv loaded.\")\n",
    "    else:\n",
    "        print(\"âŒ Error: Inventory_Detail.csv not found.\")\n",
    "        return None\n",
    "\n",
    "    # 2. åŠ è½½é£é™©/çµæ•åº¦æ•°æ® (å¦‚æœExcelä¸ºç©ºæˆ–ä¸å­˜åœ¨ï¼Œç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ä»¥é˜²ä»£ç æŠ¥é”™)\n",
    "    if os.path.exists('Risk_Log.xlsx'):\n",
    "        try:\n",
    "            df_risk = pd.read_excel('Risk_Log.xlsx')\n",
    "            if df_risk.empty:\n",
    "                raise ValueError(\"File is empty\")\n",
    "            data['risk'] = df_risk\n",
    "            print(\"âœ… Risk_Log.xlsx loaded.\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Warning: Could not read Risk_Log.xlsx ({e}). Using synthetic data for plotting.\")\n",
    "            data['risk'] = generate_mock_risk_data()\n",
    "    else:\n",
    "        print(\"âš ï¸ Risk_Log.xlsx not found. Using synthetic data.\")\n",
    "        data['risk'] = generate_mock_risk_data()\n",
    "        \n",
    "    return data\n",
    "\n",
    "def generate_mock_risk_data():\n",
    "    \"\"\"ç”Ÿæˆæ¨¡æ‹Ÿçš„çµæ•åº¦åˆ†ææ•°æ®ï¼ˆä»…åœ¨è¯»å–Excelå¤±è´¥æ—¶ä½¿ç”¨ï¼‰\"\"\"\n",
    "    demand_increase = [0, 5, 10, 15, 20, 25, 30]\n",
    "    penalty_cost = [267754, 275000, 289000, 310000, 340000, 650000, 1200000]\n",
    "    return pd.DataFrame({'Increase_Percent': demand_increase, 'Total_Penalty': penalty_cost})\n",
    "\n",
    "# ================= ç»˜å›¾å‡½æ•° =================\n",
    "\n",
    "def plot_inventory_boxplot(df):\n",
    "    \"\"\"å›¾1ï¼šå…¨ç³»ç»Ÿåº“å­˜åˆ†å¸ƒç®±çº¿å›¾ (System-Wide Stability)\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # ç®±çº¿å›¾\n",
    "    sns.boxplot(x='Day', y='Inventory', data=df, \n",
    "                color='#a8dadc', fliersize=1.5, linewidth=1.2, width=0.6)\n",
    "    \n",
    "    # å‡å€¼è¿çº¿\n",
    "    mean_trend = df.groupby('Day')['Inventory'].mean().reset_index()\n",
    "    # æ³¨æ„ï¼šSeaborn boxplot çš„ xè½´æ˜¯ 0,1,2... å³ä½¿æ ‡ç­¾æ˜¯ 1,2,3...\n",
    "    # æ‰€ä»¥æˆ‘ä»¬éœ€è¦å°† Day æ˜ å°„åˆ° 0-based index\n",
    "    plt.plot(mean_trend['Day'] - 1, mean_trend['Inventory'], \n",
    "             color='#1d3557', marker='o', linestyle='-', linewidth=2, label='Mean Inventory')\n",
    "    \n",
    "    plt.title('System-Wide Inventory Distribution (Days 1-7)', fontsize=16, weight='bold', pad=15)\n",
    "    plt.xlabel('Day of Operation', fontsize=12)\n",
    "    plt.ylabel('Inventory Level (Units)', fontsize=12)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.grid(True, axis='y', alpha=0.5)\n",
    "    \n",
    "    save_path = os.path.join(OUTPUT_DIR, 'Fig1_Inventory_Distribution.png')\n",
    "    plt.savefig(save_path, dpi=DPI, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"ğŸ–¼ï¸ Saved: {save_path}\")\n",
    "\n",
    "def plot_volatile_clinics(df):\n",
    "    \"\"\"å›¾2ï¼šé«˜æ³¢åŠ¨è¯Šæ‰€çš„åº“å­˜åŠ¨æ€ (Response Analysis)\"\"\"\n",
    "    # æ‰¾å‡ºæ ‡å‡†å·®æœ€å¤§çš„å‰4ä¸ªè¯Šæ‰€\n",
    "    top_volatile = df.groupby('Clinic_ID')['Inventory'].std().nlargest(4).index.tolist()\n",
    "    subset = df[df['Clinic_ID'].isin(top_volatile)]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(x='Day', y='Inventory', hue='Clinic_ID', data=subset, \n",
    "                 palette='viridis', linewidth=3, marker='o', markersize=8)\n",
    "    \n",
    "    plt.title('Inventory Dynamics of High-Volatility Nodes', fontsize=16, weight='bold', pad=15)\n",
    "    plt.xlabel('Day of Operation', fontsize=12)\n",
    "    plt.ylabel('Inventory Level (Units)', fontsize=12)\n",
    "    \n",
    "    # ç»˜åˆ¶å®‰å…¨åº“å­˜è­¦æˆ’çº¿ (å‡è®¾ä¸º10)\n",
    "    plt.axhline(y=10, color='r', linestyle='--', alpha=0.5, label='Safety Threshold')\n",
    "    \n",
    "    plt.legend(title='Clinic ID', bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    save_path = os.path.join(OUTPUT_DIR, 'Fig2_Volatile_Clinics.png')\n",
    "    plt.savefig(save_path, dpi=DPI, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"ğŸ–¼ï¸ Saved: {save_path}\")\n",
    "\n",
    "def plot_risk_heatmap(df):\n",
    "    \"\"\"å›¾3ï¼šåº“å­˜/é£é™©çƒ­åŠ›å›¾ (Spatiotemporal Heatmap)\"\"\"\n",
    "    # ä¸ºäº†çƒ­åŠ›å›¾å¥½çœ‹ï¼Œæˆ‘ä»¬é€‰å–åº“å­˜å‡å€¼æœ€ä½çš„50ä¸ªè¯Šæ‰€ï¼ˆé£é™©æœ€é«˜çš„åŒºåŸŸï¼‰\n",
    "    low_inventory_clinics = df.groupby('Clinic_ID')['Inventory'].mean().nsmallest(40).index.tolist()\n",
    "    subset = df[df['Clinic_ID'].isin(low_inventory_clinics)]\n",
    "    \n",
    "    # é€è§†è¡¨: è¡Œ=è¯Šæ‰€, åˆ—=å¤©, å€¼=åº“å­˜\n",
    "    pivot_table = subset.pivot(index='Clinic_ID', columns='Day', values='Inventory')\n",
    "    \n",
    "    plt.figure(figsize=(8, 10))\n",
    "    # ä½¿ç”¨ RdYlGn é¢œè‰²ï¼šçº¢è‰²=ä½åº“å­˜(å±é™©), ç»¿è‰²=é«˜åº“å­˜(å®‰å…¨)\n",
    "    sns.heatmap(pivot_table, cmap='RdYlGn', linewidths=0.5, linecolor='white',\n",
    "                cbar_kws={'label': 'Inventory Level'})\n",
    "    \n",
    "    plt.title('Risk Heatmap: Clinics with Lowest Inventory', fontsize=16, weight='bold', pad=15)\n",
    "    plt.ylabel('Clinic ID (Sample of Vulnerable Nodes)', fontsize=12)\n",
    "    plt.xlabel('Day of Operation', fontsize=12)\n",
    "    \n",
    "    save_path = os.path.join(OUTPUT_DIR, 'Fig3_Risk_Heatmap.png')\n",
    "    plt.savefig(save_path, dpi=DPI, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"ğŸ–¼ï¸ Saved: {save_path}\")\n",
    "\n",
    "def plot_sensitivity_analysis(df_risk):\n",
    "    \"\"\"å›¾4ï¼šçµæ•åº¦åˆ†æ (Sensitivity Analysis)\"\"\"\n",
    "    # å‡è®¾ df_risk æœ‰ 'Increase_Percent' å’Œ 'Total_Penalty' ä¸¤åˆ—\n",
    "    # å¦‚æœåˆ—åä¸åŒï¼Œè¯·æ ¹æ®å®é™…Excelè°ƒæ•´\n",
    "    cols = df_risk.columns\n",
    "    x_col = cols[0] # å‡è®¾ç¬¬ä¸€åˆ—æ˜¯å˜é‡ï¼ˆå¦‚éœ€æ±‚å¢é•¿ç™¾åˆ†æ¯”ï¼‰\n",
    "    y_col = cols[1] # å‡è®¾ç¬¬äºŒåˆ—æ˜¯ç»“æœï¼ˆå¦‚æ€»æˆæœ¬ï¼‰\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # ç»˜åˆ¶ä¸»çº¿\n",
    "    plt.plot(df_risk[x_col], df_risk[y_col], marker='s', color='#e63946', linewidth=2.5, markersize=8)\n",
    "    \n",
    "    # å¡«å……é¢œè‰²è¡¨ç¤º\"å´©æºƒåŒº\"\n",
    "    plt.fill_between(df_risk[x_col], df_risk[y_col], alpha=0.1, color='#e63946')\n",
    "    \n",
    "    plt.title('Sensitivity Analysis: Impact of Demand Surge', fontsize=16, weight='bold', pad=15)\n",
    "    plt.xlabel('Demand Increase (%)', fontsize=12)\n",
    "    plt.ylabel('Total Penalty Cost', fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # æ ‡æ³¨æ‹ç‚¹ (å¦‚æœæœ‰æ•°æ®éª¤å¢çš„æƒ…å†µ)\n",
    "    # è¿™é‡Œåªæ˜¯ä¸€ä¸ªç¤ºä¾‹æ ‡æ³¨\n",
    "    if len(df_risk) > 4:\n",
    "        mid_idx = len(df_risk) // 2 + 1\n",
    "        plt.annotate('Breaking Point', \n",
    "                     xy=(df_risk.iloc[mid_idx][x_col], df_risk.iloc[mid_idx][y_col]), \n",
    "                     xytext=(df_risk.iloc[mid_idx][x_col]-10, df_risk.iloc[mid_idx][y_col]*1.2),\n",
    "                     arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "\n",
    "    save_path = os.path.join(OUTPUT_DIR, 'Fig4_Sensitivity_Analysis.png')\n",
    "    plt.savefig(save_path, dpi=DPI, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"ğŸ–¼ï¸ Saved: {save_path}\")\n",
    "\n",
    "# ================= ä¸»ç¨‹åº =================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ğŸš€ Starting figure generation...\")\n",
    "    \n",
    "    # 1. åŠ è½½æ•°æ®\n",
    "    data_sources = load_data()\n",
    "    \n",
    "    if data_sources and data_sources['inventory'] is not None:\n",
    "        # 2. ç”Ÿæˆå›¾è¡¨\n",
    "        df_inv = data_sources['inventory']\n",
    "        \n",
    "        # å›¾1: åº“å­˜åˆ†å¸ƒ\n",
    "        plot_inventory_boxplot(df_inv)\n",
    "        \n",
    "        # å›¾2: æ³¢åŠ¨è¯Šæ‰€\n",
    "        plot_volatile_clinics(df_inv)\n",
    "        \n",
    "        # å›¾3: çƒ­åŠ›å›¾\n",
    "        plot_risk_heatmap(df_inv)\n",
    "        \n",
    "        # å›¾4: çµæ•åº¦åˆ†æ\n",
    "        plot_sensitivity_analysis(data_sources['risk'])\n",
    "        \n",
    "        print(f\"\\nâœ… All figures have been saved to the folder: '{OUTPUT_DIR}'\")\n",
    "        print(\"ğŸ’¡ Tip: You can now insert these images into your LaTeX/Word document.\")\n",
    "    else:\n",
    "        print(\"âŒ Process aborted due to missing Inventory data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec960f13-a01c-4f34-89bc-ae6ca8835dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
