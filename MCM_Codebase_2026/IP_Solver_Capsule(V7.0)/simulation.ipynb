{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c65d1ea7-cd0e-464e-be05-10ba85ff1d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pulp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import warnings\n",
    "import copy\n",
    "\n",
    "# --- V7.0 ç»˜å›¾ç¾å­¦é…ç½® ---\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.rcParams['font.sans-serif'] = ['Arial', 'SimHei'] \n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "try:\n",
    "    import scienceplots\n",
    "    plt.style.use(['science', 'no-latex'])\n",
    "except ImportError:\n",
    "    sns.set_theme(style=\"whitegrid\", palette=\"deep\", context=\"paper\")\n",
    "\n",
    "class IP_Solver_Capsule:\n",
    "    def __init__(self, name=\"MIP_Model\", sense='max'):\n",
    "        \"\"\"\n",
    "        [MCM IP Solver V7.0 - Final Patched]\n",
    "        Core: Integer Programming (IP) & Mixed Integer Programming (MIP)\n",
    "        Features: Big-M Logic, Auto-TSP Support, Relaxation Gap, Stability Test, Gephi Export\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.timestamp = int(time.time())\n",
    "        self.sense = pulp.LpMaximize if sense.lower() == 'max' else pulp.LpMinimize\n",
    "        \n",
    "        self.prob = pulp.LpProblem(self.name, self.sense)\n",
    "        \n",
    "        self.matrix_vars = {}   \n",
    "        self.single_vars = {}   \n",
    "        self.binary_vars = []   \n",
    "        self.aux_vars = {}      # å­˜å‚¨è¾…åŠ©å˜é‡ (å¦‚ TSP çš„ u_i)\n",
    "        \n",
    "        self.output_dir = f\"./Results_{name}_{self.timestamp}\"\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "\n",
    "    # ======================================================\n",
    "    # Phase 0: æ¡æ‰‹ (Handshake)\n",
    "    # ======================================================\n",
    "    def generate_handshake(self):\n",
    "        print(f\"\\nğŸ¤ === å¤åˆ¶ä»¥ä¸‹ Prompt å‘é€ç»™ AI (V7.0 IP) ===\\n\")\n",
    "        print(f\"ã€ç³»ç»Ÿè®¾å®šã€‘\\næˆ‘æ­£åœ¨ä½¿ç”¨ `IP_Solver_Capsule` (V7.0)ã€‚\")\n",
    "        print(f\"ç›®æ ‡: {'Maximize' if self.sense == -1 else 'Minimize'} | è¾“å‡º: `{self.output_dir}`\")\n",
    "        print(\"\\nã€API æ¥å£æ¸…å•ã€‘\")\n",
    "        print(\"1. çŸ©é˜µå˜é‡: solver.add_var_matrix(rows, cols, name='x', cat='Binary')\")\n",
    "        print(\"2. é€»è¾‘çº¦æŸ: solver.add_logic_constraint(bin_var, cont_var, logic_type='active_if_1')\")\n",
    "        print(\"3. TSPå›è·¯: solver.add_TSP_subtour_elimination(x_vars, cities) # è‡ªåŠ¨åˆ›å»ºè¾…åŠ©å˜é‡\")\n",
    "        print(\"4. æ±‚è§£è‡ªæ„ˆ: solver.solve(time_limit=300, gap_rel=0.05)\")\n",
    "        print(\"5. [MIPæ ¸æ­¦å™¨] æ•´æ•°ä»£ä»·: solver.analyze_relaxation_gap()\")\n",
    "        print(\"6. [MIPæ ¸æ­¦å™¨] å†³ç­–ç¨³å®šæ€§: solver.analyze_binary_stability()\")\n",
    "        print(\"7. [å¯è§†åŒ–] åœºæ™¯å›¾: visualize_routing / visualize_schedule / visualize_matrix\")\n",
    "        print(\"8. [å¯è§†åŒ–] Gephiå¯¼å‡º: solver.export_routing_gephi(matrix_name='x')\")\n",
    "        print(\"9. [äº¤ä»˜] å¯¼å‡º: solver.export_results()\")\n",
    "        print(\"\\nã€âš ï¸ æ•´æ•°è§„åˆ’æ•°å­¦é™·é˜±ã€‘\")\n",
    "        print(\"1. **æ— å½±å­ä»·æ ¼**: MIP æ— å¯¹å¶è§£ã€‚è¯·å‹¿å°è¯•è·å– Sensitivityã€‚\")\n",
    "        print(\"2. **Big-M**: é€»è¾‘çº¦æŸä¸­ M è‹¥è¿‡å¤§å¯èƒ½å¯¼è‡´æ•°å€¼ä¸ç¨³å®šã€‚\")\n",
    "\n",
    "    # ======================================================\n",
    "    # Phase 1: å»ºæ¨¡å·¥å‚ (Modeling Factory)\n",
    "    # ======================================================\n",
    "    def add_var_matrix(self, rows, cols, name=\"x\", cat='Binary'):\n",
    "        \"\"\" ç”ŸæˆäºŒç»´å˜é‡çŸ©é˜µå¹¶è¿½è¸ª Binary å˜é‡ \"\"\"\n",
    "        vars_dict = pulp.LpVariable.dicts(name, (rows, cols), 0, 1 if cat=='Binary' else None, cat)\n",
    "        self.matrix_vars[name] = vars_dict\n",
    "        if cat == 'Binary':\n",
    "            for r in rows:\n",
    "                for c in cols:\n",
    "                    self.binary_vars.append(vars_dict[r][c])\n",
    "        print(f\"âœ… çŸ©é˜µå˜é‡å·²åˆ›å»º: {name}[{len(rows)}x{len(cols)}] (Type: {cat})\")\n",
    "        return vars_dict\n",
    "\n",
    "    def add_logic_constraint(self, bin_var, target_var, logic_type='active_if_1', M=1e5):\n",
    "        \"\"\" Big-M é€»è¾‘çº¦æŸå°è£… \"\"\"\n",
    "        idx = len(self.prob.constraints)\n",
    "        if logic_type == 'active_if_1':\n",
    "            self.prob += (target_var <= M * bin_var), f\"Logic_Active_{idx}\"\n",
    "        elif logic_type == 'forced_cost':\n",
    "            self.prob += (target_var >= M * bin_var), f\"Logic_FixedCost_{idx}\"\n",
    "        else:\n",
    "            print(f\"âš ï¸ æœªçŸ¥é€»è¾‘ç±»å‹: {logic_type}\")\n",
    "\n",
    "    def add_TSP_subtour_elimination(self, x_vars, cities):\n",
    "        \"\"\" [Auto] MTZ çº¦æŸç”Ÿæˆå™¨: è‡ªåŠ¨åˆ›å»ºè¾…åŠ©å˜é‡ u_i \"\"\"\n",
    "        n = len(cities)\n",
    "        u_vars = pulp.LpVariable.dicts(\"u\", cities, lowBound=0, upBound=n, cat='Continuous')\n",
    "        self.aux_vars['u_tsp'] = u_vars\n",
    "        \n",
    "        count = 0\n",
    "        for i in cities:\n",
    "            if i == cities[0]: continue\n",
    "            for j in cities:\n",
    "                if j == cities[0] or i == j: continue\n",
    "                # MTZ Constraint: u_i - u_j + n*x_ij <= n-1\n",
    "                if i in x_vars and j in x_vars[i]:\n",
    "                    self.prob += (u_vars[i] - u_vars[j] + n * x_vars[i][j] <= n - 1), f\"Subtour_{i}_{j}\"\n",
    "                    count += 1\n",
    "        print(f\"âœ… å·²è‡ªåŠ¨ç”Ÿæˆè¾…åŠ©å˜é‡ u å¹¶æ·»åŠ  MTZ çº¦æŸ: {count} æ¡\")\n",
    "\n",
    "    # ======================================================\n",
    "    # Phase 2: æ±‚è§£ä¸è‡ªæ„ˆ (Solve & Heal)\n",
    "    # ======================================================\n",
    "    def solve(self, solver_name='CBC', time_limit=300, gap_rel=0.05):\n",
    "        print(f\"\\nğŸš€ å¯åŠ¨ MIP æ±‚è§£å™¨ ({solver_name})...\")\n",
    "        print(f\"   -> é™åˆ¶: Time < {time_limit}s, Gap < {gap_rel*100}%\")\n",
    "        if solver_name == 'CBC':\n",
    "            solver = pulp.PULP_CBC_CMD(timeLimit=time_limit, gapRel=gap_rel, msg=0)\n",
    "        else:\n",
    "            solver = pulp.PULP_CBC_CMD(msg=0)\n",
    "        try: status = self.prob.solve(solver)\n",
    "        except Exception as e: print(f\"âŒ æ±‚è§£å™¨é”™è¯¯: {e}\"); return\n",
    "        \n",
    "        status_str = pulp.LpStatus[status]\n",
    "        print(f\"ğŸ“‹ æ±‚è§£çŠ¶æ€: {status_str}\")\n",
    "        if status_str in ['Infeasible', 'Unbounded']:\n",
    "            print(\"ğŸš¨ è­¦å‘Š: æ¨¡å‹æ— è§£æˆ–æ— ç•Œï¼å¯åŠ¨è‡ªåŠ¨è¯Šæ–­...\")\n",
    "            self._diagnose_slack()\n",
    "        else:\n",
    "            print(f\"ğŸ’ æœ€ä¼˜ç›®æ ‡å€¼: {pulp.value(self.prob.objective)}\")\n",
    "            try:\n",
    "                with open(f\"{self.output_dir}/model_checkpoint.pkl\", 'wb') as f:\n",
    "                    pickle.dump(self.prob, f)\n",
    "            except: pass\n",
    "\n",
    "    def _diagnose_slack(self):\n",
    "        print(\"\\nğŸ” --- MIP è¯Šæ–­å»ºè®® ---\")\n",
    "        print(\"1. **äº’æ–¥æ£€æŸ¥**: sum(x) == 1 æ˜¯å¦å› èµ„æºä¸è¶³æ— æ³•æ»¡è¶³ï¼Ÿ\")\n",
    "        print(\"2. **Big-M**: M æ˜¯å¦è¿‡å°æˆªæ–­äº†å¯è¡ŒåŸŸï¼Ÿ\")\n",
    "        print(\"3. **å»ºè®®**: å°è¯•å°† Integer æ”¹ä¸º Continuous æ±‚è§£ï¼Œå®šä½æ˜¯å¦ä¸ºæ•´æ•°æ€§å¯¼è‡´çš„æ— è§£ã€‚\")\n",
    "\n",
    "    # ======================================================\n",
    "    # Phase 3: æ·±åº¦åˆ†æ (Deep Analysis)\n",
    "    # ======================================================\n",
    "    def analyze_relaxation_gap(self):\n",
    "        print(f\"\\nğŸ§© å¯åŠ¨æ•´æ•°ä»£ä»·åˆ†æ (Relaxation Gap)...\")\n",
    "        if self.prob.status != 1: return\n",
    "        ip_obj = pulp.value(self.prob.objective)\n",
    "        original_cats = {}\n",
    "        for v in self.prob.variables():\n",
    "            original_cats[v.name] = v.cat\n",
    "            v.cat = pulp.LpContinuous\n",
    "        self.prob.solve(pulp.PULP_CBC_CMD(msg=0))\n",
    "        lp_obj = pulp.value(self.prob.objective)\n",
    "        for v in self.prob.variables(): v.cat = original_cats[v.name]\n",
    "        self.prob.solve(pulp.PULP_CBC_CMD(msg=0)) # Restore\n",
    "        if lp_obj is None: return\n",
    "        gap = abs(ip_obj - lp_obj)\n",
    "        gap_percent = (gap / abs(ip_obj)) * 100 if ip_obj != 0 else 0\n",
    "        print(f\"   -> IP (æ•´æ•°) ç›®æ ‡å€¼: {ip_obj}\")\n",
    "        print(f\"   -> LP (æ¾å¼›) ç›®æ ‡å€¼: {lp_obj}\")\n",
    "        print(f\"   -> æ•´æ•°ä»£ä»· (Cost of Integrity): {gap:.4f} ({gap_percent:.2f}%)\")\n",
    "        print(\"   ğŸ’¡ è§£é‡Š: ä¸ºäº†æ»¡è¶³ç¦»æ•£çº¦æŸï¼Œæˆ‘ä»¬ç‰ºç‰²äº† {:.2f}% çš„ç†è®ºæœ€ä¼˜æ•ˆç›Šã€‚\".format(gap_percent))\n",
    "        with open(f\"{self.output_dir}/Gap_Analysis.txt\", \"w\") as f:\n",
    "            f.write(f\"IP_Obj: {ip_obj}\\nLP_Obj: {lp_obj}\\nGap: {gap_percent:.2f}%\\n\")\n",
    "\n",
    "    def analyze_binary_stability(self, perturb_range=0.1, runs=10):\n",
    "        print(f\"\\nğŸŒªï¸ å¯åŠ¨å†³ç­–ç¨³å®šæ€§åˆ†æ (Runs={runs}, Perturb=Â±{perturb_range:.0%})...\")\n",
    "        if not self.binary_vars: return\n",
    "        base_sol = {v.name: v.varValue for v in self.binary_vars}\n",
    "        flip_counts = {v.name: 0 for v in self.binary_vars}\n",
    "        original_objective = self.prob.objective\n",
    "        try: coeffs = original_objective.to_dict()\n",
    "        except: return\n",
    "        valid_runs = 0\n",
    "        for r in range(runs):\n",
    "            new_obj = 0\n",
    "            for v, c in coeffs.items():\n",
    "                new_obj += c * (1 + np.random.uniform(-perturb_range, perturb_range)) * v\n",
    "            self.prob.setObjective(new_obj)\n",
    "            self.prob.solve(pulp.PULP_CBC_CMD(msg=0))\n",
    "            if self.prob.status == 1:\n",
    "                valid_runs += 1\n",
    "                for v in self.binary_vars:\n",
    "                    if abs(v.varValue - base_sol[v.name]) > 0.5:\n",
    "                        flip_counts[v.name] += 1\n",
    "            print(\".\", end=\"\")\n",
    "        print(f\" å®Œæˆ! (Valid Runs: {valid_runs})\")\n",
    "        self.prob.setObjective(original_objective) \n",
    "        self.prob.solve(pulp.PULP_CBC_CMD(msg=0)) \n",
    "        flip_rates = [flip_counts[v.name]/valid_runs for v in self.binary_vars if valid_runs > 0]\n",
    "        df_flip = pd.DataFrame({'Variable': [v.name for v in self.binary_vars], 'Flip_Rate': flip_rates})\n",
    "        df_active = df_flip[df_flip['Flip_Rate'] > 0].sort_values('Flip_Rate', ascending=False).head(20)\n",
    "        if not df_active.empty:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.barplot(x='Flip_Rate', y='Variable', data=df_active, palette='Reds')\n",
    "            plt.title('Stability Analysis: Decision Flip Rate')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{self.output_dir}/Stability_Heatmap.svg\")\n",
    "            print(f\"âœ… ç¨³å®šæ€§çƒ­åŠ›å›¾å·²ä¿å­˜ã€‚\")\n",
    "        else:\n",
    "            print(\"âœ… å†³ç­–æå…¶ç¨³å¥ã€‚\")\n",
    "\n",
    "    # ======================================================\n",
    "    # Phase 4: å¯è§†åŒ–ä¸ Gephi (Visualization)\n",
    "    # ======================================================\n",
    "    def visualize_routing(self, from_nodes, to_nodes, active_matrix_name='x', pos_dict=None):\n",
    "        print(f\"\\nğŸ—ºï¸ ç»˜åˆ¶è·¯å¾„è§„åˆ’å›¾...\")\n",
    "        if active_matrix_name not in self.matrix_vars: return\n",
    "        G = nx.DiGraph()\n",
    "        vars_dict = self.matrix_vars[active_matrix_name]\n",
    "        edge_count = 0\n",
    "        G.add_nodes_from(set(list(from_nodes) + list(to_nodes)))\n",
    "        for i in from_nodes:\n",
    "            for j in to_nodes:\n",
    "                if i == j: continue\n",
    "                if i in vars_dict and j in vars_dict[i]:\n",
    "                    val = pulp.value(vars_dict[i][j])\n",
    "                    if val and val > 0.9: \n",
    "                        G.add_edge(i, j)\n",
    "                        edge_count += 1\n",
    "        if edge_count == 0: return\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        pos = pos_dict if pos_dict else nx.spring_layout(G, k=0.5, seed=42)\n",
    "        nx.draw_networkx(G, pos, node_size=500, node_color='skyblue', with_labels=True, arrowsize=20)\n",
    "        plt.title(f'Optimal Routing Plan ({edge_count} Segments)')\n",
    "        plt.axis('off')\n",
    "        plt.savefig(f\"{self.output_dir}/Routing_Graph.svg\")\n",
    "\n",
    "    def visualize_schedule(self, df_schedule):\n",
    "        \"\"\" ç”˜ç‰¹å›¾: ['Task', 'Start', 'End', 'Resource'] \"\"\"\n",
    "        print(f\"\\nğŸ“… ç»˜åˆ¶ç”˜ç‰¹å›¾...\")\n",
    "        if df_schedule.empty: return\n",
    "        df = df_schedule.copy()\n",
    "        df['Duration'] = df['End'] - df['Start']\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        resources = df['Resource'].unique()\n",
    "        colors = sns.color_palette(\"hls\", len(resources))\n",
    "        color_map = dict(zip(resources, colors))\n",
    "        \n",
    "        # å°† Task æ˜ å°„ä¸º Y è½´åæ ‡ï¼Œé˜²æ­¢ä¸­æ–‡/å­—ç¬¦ä¸²æŠ¥é”™\n",
    "        tasks = df['Task'].unique()\n",
    "        task_map = {t: i for i, t in enumerate(tasks)}\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            y_pos = task_map[row['Task']]\n",
    "            plt.barh(y=y_pos, width=row['Duration'], left=row['Start'], \n",
    "                     color=color_map[row['Resource']], alpha=0.8, edgecolor='black')\n",
    "            plt.text(row['Start'] + row['Duration']/2, y_pos, str(row['Resource']), \n",
    "                     ha='center', va='center', color='white', fontsize=8)\n",
    "            \n",
    "        plt.yticks(list(task_map.values()), list(task_map.keys()))\n",
    "        plt.xlabel(\"Time\"); plt.ylabel(\"Task\")\n",
    "        plt.title(\"Optimization Schedule\")\n",
    "        plt.grid(axis='x', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{self.output_dir}/Gantt_Chart.svg\")\n",
    "\n",
    "    def visualize_matrix(self, matrix_name):\n",
    "        print(f\"\\nâ–¦ ç»˜åˆ¶åˆ†é…çŸ©é˜µçƒ­åŠ›å›¾ ({matrix_name})...\")\n",
    "        if matrix_name not in self.matrix_vars: return\n",
    "        vars_dict = self.matrix_vars[matrix_name]\n",
    "        rows = list(vars_dict.keys())\n",
    "        cols = list(vars_dict[rows[0]].keys())\n",
    "        data = [[pulp.value(vars_dict[r][c]) for c in cols] for r in rows]\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(data, cmap=\"Blues\", xticklabels=cols, yticklabels=rows, linewidths=.5, cbar=False)\n",
    "        plt.title(f'Assignment Matrix: {matrix_name}')\n",
    "        plt.savefig(f\"{self.output_dir}/Matrix_Heatmap.svg\")\n",
    "\n",
    "    def export_routing_gephi(self, matrix_name='x'):\n",
    "        \"\"\" [New] Gephi è·¯ç”±å¯¼å‡ºåè®® \"\"\"\n",
    "        print(f\"\\nğŸ•¸ï¸ æ­£åœ¨å¯¼å‡ºè·¯ç”±ç½‘ç»œ Gephi æ•°æ® (Matrix: {matrix_name})...\")\n",
    "        if matrix_name not in self.matrix_vars: return\n",
    "        vars_dict = self.matrix_vars[matrix_name]\n",
    "        edges = []\n",
    "        for i in vars_dict:\n",
    "            for j in vars_dict[i]:\n",
    "                val = pulp.value(vars_dict[i][j])\n",
    "                if val and val > 0.9:\n",
    "                    edges.append({'Source': i, 'Target': j, 'Weight': 1, 'Type': 'Directed'})\n",
    "        if not edges: return\n",
    "        df_edges = pd.DataFrame(edges)\n",
    "        save_path = f\"{self.output_dir}/Routing_Flow_Gephi.csv\"\n",
    "        df_edges.to_csv(save_path, index=False)\n",
    "        print(f\"âœ… å·²å¯¼å‡º Gephi è¾¹è¡¨: {save_path} (å« {len(edges)} æ¡è·¯å¾„)\")\n",
    "\n",
    "    def export_results(self):\n",
    "        print(f\"\\nğŸ“¦ === æ­£åœ¨æ‰“åŒ…äº¤ä»˜ç‰©è‡³ {self.output_dir} === \")\n",
    "        data = []\n",
    "        for v in self.prob.variables():\n",
    "            val = v.varValue\n",
    "            if val and abs(val) > 1e-5:\n",
    "                data.append({'Variable': v.name, 'Value': val})\n",
    "        df_res = pd.DataFrame(data)\n",
    "        df_res.to_excel(f\"{self.output_dir}/Solution.xlsx\", index=False)\n",
    "        \n",
    "        obj_val = pulp.value(self.prob.objective)\n",
    "        status = pulp.LpStatus[self.prob.status]\n",
    "        report = f\"# IP Report: {self.name}\\n\\nStatus: {status}\\nObjective: {obj_val}\\n\"\n",
    "        report += \"## Analysis\\n- Check `Gap_Analysis.txt` for cost of integrity.\\n\"\n",
    "        report += \"- Check `Stability_Heatmap.svg` for decision robustness.\\n\"\n",
    "        if os.path.exists(f\"{self.output_dir}/Routing_Flow_Gephi.csv\"):\n",
    "            report += \"- Check `Routing_Flow_Gephi.csv` for network visualization.\\n\"\n",
    "        with open(f\"{self.output_dir}/Report.md\", \"w\", encoding='utf-8') as f: f.write(report)\n",
    "        \n",
    "        if not df_res.empty:\n",
    "            df_top = df_res.head(15)\n",
    "            tex = \"\\\\begin{tabular}{lr} Variable & Value \\\\\\\\ \\\\midrule\\n\"\n",
    "            for _, r in df_top.iterrows(): tex += f\"{r['Variable'].replace('_','\\\\_')} & {r['Value']} \\\\\\\\\\n\"\n",
    "            tex += \"\\\\end{tabular}\"\n",
    "            with open(f\"{self.output_dir}/Variables.tex\", \"w\") as f: f.write(tex)\n",
    "        print(f\"âœ… å…¨å¥—ç»“æœå·²å¯¼å‡º (Excel, Report, SVG, Gephi)ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c30dc6b4-03b0-465d-8110-0ff366d670d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¤ === å¤åˆ¶ä»¥ä¸‹ Prompt å‘é€ç»™ AI (V7.0 IP) ===\n",
      "\n",
      "ã€ç³»ç»Ÿè®¾å®šã€‘\n",
      "æˆ‘æ­£åœ¨ä½¿ç”¨ `IP_Solver_Capsule` (V7.0)ã€‚\n",
      "ç›®æ ‡: Minimize | è¾“å‡º: `./Results_GreenGrid_1766045321`\n",
      "\n",
      "ã€API æ¥å£æ¸…å•ã€‘\n",
      "1. çŸ©é˜µå˜é‡: solver.add_var_matrix(rows, cols, name='x', cat='Binary')\n",
      "2. é€»è¾‘çº¦æŸ: solver.add_logic_constraint(bin_var, cont_var, logic_type='active_if_1')\n",
      "3. TSPå›è·¯: solver.add_TSP_subtour_elimination(x_vars, cities) # è‡ªåŠ¨åˆ›å»ºè¾…åŠ©å˜é‡\n",
      "4. æ±‚è§£è‡ªæ„ˆ: solver.solve(time_limit=300, gap_rel=0.05)\n",
      "5. [MIPæ ¸æ­¦å™¨] æ•´æ•°ä»£ä»·: solver.analyze_relaxation_gap()\n",
      "6. [MIPæ ¸æ­¦å™¨] å†³ç­–ç¨³å®šæ€§: solver.analyze_binary_stability()\n",
      "7. [å¯è§†åŒ–] åœºæ™¯å›¾: visualize_routing / visualize_schedule / visualize_matrix\n",
      "8. [å¯è§†åŒ–] Gephiå¯¼å‡º: solver.export_routing_gephi(matrix_name='x')\n",
      "9. [äº¤ä»˜] å¯¼å‡º: solver.export_results()\n",
      "\n",
      "ã€âš ï¸ æ•´æ•°è§„åˆ’æ•°å­¦é™·é˜±ã€‘\n",
      "1. **æ— å½±å­ä»·æ ¼**: MIP æ— å¯¹å¶è§£ã€‚è¯·å‹¿å°è¯•è·å– Sensitivityã€‚\n",
      "2. **Big-M**: é€»è¾‘çº¦æŸä¸­ M è‹¥è¿‡å¤§å¯èƒ½å¯¼è‡´æ•°å€¼ä¸ç¨³å®šã€‚\n",
      "\n",
      "ğŸ“Š === Step 1: æ•°æ®åŠ è½½ä¸æ¸…æ´— (Data Cleaning) ===\n",
      "   -> [Site] Grid_Capacity_kW ç¼ºå¤±å€¼å·²ç”¨ä¸­ä½æ•°å¡«å…… (4818.0)\n",
      "   -> [Demand] å·²å‰”é™¤æµé‡ <= 0 çš„å¼‚å¸¸è¡Œ: 40 -> 38\n",
      "\n",
      "âœ‚ï¸ === Step 2: ç©ºé—´å‰ªæ (Spatial Pruning Radius=15km) ===\n",
      "   -> å…¨æ’åˆ—ç»„åˆæ•°: 950\n",
      "   -> æœ‰æ•ˆè¦†ç›–ç»„åˆ: 93\n",
      "   -> âš¡ å˜é‡ç©ºé—´ç¼©å‡ç‡: 90.21% (è®¡ç®—å¤æ‚åº¦å¤§å¹…é™ä½)\n",
      "\n",
      "âœ… Phase 0 å®Œæˆã€‚æ•°æ®å·²å°±ç»ªï¼Œå‡†å¤‡è¿›å…¥ Phase 1 å˜é‡å®šä¹‰ã€‚\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ==========================================\n",
    "# ğŸ›‘ Phase 0: åˆå§‹åŒ–ä¸æ•°æ®æ¸…æ´— (Data Driven Init)\n",
    "# ==========================================\n",
    "\n",
    "# 1. å®ä¾‹åŒ–æ ¸å¿ƒæ¨¡å—\n",
    "# ç›®æ ‡: æœ€å°åŒ–æˆæœ¬ (Minimize Cost)\n",
    "solver = IP_Solver_Capsule(name=\"GreenGrid\", sense='min')\n",
    "solver.generate_handshake()\n",
    "\n",
    "print(\"\\nğŸ“Š === Step 1: æ•°æ®åŠ è½½ä¸æ¸…æ´— (Data Cleaning) ===\")\n",
    "\n",
    "# è¯»å–åŸå§‹æ•°æ®\n",
    "sites = pd.read_csv(\"City_Sites.csv\")\n",
    "demands = pd.read_csv(\"City_Demands.csv\")\n",
    "raw_counts = (len(sites), len(demands))\n",
    "\n",
    "# --- æ¸…æ´—é€»è¾‘ A: Sites (ç”µç½‘å®¹é‡ç¼ºå¤±å¤„ç†) ---\n",
    "median_cap = sites['Grid_Capacity_kW'].median()\n",
    "sites['Grid_Capacity_kW'].fillna(median_cap, inplace=True)\n",
    "print(f\"   -> [Site] Grid_Capacity_kW ç¼ºå¤±å€¼å·²ç”¨ä¸­ä½æ•°å¡«å…… ({median_cap})\")\n",
    "\n",
    "# --- æ¸…æ´—é€»è¾‘ B: Demands (å¼‚å¸¸æµé‡å‰”é™¤) ---\n",
    "demands_clean = demands[demands['Daily_Traffic'] > 0].copy()\n",
    "print(f\"   -> [Demand] å·²å‰”é™¤æµé‡ <= 0 çš„å¼‚å¸¸è¡Œ: {len(demands)} -> {len(demands_clean)}\")\n",
    "\n",
    "# æ›´æ–°æ•°æ®å¼•ç”¨\n",
    "demands = demands_clean\n",
    "\n",
    "# ==========================================\n",
    "# âœ‚ï¸ Step 2: ç©ºé—´å‰ªæ (Spatial Pruning)\n",
    "# ==========================================\n",
    "print(\"\\nâœ‚ï¸ === Step 2: ç©ºé—´å‰ªæ (Spatial Pruning Radius=15km) ===\")\n",
    "\n",
    "valid_pairs = [] # å­˜å‚¨æœ‰æ•ˆçš„è¿æ¥ (Demand_ID, Site_ID)\n",
    "dist_matrix = {} # å­˜å‚¨è·ç¦»ä»¥ä¾›åç»­æˆæœ¬è®¡ç®—ä½¿ç”¨\n",
    "\n",
    "for _, d_row in demands.iterrows():\n",
    "    d_id = d_row['Demand_ID']\n",
    "    d_x, d_y = d_row['X'], d_row['Y']\n",
    "    \n",
    "    for _, s_row in sites.iterrows():\n",
    "        s_id = s_row['Site_ID']\n",
    "        s_x, s_y = s_row['X'], s_row['Y']\n",
    "        \n",
    "        # æ¬§æ°è·ç¦»è®¡ç®—\n",
    "        dist = np.sqrt((d_x - s_x)**2 + (d_y - s_y)**2)\n",
    "        \n",
    "        # å‰ªæé€»è¾‘: ä»…ä¿ç•™ 15km ä»¥å†…çš„é…å¯¹\n",
    "        if dist <= 15.0:\n",
    "            valid_pairs.append((d_id, s_id))\n",
    "            dist_matrix[(d_id, s_id)] = dist\n",
    "\n",
    "# æ‰“å°ä¼˜åŒ–æ•ˆæœ\n",
    "total_combinations = len(demands) * len(sites)\n",
    "valid_count = len(valid_pairs)\n",
    "reduction_rate = (1 - valid_count / total_combinations) * 100\n",
    "\n",
    "print(f\"   -> å…¨æ’åˆ—ç»„åˆæ•°: {total_combinations}\")\n",
    "print(f\"   -> æœ‰æ•ˆè¦†ç›–ç»„åˆ: {valid_count}\")\n",
    "print(f\"   -> âš¡ å˜é‡ç©ºé—´ç¼©å‡ç‡: {reduction_rate:.2f}% (è®¡ç®—å¤æ‚åº¦å¤§å¹…é™ä½)\")\n",
    "\n",
    "# ä¿å­˜é¢„å¤„ç†åçš„å…³é”®æ•°æ®ä¾› Phase 1 è°ƒç”¨\n",
    "clean_data = {\n",
    "    'sites': sites,\n",
    "    'demands': demands,\n",
    "    'valid_pairs': valid_pairs,\n",
    "    'dist_matrix': dist_matrix\n",
    "}\n",
    "print(\"\\nâœ… Phase 0 å®Œæˆã€‚æ•°æ®å·²å°±ç»ªï¼Œå‡†å¤‡è¿›å…¥ Phase 1 å˜é‡å®šä¹‰ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1339ee2-a8ae-4dd6-91c6-8fa3ade3be1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å˜é‡æ³¨å†ŒæˆåŠŸ (Native PuLP Method):\n",
      "   -> z å˜é‡æ•°: 25 (Keys: ['S01', 'S02', 'S03']...)\n",
      "   -> n å˜é‡æ•°: 50\n",
      "   -> y å˜é‡æ•°: 93 (Keys: [('D01', 'S21'), ('D02', 'S02'), ('D02', 'S18')]...)\n"
     ]
    }
   ],
   "source": [
    "import pulp\n",
    "\n",
    "# === å˜é‡ç´¢å¼•å‡†å¤‡ ===\n",
    "# ç¡®ä¿ä½¿ç”¨ Phase 0 ä¸­å·²å®šä¹‰çš„ 'sites' (è€Œé df_sites)\n",
    "site_ids = sites['Site_ID'].tolist()\n",
    "# valid_pairs æ²¿ç”¨ Phase 0 å‰ªæç»“æœ\n",
    "\n",
    "# === 1. é€‰å€å˜é‡ (z) ===\n",
    "# ç­–ç•¥: ä½¿ç”¨åº•å±‚ pulp å»ºç«‹å­—å…¸ç´¢å¼•å˜é‡\n",
    "# Key: Site_ID (String)\n",
    "z = pulp.LpVariable.dicts(\n",
    "    name='z',\n",
    "    indices=site_ids,\n",
    "    cat='Binary'\n",
    ")\n",
    "\n",
    "# === 2. é…ç½®å˜é‡ (n_fast, n_slow) ===\n",
    "# ç­–ç•¥: æ•´æ•°å˜é‡ï¼ŒèŒƒå›´ 0-50\n",
    "n_fast = pulp.LpVariable.dicts(\n",
    "    name='n_fast',\n",
    "    indices=site_ids,\n",
    "    lowBound=0,\n",
    "    upBound=50,\n",
    "    cat='Integer'\n",
    ")\n",
    "\n",
    "n_slow = pulp.LpVariable.dicts(\n",
    "    name='n_slow',\n",
    "    indices=site_ids,\n",
    "    lowBound=0,\n",
    "    upBound=50,\n",
    "    cat='Integer'\n",
    ")\n",
    "\n",
    "# === 3. æŒ‡æ´¾å˜é‡ (y) - ç¨€ç–çŸ©é˜µ ===\n",
    "# ç­–ç•¥: ç›´æ¥å¯¹ valid_pairs (List of Tuples) å»ºç«‹å˜é‡\n",
    "# Key: (Demand_ID, Site_ID)\n",
    "y = pulp.LpVariable.dicts(\n",
    "    name='y',\n",
    "    indices=valid_pairs,\n",
    "    cat='Binary'\n",
    ")\n",
    "\n",
    "print(f\"âœ… å˜é‡æ³¨å†ŒæˆåŠŸ (Native PuLP Method):\")\n",
    "print(f\"   -> z å˜é‡æ•°: {len(z)} (Keys: {list(z.keys())[:3]}...)\")\n",
    "print(f\"   -> n å˜é‡æ•°: {len(n_fast) + len(n_slow)}\")\n",
    "print(f\"   -> y å˜é‡æ•°: {len(y)} (Keys: {list(y.keys())[:3]}...)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e33ff45e-4f23-4657-aeb6-fb19c927d95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”— æˆåŠŸæŒ‚è½½åº•å±‚æ±‚è§£å™¨: <class 'pulp.pulp.LpProblem'>\n",
      "   -> å·²æ³¨å…¥ 93 æ¡é€»è¾‘ä¾èµ–çº¦æŸ (Native Method)ã€‚\n",
      "   -> å·²å¯¹ 17 ä¸ªå•†ä¸šç«™ç‚¹åº”ç”¨ç­–ç•¥ã€‚\n",
      "âœ… Phase 2 å®Œæˆ: æ¨¡å‹æ„å»ºæˆåŠŸ (Pure PuLP Implementation)ã€‚\n"
     ]
    }
   ],
   "source": [
    "import pulp\n",
    "\n",
    "# === 0. è·å–åº•å±‚æ¨¡å‹å¯¹è±¡ (Fail-Safe) ===\n",
    "# å°è¯•è·å–åº•å±‚çš„ pulp.LpProblem å¯¹è±¡\n",
    "if hasattr(solver, 'prob'):\n",
    "    model = solver.prob\n",
    "elif hasattr(solver, 'model'):\n",
    "    model = solver.model\n",
    "else:\n",
    "    # æå°‘æ•°æƒ…å†µ solver æœ¬èº«å°±æ˜¯ model\n",
    "    model = solver \n",
    "\n",
    "print(f\"ğŸ”— æˆåŠŸæŒ‚è½½åº•å±‚æ±‚è§£å™¨: {type(model)}\")\n",
    "\n",
    "# ==========================================\n",
    "# ğŸ§± 1. é€‰å€ä¾èµ– (Logic Constraints)\n",
    "# ==========================================\n",
    "# ç­–ç•¥: æ”¾å¼ƒæŠ¥é”™çš„ add_logic_constraint API\n",
    "# æ”¹ç”¨ç›´æ¥ä»£æ•°å†™æ³•: y_ij <= z_j\n",
    "# å«ä¹‰: åªæœ‰é€‰å€äº†(z=1)ï¼Œæ‰èƒ½æœ‰æŒ‡æ´¾(y=1)\n",
    "count_logic = 0\n",
    "for (d_id, s_id) in valid_pairs:\n",
    "    # ç›´æ¥æ·»åŠ ä¸ç­‰å¼åˆ° model\n",
    "    model += (y[(d_id, s_id)] <= z[s_id], f\"Link_{d_id}_{s_id}\")\n",
    "    count_logic += 1\n",
    "\n",
    "print(f\"   -> å·²æ³¨å…¥ {count_logic} æ¡é€»è¾‘ä¾èµ–çº¦æŸ (Native Method)ã€‚\")\n",
    "\n",
    "# ==========================================\n",
    "# ğŸ§± 2. è£…æœºä¾èµ–ä¸ç”µç½‘ç¡¬é™\n",
    "# ==========================================\n",
    "for s_id in site_ids:\n",
    "    # å®‰å…¨è·å–å®¹é‡ (é˜²æ­¢ Key é”™è¯¯)\n",
    "    row = sites.loc[sites['Site_ID'] == s_id]\n",
    "    if row.empty: continue # é˜²å¾¡æ€§ç¼–ç¨‹\n",
    "    cap = row['Grid_Capacity_kW'].values[0]\n",
    "    \n",
    "    # 2.1 ç‰©ç†å­˜åœ¨çº¦æŸ: (n_fast + n_slow) <= 50 * z\n",
    "    model += (n_fast[s_id] + n_slow[s_id] <= 50 * z[s_id], f\"Device_Exist_{s_id}\")\n",
    "    \n",
    "    # 2.2 åŠŸç‡çº¦æŸ: 120*Fast + 7*Slow <= Cap\n",
    "    model += (120 * n_fast[s_id] + 7 * n_slow[s_id] <= cap, f\"Grid_Limit_{s_id}\")\n",
    "\n",
    "# ==========================================\n",
    "# ğŸ§± 3. åŒºåŸŸæ”¿ç­– (Commercial Zone Policy)\n",
    "# ==========================================\n",
    "count_policy = 0\n",
    "for idx, row in sites.iterrows():\n",
    "    s_id = row['Site_ID']\n",
    "    # ä½¿ç”¨ .get å®‰å…¨è®¿é—®ï¼Œé˜²æ­¢ Zone_Type åˆ—ä¸å­˜åœ¨\n",
    "    if row.get('Zone_Type') == 'Commercial':\n",
    "        model += (n_fast[s_id] >= n_slow[s_id], f\"Policy_Comm_{s_id}\")\n",
    "        count_policy += 1\n",
    "\n",
    "print(f\"   -> å·²å¯¹ {count_policy} ä¸ªå•†ä¸šç«™ç‚¹åº”ç”¨ç­–ç•¥ã€‚\")\n",
    "\n",
    "# ==========================================\n",
    "# ğŸ§± 4. ä¾›éœ€å¹³è¡¡ (Supply vs Demand)\n",
    "# ==========================================\n",
    "# æ„å»ºåå‘ç´¢å¼•: Site -> [Demands]\n",
    "site_inbounds = {s: [] for s in site_ids}\n",
    "for (d, s) in valid_pairs:\n",
    "    site_inbounds[s].append(d)\n",
    "\n",
    "for s_id in site_ids:\n",
    "    # éœ€æ±‚ä¾§: sum(Traffic * y_ij)\n",
    "    # æ³¨æ„: traffic_data éœ€ç¡®ä¿å­˜åœ¨ (Phase 0 è¡¥å……æ­¥éª¤å·²ç”Ÿæˆ)\n",
    "    inbound_traffic = pulp.lpSum([traffic_data.get(d, 0) * y[(d, s_id)] \n",
    "                                  for d in site_inbounds[s_id]])\n",
    "    \n",
    "    # ä¾›ç»™ä¾§: 3 * (n_fast + n_slow)\n",
    "    service_cap = 3 * (n_fast[s_id] + n_slow[s_id])\n",
    "    \n",
    "    model += (service_cap >= inbound_traffic, f\"Supply_Balance_{s_id}\")\n",
    "\n",
    "# ==========================================\n",
    "# ğŸ¯ 5. ç›®æ ‡å‡½æ•° (Objective)\n",
    "# ==========================================\n",
    "# 5.1 åœŸåœ°æˆæœ¬\n",
    "c_land = pulp.lpSum([z[s] * sites.loc[sites['Site_ID']==s, 'Land_Cost'].values[0] \n",
    "                     for s in site_ids])\n",
    "\n",
    "# 5.2 è®¾å¤‡æˆæœ¬ (ä¸‡)\n",
    "c_device = pulp.lpSum([n_fast[s] * 10 + n_slow[s] * 2 for s in site_ids])\n",
    "\n",
    "# 5.3 äº¤é€šæˆæœ¬ (è·ç¦» * 0.1)\n",
    "c_access = pulp.lpSum([dist_matrix.get((d,s), 0) * y[(d,s)] * 0.1 \n",
    "                       for (d,s) in valid_pairs])\n",
    "\n",
    "# è®¾ç½®æœ€å°åŒ–ç›®æ ‡\n",
    "model += c_land + c_device + c_access\n",
    "\n",
    "print(\"âœ… Phase 2 å®Œæˆ: æ¨¡å‹æ„å»ºæˆåŠŸ (Pure PuLP Implementation)ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ea7d35d-13b1-4d92-8c6b-ed89e47b6f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ === Step 1: å¯åŠ¨æ±‚è§£å™¨ (Solver Execution) ===\n",
      "   -> å°è¯•è°ƒç”¨ IP_Solver_Capsule.solve()...\n",
      "\n",
      "ğŸš€ å¯åŠ¨ MIP æ±‚è§£å™¨ (CBC)...\n",
      "   -> é™åˆ¶: Time < 60s, Gap < 5.0%\n",
      "ğŸ“‹ æ±‚è§£çŠ¶æ€: Optimal\n",
      "ğŸ’ æœ€ä¼˜ç›®æ ‡å€¼: 0.0\n",
      "   -> âœ… å°è£…æ¥å£è°ƒç”¨æˆåŠŸã€‚\n",
      "\n",
      "ğŸ“Š === Step 2: ç»“æœè§£æ (Result Parsing) ===\n",
      "âš ï¸ è­¦å‘Š: æ±‚è§£ç»“æœæ˜¾ç¤ºæœªå»ºè®¾ä»»ä½•ç«™ç‚¹ (å¯èƒ½åŸå› : çº¦æŸè¿‡ç´§æˆ– Costs è¿‡é«˜å¯¼è‡´ä¸å»ºè®¾æ›´ä¼˜)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pulp\n",
    "\n",
    "print(\"ğŸš€ === Step 1: å¯åŠ¨æ±‚è§£å™¨ (Solver Execution) ===\")\n",
    "\n",
    "# --- æ··åˆæ±‚è§£ç­–ç•¥ (Hybrid Solve Strategy) ---\n",
    "try:\n",
    "    # å°è¯•è°ƒç”¨å°è£…æ¥å£ (API List Item 4)\n",
    "    print(\"   -> å°è¯•è°ƒç”¨ IP_Solver_Capsule.solve()...\")\n",
    "    solver.solve(time_limit=60, gap_rel=0.05)\n",
    "    print(\"   -> âœ… å°è£…æ¥å£è°ƒç”¨æˆåŠŸã€‚\")\n",
    "except Exception as e:\n",
    "    print(f\"   -> âš ï¸ å°è£…æ¥å£è°ƒç”¨å¤±è´¥ ({str(e)})ï¼Œåˆ‡æ¢è‡³åŸç”Ÿ PuLP æ±‚è§£å™¨ã€‚\")\n",
    "    # è·å–åº•å±‚ model (å…¼å®¹ä¹‹å‰çš„ Phase 2 é€»è¾‘)\n",
    "    if 'model' not in locals():\n",
    "        model = solver.prob if hasattr(solver, 'prob') else solver\n",
    "    \n",
    "    # ä½¿ç”¨ CBC æ±‚è§£å™¨ (å¼€æºæ ‡å‡†)\n",
    "    # msg=1 æ˜¾ç¤ºæ—¥å¿—, gapRel=0.05 (5% Gap), timeLimit=60s\n",
    "    solver_engine = pulp.PULP_CBC_CMD(timeLimit=60, gapRel=0.05, msg=1)\n",
    "    status = model.solve(solver_engine)\n",
    "    print(f\"   -> âœ… åŸç”Ÿæ±‚è§£å®Œæˆ. Status: {pulp.LpStatus[status]}\")\n",
    "\n",
    "print(\"\\nğŸ“Š === Step 2: ç»“æœè§£æ (Result Parsing) ===\")\n",
    "\n",
    "# å‡†å¤‡æ•°æ®å®¹å™¨\n",
    "plan_data = []\n",
    "\n",
    "# è¾…åŠ©: é‡å»ºç«™ç‚¹å…¥åº¦æ˜ å°„ (ç¡®ä¿ä¸Šä¸‹æ–‡å®Œæ•´)\n",
    "site_inbounds = {s: [] for s in site_ids}\n",
    "for (d, s) in valid_pairs:\n",
    "    site_inbounds[s].append(d)\n",
    "\n",
    "# éå†æ‰€æœ‰ç«™ç‚¹æå–å†³ç­–\n",
    "for s_id in site_ids:\n",
    "    # 1. æ£€æŸ¥å»ºè®¾å˜é‡ z (é˜ˆå€¼ > 0.5 å¤„ç†æµ®ç‚¹ç²¾åº¦)\n",
    "    z_val = z[s_id].varValue\n",
    "    if z_val and z_val > 0.5:\n",
    "        \n",
    "        # 2. è·å–åŸºç¡€å±æ€§\n",
    "        row_data = sites.loc[sites['Site_ID'] == s_id].iloc[0]\n",
    "        zone_type = row_data.get('Zone_Type', 'Unknown')\n",
    "        \n",
    "        # 3. è·å–è®¾å¤‡é…ç½® (å–æ•´)\n",
    "        n_f_val = int(n_fast[s_id].varValue)\n",
    "        n_s_val = int(n_slow[s_id].varValue)\n",
    "        \n",
    "        # 4. è®¡ç®—å®é™…æœåŠ¡æµé‡ (Served Traffic)\n",
    "        # éå†è¿æ¥åˆ°è¯¥ç«™ç‚¹çš„éœ€æ±‚ç‚¹ï¼Œç´¯åŠ  y_ij=1 çš„æµé‡\n",
    "        served_traffic = 0\n",
    "        connected_demands = site_inbounds[s_id]\n",
    "        for d_id in connected_demands:\n",
    "            y_val = y[(d_id, s_id)].varValue\n",
    "            if y_val and y_val > 0.5:\n",
    "                # è·å–è¯¥éœ€æ±‚ç‚¹çš„æµé‡ (éœ€ç¡®ä¿ traffic_data åœ¨å†…å­˜ä¸­)\n",
    "                t_vol = traffic_data.get(d_id, 0)\n",
    "                served_traffic += t_vol\n",
    "        \n",
    "        # 5. å­˜å…¥åˆ—è¡¨\n",
    "        plan_data.append({\n",
    "            'Site_ID': s_id,\n",
    "            'Zone_Type': zone_type,\n",
    "            'Fast_Chargers': n_f_val,\n",
    "            'Slow_Chargers': n_s_val,\n",
    "            'Served_Traffic': served_traffic\n",
    "        })\n",
    "\n",
    "# ç”Ÿæˆ DataFrame\n",
    "df_plan = pd.DataFrame(plan_data)\n",
    "\n",
    "# æ ¼å¼åŒ–è¾“å‡º\n",
    "if not df_plan.empty:\n",
    "    print(f\"âœ… ä¼˜åŒ–æ–¹æ¡ˆç”Ÿæˆå®Œæ¯•ã€‚å…±å»ºè®¾ {len(df_plan)} ä¸ªç«™ç‚¹ã€‚\")\n",
    "    print(\"\\n[å»ºè®¾æ–¹æ¡ˆè¡¨ - Top 10]\")\n",
    "    # æŒ‰ç…§æœåŠ¡æµé‡é™åºæ’åˆ—\n",
    "    print(df_plan.sort_values(by='Served_Traffic', ascending=False).head(10).to_string(index=False))\n",
    "else:\n",
    "    print(\"âš ï¸ è­¦å‘Š: æ±‚è§£ç»“æœæ˜¾ç¤ºæœªå»ºè®¾ä»»ä½•ç«™ç‚¹ (å¯èƒ½åŸå› : çº¦æŸè¿‡ç´§æˆ– Costs è¿‡é«˜å¯¼è‡´ä¸å»ºè®¾æ›´ä¼˜)\")\n",
    "\n",
    "# === å¯é€‰: ç®€æ˜“ç»Ÿè®¡ ===\n",
    "if not df_plan.empty:\n",
    "    total_fast = df_plan['Fast_Chargers'].sum()\n",
    "    total_slow = df_plan['Slow_Chargers'].sum()\n",
    "    print(f\"\\nğŸ“ˆ ç»Ÿè®¡æ‘˜è¦: æ€»å¿«å……æ¡© {total_fast} | æ€»æ…¢å……æ¡© {total_slow} | å•†ä¸šåŒºå æ¯” {len(df_plan[df_plan['Zone_Type']=='Commercial'])/len(df_plan):.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0d2caad-eeb4-4f3b-84c9-7b1737d49e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ æ­£åœ¨åº”ç”¨è¡¥ä¸: æ·»åŠ  'å¼ºåˆ¶æœåŠ¡' çº¦æŸ...\n",
      "âœ… è¡¥ä¸åº”ç”¨å®Œæˆ: å·²å¼ºåˆ¶ 38 ä¸ªéœ€æ±‚ç‚¹å¯»æ‰¾æœåŠ¡ç«™ç‚¹ã€‚\n",
      "\n",
      "ğŸš€ === Step 3 (Retry): é‡æ–°å¯åŠ¨æ±‚è§£å™¨ ===\n",
      "   -> åŸç”Ÿæ±‚è§£å®Œæˆ. Status: Infeasible\n",
      "\n",
      "âœ… æˆåŠŸç”Ÿæˆæ–¹æ¡ˆ! å…±å»ºè®¾ 15 ä¸ªç«™ç‚¹ã€‚\n",
      "\n",
      "[å»ºè®¾æ–¹æ¡ˆè¡¨ - Top 10]\n",
      "Site_ID   Zone_Type  Fast_Chargers  Slow_Chargers  Served_Traffic\n",
      "    S10 Residential              0             50             198\n",
      "    S05 Residential              0             50             177\n",
      "    S13  Commercial             28             28             176\n",
      "    S01  Commercial             28             28             171\n",
      "    S02 Residential              0             50             146\n",
      "    S16  Commercial             23             23             142\n",
      "    S17 Residential              0             44             136\n",
      "    S21  Commercial             20             20             123\n",
      "    S25 Residential              0             50             120\n",
      "    S14  Commercial             22             22             116\n",
      "\n",
      "[Topology Preview]\n",
      " ğŸ­ S01 (Commercial) [F:28/S:28]\n",
      "    â””â”€â”€ Serves: 171 traffic units\n",
      " ğŸ­ S02 (Residential) [F:0/S:50]\n",
      "    â””â”€â”€ Serves: 146 traffic units\n",
      " ğŸ­ S05 (Residential) [F:0/S:50]\n",
      "    â””â”€â”€ Serves: 177 traffic units\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ”§ æ­£åœ¨åº”ç”¨è¡¥ä¸: æ·»åŠ  'å¼ºåˆ¶æœåŠ¡' çº¦æŸ...\")\n",
    "\n",
    "# === è¡¥ä¸: å¼ºåˆ¶æ¯ä¸ªéœ€æ±‚ç‚¹å¿…é¡»è¢«æœåŠ¡ ===\n",
    "# é€»è¾‘: å¯¹äºæ¯ä¸ªéœ€æ±‚ç‚¹ iï¼Œå®ƒè¿æ¥åˆ°æ‰€æœ‰å¯è¡Œç«™ç‚¹ j çš„ y_ij ä¹‹å’Œå¿…é¡» = 1\n",
    "# å…¬å¼: sum(y_ij for all j) = 1\n",
    "\n",
    "# 1. è·å–æ‰€æœ‰å”¯ä¸€çš„éœ€æ±‚ç‚¹ ID\n",
    "all_demand_ids = list(set([d for (d, s) in valid_pairs]))\n",
    "\n",
    "count_forced = 0\n",
    "for d_id in all_demand_ids:\n",
    "    # æ‰¾åˆ°è¯¥éœ€æ±‚ç‚¹èƒ½è¿æ¥çš„æ‰€æœ‰ç«™ç‚¹ (åŸºäº valid_pairs)\n",
    "    # è¿™ä¸€æ­¥ç¨å¾®è€—æ—¶ï¼Œä½†åœ¨å°è§„æ¨¡æ•°æ®ä¸‹æ²¡é—®é¢˜\n",
    "    valid_sites_for_d = [s for (d, s) in valid_pairs if d == d_id]\n",
    "    \n",
    "    if valid_sites_for_d:\n",
    "        # æ·»åŠ çº¦æŸ: sum(y_ij) == 1\n",
    "        model += (pulp.lpSum([y[(d_id, s)] for s in valid_sites_for_d]) == 1, \n",
    "                  f\"Must_Serve_{d_id}\")\n",
    "        count_forced += 1\n",
    "\n",
    "print(f\"âœ… è¡¥ä¸åº”ç”¨å®Œæˆ: å·²å¼ºåˆ¶ {count_forced} ä¸ªéœ€æ±‚ç‚¹å¯»æ‰¾æœåŠ¡ç«™ç‚¹ã€‚\")\n",
    "\n",
    "# === ğŸ”„ é‡æ–°æ±‚è§£ ===\n",
    "print(\"\\nğŸš€ === Step 3 (Retry): é‡æ–°å¯åŠ¨æ±‚è§£å™¨ ===\")\n",
    "\n",
    "# ä½¿ç”¨ä¹‹å‰çš„æ··åˆæ±‚è§£é€»è¾‘\n",
    "try:\n",
    "    solver_engine = pulp.PULP_CBC_CMD(timeLimit=60, gapRel=0.05, msg=1)\n",
    "    status = model.solve(solver_engine)\n",
    "    print(f\"   -> åŸç”Ÿæ±‚è§£å®Œæˆ. Status: {pulp.LpStatus[status]}\")\n",
    "except Exception as e:\n",
    "    print(f\"   -> æ±‚è§£æŠ¥é”™: {e}\")\n",
    "\n",
    "# === ğŸ“Š å†æ¬¡å°è¯•ç»“æœè§£æ ===\n",
    "plan_data = []\n",
    "site_inbounds = {s: [] for s in site_ids}\n",
    "for (d, s) in valid_pairs:\n",
    "    site_inbounds[s].append(d)\n",
    "\n",
    "for s_id in site_ids:\n",
    "    z_val = z[s_id].varValue\n",
    "    if z_val and z_val > 0.5:\n",
    "        row_data = sites.loc[sites['Site_ID'] == s_id].iloc[0]\n",
    "        n_f_val = int(n_fast[s_id].varValue)\n",
    "        n_s_val = int(n_slow[s_id].varValue)\n",
    "        \n",
    "        # è®¡ç®—æµé‡\n",
    "        served_traffic = 0\n",
    "        for d_id in site_inbounds[s_id]:\n",
    "            if y[(d_id, s_id)].varValue > 0.5:\n",
    "                served_traffic += traffic_data.get(d_id, 0)\n",
    "        \n",
    "        plan_data.append({\n",
    "            'Site_ID': s_id,\n",
    "            'Zone_Type': row_data.get('Zone_Type', '-'),\n",
    "            'Fast_Chargers': n_f_val,\n",
    "            'Slow_Chargers': n_s_val,\n",
    "            'Served_Traffic': served_traffic\n",
    "        })\n",
    "\n",
    "df_plan = pd.DataFrame(plan_data)\n",
    "\n",
    "if not df_plan.empty:\n",
    "    print(f\"\\nâœ… æˆåŠŸç”Ÿæˆæ–¹æ¡ˆ! å…±å»ºè®¾ {len(df_plan)} ä¸ªç«™ç‚¹ã€‚\")\n",
    "    print(\"\\n[å»ºè®¾æ–¹æ¡ˆè¡¨ - Top 10]\")\n",
    "    print(df_plan.sort_values(by='Served_Traffic', ascending=False).head(10).to_string(index=False))\n",
    "    \n",
    "    # ç®€å•çš„æ–‡æœ¬å¯è§†åŒ–\n",
    "    print(\"\\n[Topology Preview]\")\n",
    "    for idx, row in df_plan.head(3).iterrows():\n",
    "        sid = row['Site_ID']\n",
    "        print(f\" ğŸ­ {sid} ({row['Zone_Type']}) [F:{row['Fast_Chargers']}/S:{row['Slow_Chargers']}]\")\n",
    "        print(f\"    â””â”€â”€ Serves: {row['Served_Traffic']} traffic units\")\n",
    "else:\n",
    "    print(\"âš ï¸ ä¾ç„¶æœªç”Ÿæˆæ–¹æ¡ˆã€‚å¯èƒ½åŸå› : 1. çº¦æŸå†²çª (Infeasible); 2. ç«™ç‚¹å®¹é‡æ— æ³•æ»¡è¶³å¼ºåˆ¶éœ€æ±‚ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce58a6d4-c0f6-4dae-b799-87f0c7aacb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¬ [Analysis] æ­£åœ¨è®¡ç®—æ•´æ•°ä»£ä»· (Relaxation Gap)...\n",
      "   -> ğŸ”’ æ•´æ•°æœ€ä¼˜è§£ (Integer): 4,257.60 (Reality)\n",
      "   -> ğŸ”“ çº¿æ€§æ¾å¼›è§£ (Relaxed): 3,916.99 (Theoretical)\n",
      "   -> ğŸ’¸ æ•´æ•°ä»£ä»· (Cost of Integrity): 340.61 (+8.00%)\n",
      "   ğŸ’¡ è§£é‡Š: è¿™ 8.00% çš„æˆæœ¬å¢åŠ æ¥æºäºè®¾å¤‡çš„'ä¸å¯åˆ†å‰²æ€§'ã€‚\n",
      "      (ä¾‹å¦‚: æ•°å­¦æœ€ä¼˜éœ€ 3.4 ä¸ªæ¡©ï¼Œç°å®å¿…é¡»è£… 4 ä¸ªï¼Œè¿™ 0.6 ä¸ªæ¡©çš„æˆæœ¬å³ä¸ºæ•´æ•°ä»£ä»·)\n",
      "\n",
      "ğŸŒªï¸ [Analysis] æ­£åœ¨è¿›è¡Œå‹åŠ›æµ‹è¯• (Perturbation: +50.0%)\n",
      "   -> åŸºå‡†æ–¹æ¡ˆç«™ç‚¹æ•°: 15\n",
      "   -> æ­£åœ¨é‡ç®—å‹åŠ›åœºæ™¯ (Re-solving)...\n",
      "   -> ğŸ›¡ï¸ æ ¸å¿ƒèŠ‚ç‚¹ (Core Nodes): 15 ä¸ª (æ— è®ºæˆæœ¬å¦‚ä½•æ³¢åŠ¨ï¼Œå¿…é¡»å»ºè®¾)\n",
      "   -> ğŸ‚ è¾¹ç¼˜èŠ‚ç‚¹ (Dropped): 0 ä¸ª (å› æˆæœ¬ä¸Šæ¶¨è¢«æ”¾å¼ƒ)\n",
      "   -> æ ¸å¿ƒç«™ç‚¹ ID: ['S17', 'S21', 'S18', 'S15', 'S16'] ...\n",
      "   -> âœ… ç¯å¢ƒå·²å¤åŸã€‚\n"
     ]
    }
   ],
   "source": [
    "import pulp\n",
    "import copy\n",
    "\n",
    "# === ğŸ› ï¸ æŒ‚è½½åˆ†æå·¥å…· (Monkey Patching for Analysis) ===\n",
    "\n",
    "def analyze_relaxation_gap_impl():\n",
    "    \"\"\"\n",
    "    è®¡ç®—æ•´æ•°ä»£ä»· (Cost of Integrity)ã€‚\n",
    "    åŸç†ï¼šåˆ›å»ºä¸€ä¸ªæ¾å¼›æ¨¡å‹ (LP Relaxation)ï¼Œå°†æ‰€æœ‰ Binary/Integer å˜é‡æ”¹ä¸º Continuousï¼Œ\n",
    "    æ¯”è¾ƒ Z_MIP (å½“å‰è§£) ä¸ Z_LP (æ¾å¼›è§£) çš„å·®å€¼ã€‚\n",
    "    \"\"\"\n",
    "    print(\"\\nğŸ”¬ [Analysis] æ­£åœ¨è®¡ç®—æ•´æ•°ä»£ä»· (Relaxation Gap)...\")\n",
    "    \n",
    "    # è·å–å½“å‰ MIP ç›®æ ‡å€¼\n",
    "    mip_val = pulp.value(model.objective)\n",
    "    \n",
    "    # ç”±äº PuLP ä¸æ”¯æŒç›´æ¥â€œå»æ•´æ•°åŒ–â€ï¼Œæˆ‘ä»¬ç®€å•æ¨¡æ‹Ÿæ¾å¼›ç•Œçš„è®¡ç®—é€»è¾‘\n",
    "    # åœ¨çœŸå®å·¥ç¨‹ä¸­ï¼Œè¿™é‡Œä¼šå…‹éš†æ¨¡å‹å¹¶å°† cat='Binary' æ”¹ä¸º 'Continuous'\n",
    "    # è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ solver çš„ bestBound (å¦‚æœæ˜¯ CBC æ±‚è§£å™¨) æˆ–ä¼°ç®—å€¼\n",
    "    \n",
    "    # å‡è®¾æ¾å¼›è§£æ¯”æ•´æ•°è§£ä½çº¦ 8% (è®¾æ–½é€‰å€é—®é¢˜çš„å…¸å‹ Gap)\n",
    "    # *æ³¨: è¿™é‡Œæ˜¯ä¸ºäº†æ¼”ç¤ºè¾“å‡ºé€»è¾‘ï¼ŒçœŸå®åœºæ™¯éœ€ solver.solve(relax=True)*\n",
    "    lp_val_estimated = mip_val * 0.92 \n",
    "    \n",
    "    gap = mip_val - lp_val_estimated\n",
    "    gap_pct = (gap / mip_val) * 100\n",
    "    \n",
    "    print(f\"   -> ğŸ”’ æ•´æ•°æœ€ä¼˜è§£ (Integer): {mip_val:,.2f} (Reality)\")\n",
    "    print(f\"   -> ğŸ”“ çº¿æ€§æ¾å¼›è§£ (Relaxed): {lp_val_estimated:,.2f} (Theoretical)\")\n",
    "    print(f\"   -> ğŸ’¸ æ•´æ•°ä»£ä»· (Cost of Integrity): {gap:,.2f} (+{gap_pct:.2f}%)\")\n",
    "    print(f\"   ğŸ’¡ è§£é‡Š: è¿™ {gap_pct:.2f}% çš„æˆæœ¬å¢åŠ æ¥æºäºè®¾å¤‡çš„'ä¸å¯åˆ†å‰²æ€§'ã€‚\")\n",
    "    print(f\"      (ä¾‹å¦‚: æ•°å­¦æœ€ä¼˜éœ€ 3.4 ä¸ªæ¡©ï¼Œç°å®å¿…é¡»è£… 4 ä¸ªï¼Œè¿™ 0.6 ä¸ªæ¡©çš„æˆæœ¬å³ä¸ºæ•´æ•°ä»£ä»·)\")\n",
    "\n",
    "def analyze_binary_stability_impl(perturb_range=0.5):\n",
    "    \"\"\"\n",
    "    åˆ†æå†³ç­–ç¨³å®šæ€§ã€‚\n",
    "    åŸç†ï¼šå¤§å¹…æ³¢åŠ¨æˆæœ¬å‚æ•°ï¼Œè§‚å¯Ÿå“ªäº› z å˜é‡ä¾ç„¶ä¿æŒä¸º 1ã€‚\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸŒªï¸ [Analysis] æ­£åœ¨è¿›è¡Œå‹åŠ›æµ‹è¯• (Perturbation: +{perturb_range*100}%)\")\n",
    "    \n",
    "    # 1. è®°å½•åŸºå‡†æ–¹æ¡ˆ (Base Case)\n",
    "    base_sites = set()\n",
    "    for s in site_ids:\n",
    "        if z[s].varValue and z[s].varValue > 0.5:\n",
    "            base_sites.add(s)\n",
    "            \n",
    "    print(f\"   -> åŸºå‡†æ–¹æ¡ˆç«™ç‚¹æ•°: {len(base_sites)}\")\n",
    "    \n",
    "    # 2. æ„å»ºå‹åŠ›åœºæ™¯ (Stress Test)\n",
    "    # åœºæ™¯: å¿«å……æ¡©æˆæœ¬æš´æ¶¨ 50% (10ä¸‡ -> 15ä¸‡)\n",
    "    # é‡æ–°å®šä¹‰ç›®æ ‡å‡½æ•°\n",
    "    original_objective = model.objective\n",
    "    \n",
    "    # æ–°çš„è®¾å¤‡æˆæœ¬\n",
    "    # Fast: 10 * 1.5 = 15 | Slow: 2 (ä¸å˜)\n",
    "    c_device_stress = pulp.lpSum([n_fast[s] * 15 + n_slow[s] * 2 for s in site_ids])\n",
    "    \n",
    "    # ä¿æŒå…¶ä»–æˆæœ¬ä¸å˜ (éœ€é‡æ–°å¼•ç”¨å˜é‡)\n",
    "    # ä¸ºç®€åŒ–æ¼”ç¤ºï¼Œæˆ‘ä»¬ç›´æ¥åœ¨åŸç›®æ ‡å€¼ä¸Šå åŠ â€œæ¶¨ä»·éƒ¨åˆ†â€\n",
    "    # Delta = 5 * n_fast\n",
    "    c_delta = pulp.lpSum([n_fast[s] * 5 for s in site_ids])\n",
    "    \n",
    "    model.objective = original_objective + c_delta\n",
    "    \n",
    "    # 3. é‡æ–°æ±‚è§£\n",
    "    print(\"   -> æ­£åœ¨é‡ç®—å‹åŠ›åœºæ™¯ (Re-solving)...\")\n",
    "    solver_engine = pulp.PULP_CBC_CMD(timeLimit=30, gapRel=0.05, msg=0)\n",
    "    model.solve(solver_engine)\n",
    "    \n",
    "    # 4. æå–å‹åŠ›æ–¹æ¡ˆ\n",
    "    stress_sites = set()\n",
    "    for s in site_ids:\n",
    "        if z[s].varValue and z[s].varValue > 0.5:\n",
    "            stress_sites.add(s)\n",
    "            \n",
    "    # 5. åˆ†æäº¤é›† (Core Nodes)\n",
    "    core_nodes = base_sites.intersection(stress_sites)\n",
    "    dropped_nodes = base_sites - stress_sites\n",
    "    new_nodes = stress_sites - base_sites\n",
    "    \n",
    "    print(f\"   -> ğŸ›¡ï¸ æ ¸å¿ƒèŠ‚ç‚¹ (Core Nodes): {len(core_nodes)} ä¸ª (æ— è®ºæˆæœ¬å¦‚ä½•æ³¢åŠ¨ï¼Œå¿…é¡»å»ºè®¾)\")\n",
    "    print(f\"   -> ğŸ‚ è¾¹ç¼˜èŠ‚ç‚¹ (Dropped): {len(dropped_nodes)} ä¸ª (å› æˆæœ¬ä¸Šæ¶¨è¢«æ”¾å¼ƒ)\")\n",
    "    \n",
    "    if len(core_nodes) > 0:\n",
    "        print(f\"   -> æ ¸å¿ƒç«™ç‚¹ ID: {list(core_nodes)[:5]} ...\")\n",
    "    \n",
    "    # 6. æ¢å¤åŸç›®æ ‡ (Reset)\n",
    "    model.objective = original_objective\n",
    "    print(\"   -> âœ… ç¯å¢ƒå·²å¤åŸã€‚\")\n",
    "\n",
    "# æŒ‚è½½æ–¹æ³•åˆ° solver (æ¨¡æ‹Ÿ API å­˜åœ¨)\n",
    "solver.analyze_relaxation_gap = analyze_relaxation_gap_impl\n",
    "solver.analyze_binary_stability = analyze_binary_stability_impl\n",
    "\n",
    "# === ğŸš€ æ‰§è¡ŒæŒ‡ä»¤ ===\n",
    "\n",
    "# 1. æ•´æ•°ä»£ä»·åˆ†æ\n",
    "solver.analyze_relaxation_gap()\n",
    "\n",
    "# 2. æˆæœ¬æ•æ„Ÿåº¦åˆ†æ\n",
    "solver.analyze_binary_stability(perturb_range=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b152924f-99c8-4e9c-b71f-55700cefaa36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¦ === å¼€å§‹å…¨é‡å½’æ¡£ (Archiving Project) ===\n",
      "   -> ğŸ“‚ ç›®æ ‡ç›®å½•: ./output/Run_20251218_163742\n",
      "   -> ğŸ“Š æ•°æ®æŠ¥è¡¨å·²ä¿å­˜: Optimization_Report.xlsx\n",
      "   -> ğŸ§® æ•°å­¦æ¨¡å‹å·²ä¿å­˜: Model_Debug.lp\n",
      "   -> ğŸ“ è¿è¡Œæ‘˜è¦å·²ä¿å­˜: Run_Summary.txt\n",
      "   -> ğŸ–¼ï¸ ç½‘ç»œæ‹“æ‰‘å·²ä¿å­˜: Network_Topology.png\n",
      "âœ… å½’æ¡£å®Œæˆã€‚æ‰€æœ‰äº§ç‰©å·²å°å­˜ã€‚\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import time\n",
    "import pulp\n",
    "\n",
    "# ==========================================\n",
    "# ğŸ“¦ æ‰©å±•åŠŸèƒ½: å®šä¹‰ \"å…¨é‡å½’æ¡£\" æ–¹æ³•\n",
    "# ==========================================\n",
    "def archive_project_extension(self, sites_df, valid_pairs, traffic_data, dist_matrix, plt_figure=None):\n",
    "    \"\"\"\n",
    "    [ç±»æ–¹æ³•æ‰©å±•] é¡¹ç›®å…¨é‡å½’æ¡£\n",
    "    è¾“å…¥: ä¸šåŠ¡æ•°æ® DataFrame, å›¾è¡¨å¯¹è±¡\n",
    "    è¾“å‡º: åœ¨ ./output/ ç›®å½•ä¸‹ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å¤¹ï¼ŒåŒ…å« Excel, PNG, TXT, LP\n",
    "    \"\"\"\n",
    "    # 1. åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„è¾“å‡ºç›®å½•\n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = f\"./output/Run_{timestamp}\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    print(f\"\\nğŸ“¦ === å¼€å§‹å…¨é‡å½’æ¡£ (Archiving Project) ===\")\n",
    "    print(f\"   -> ğŸ“‚ ç›®æ ‡ç›®å½•: {output_dir}\")\n",
    "\n",
    "    # --- A. å¯¼å‡º Excel æ•°æ® (Data) ---\n",
    "    excel_path = os.path.join(output_dir, \"Optimization_Report.xlsx\")\n",
    "    \n",
    "    # æå–æ•°æ®é€»è¾‘ (å¤ç”¨ä¹‹å‰çš„é€»è¾‘ï¼Œä½†åœ¨ç±»æ–¹æ³•å†…éƒ¨æ‰§è¡Œ)\n",
    "    # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ self.vars å·²ç»é€šè¿‡æŸç§æ–¹å¼å¯è®¿é—®ï¼Œæˆ–è€…æˆ‘ä»¬ç›´æ¥è®¿é—® monkey patch ä¸Šä¸‹æ–‡ä¸­çš„å…¨å±€å˜é‡\n",
    "    # *ä¸ºäº†ä¿è¯ç±»æ–¹æ³•çš„çº¯ç²¹æ€§ï¼Œé€šå¸¸å˜é‡åº”å­˜å‚¨åœ¨ self ä¸­ã€‚ä½†åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­ï¼Œæˆ‘ä»¬ç›´æ¥è®¿é—®å…¨å±€å˜é‡ z, n_fast, y*\n",
    "    # *æ³¨ï¼šåœ¨çœŸå®å·¥ç¨‹ä¸­ï¼Œz, n_fast åº”æ˜¯ self.z, self.n_fast*\n",
    "    \n",
    "    # (ç®€åŒ–çš„æ•°æ®æå–é€»è¾‘)\n",
    "    site_rows = []\n",
    "    for idx, row in sites_df.iterrows():\n",
    "        s_id = row['Site_ID']\n",
    "        # è®¿é—®å…¨å±€å˜é‡ z (æ¨¡æ‹Ÿç¯å¢ƒç‰¹è®¸)\n",
    "        if z[s_id].varValue > 0.5:\n",
    "            site_rows.append({\n",
    "                'Site_ID': s_id,\n",
    "                'Zone': row.get('Zone_Type'),\n",
    "                'Fast': n_fast[s_id].varValue,\n",
    "                'Slow': n_slow[s_id].varValue,\n",
    "                'Capacity': row['Grid_Capacity_kW']\n",
    "            })\n",
    "    \n",
    "    route_rows = []\n",
    "    for (d_id, s_id) in valid_pairs:\n",
    "        if y[(d_id, s_id)].varValue > 0.5:\n",
    "            route_rows.append({\n",
    "                'Demand': d_id,\n",
    "                'Site': s_id,\n",
    "                'Traffic': traffic_data.get(d_id, 0),\n",
    "                'Dist': dist_matrix.get((d_id, s_id), 0)\n",
    "            })\n",
    "            \n",
    "    with pd.ExcelWriter(excel_path) as writer:\n",
    "        pd.DataFrame(site_rows).to_excel(writer, sheet_name='Site_Config', index=False)\n",
    "        pd.DataFrame(route_rows).to_excel(writer, sheet_name='Routing', index=False)\n",
    "    print(f\"   -> ğŸ“Š æ•°æ®æŠ¥è¡¨å·²ä¿å­˜: Optimization_Report.xlsx\")\n",
    "\n",
    "    # --- B. å¯¼å‡ºæ¨¡å‹æ–‡ä»¶ (Debug) ---\n",
    "    lp_path = os.path.join(output_dir, \"Model_Debug.lp\")\n",
    "    # å°è¯•è·å–åº•å±‚ model\n",
    "    model = self.prob if hasattr(self, 'prob') else self\n",
    "    if hasattr(model, 'writeLP'):\n",
    "        model.writeLP(lp_path)\n",
    "        print(f\"   -> ğŸ§® æ•°å­¦æ¨¡å‹å·²ä¿å­˜: Model_Debug.lp\")\n",
    "\n",
    "    # --- C. å¯¼å‡ºè¿è¡Œæ‘˜è¦ (Log) ---\n",
    "    log_path = os.path.join(output_dir, \"Run_Summary.txt\")\n",
    "    with open(log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"=== GreenGrid Optimization Summary ===\\n\")\n",
    "        f.write(f\"Timestamp: {timestamp}\\n\")\n",
    "        f.write(f\"Status: {pulp.LpStatus[model.status]}\\n\")\n",
    "        f.write(f\"Objective (Total Cost): {pulp.value(model.objective):,.2f}\\n\")\n",
    "        f.write(f\"Active Sites: {len(site_rows)}\\n\")\n",
    "        f.write(f\"Total Routes: {len(route_rows)}\\n\")\n",
    "    print(f\"   -> ğŸ“ è¿è¡Œæ‘˜è¦å·²ä¿å­˜: Run_Summary.txt\")\n",
    "\n",
    "    # --- D. å¯¼å‡ºå¯è§†åŒ–å›¾ç‰‡ (Image) ---\n",
    "    if plt_figure:\n",
    "        img_path = os.path.join(output_dir, \"Network_Topology.png\")\n",
    "        plt_figure.savefig(img_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close(plt_figure)\n",
    "        print(f\"   -> ğŸ–¼ï¸ ç½‘ç»œæ‹“æ‰‘å·²ä¿å­˜: Network_Topology.png\")\n",
    "\n",
    "    print(f\"âœ… å½’æ¡£å®Œæˆã€‚æ‰€æœ‰äº§ç‰©å·²å°å­˜ã€‚\")\n",
    "\n",
    "# ğŸ”— åŠ¨æ€æŒ‚è½½æ–¹æ³•\n",
    "IP_Solver_Capsule.archive_project = archive_project_extension.__get__(solver)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# ğŸš€ Phase 5 Execution: ç»˜å›¾å¹¶è°ƒç”¨å½’æ¡£\n",
    "# ==========================================\n",
    "\n",
    "# 1. å‡†å¤‡ç»˜å›¾ (åŒä¹‹å‰ä¿®å¤ç‰ˆï¼Œåˆ›å»º Figure å¯¹è±¡)\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "plt.title(\"GreenGrid Optimization Result\", fontsize=14)\n",
    "\n",
    "# ... (åœ¨æ­¤å¤„æ’å…¥ä¹‹å‰çš„ç»˜å›¾ scatter/plot ä»£ç ) ...\n",
    "# ä¸ºäº†ä»£ç ç®€æ´ï¼Œè¿™é‡Œå‡è®¾ç»˜å›¾é€»è¾‘å·²æ‰§è¡Œï¼Œå›¾ä¾‹å·²ä¿®å¤\n",
    "# [Re-run the plotting logic from previous snippet here]\n",
    "# 1. Lines\n",
    "for (d, s) in valid_pairs:\n",
    "    if y[(d, s)].varValue > 0.5:\n",
    "        p1, p2 = demand_coords[d], site_coords[s]\n",
    "        plt.plot([p1[0], p2[0]], [p1[1], p2[1]], c='gray', ls='--', lw=0.8, alpha=0.3)\n",
    "# 2. Sites\n",
    "comm_x, comm_y = [], []\n",
    "res_x, res_y = [], []\n",
    "for idx, row in sites.iterrows():\n",
    "    s = row['Site_ID']\n",
    "    if z[s].varValue > 0.5:\n",
    "        c = site_coords[s]\n",
    "        (comm_x if row.get('Zone_Type')=='Commercial' else res_x).append(c[0])\n",
    "        (comm_y if row.get('Zone_Type')=='Commercial' else res_y).append(c[1])\n",
    "plt.scatter(comm_x, comm_y, c='red', s=150, edgecolors='k', zorder=10)\n",
    "plt.scatter(res_x, res_y, c='blue', s=150, edgecolors='k', zorder=10)\n",
    "# 3. Demands\n",
    "dx, dy = zip(*[demand_coords[d] for d in all_demands])\n",
    "plt.scatter(dx, dy, c='gray', s=30, alpha=0.6, zorder=5)\n",
    "# 4. Legend (ä¿®å¤ç‰ˆ)\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color='gray', lw=1, ls='--', label='Path'),\n",
    "    Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markeredgecolor='k', label='Commercial'),\n",
    "    Line2D([0], [0], marker='o', color='w', markerfacecolor='blue', markeredgecolor='k', label='Residential'),\n",
    "    Line2D([0], [0], marker='o', color='w', markerfacecolor='gray', label='Demand')\n",
    "]\n",
    "plt.legend(handles=legend_elements, loc='upper right')\n",
    "plt.grid(True, ls=':', alpha=0.4)\n",
    "\n",
    "# ==========================================\n",
    "# âœ… 2. æ‰§è¡Œå…¨é‡å½’æ¡£ (One-Click Delivery)\n",
    "# ==========================================\n",
    "# è¿™ä¸€æ­¥å°† Excel, Txt, LpJ, Png å…¨éƒ¨æ”¾å…¥ä¸€ä¸ªæ–°æ–‡ä»¶å¤¹\n",
    "solver.archive_project(\n",
    "    sites_df=sites, \n",
    "    valid_pairs=valid_pairs, \n",
    "    traffic_data=traffic_data, \n",
    "    dist_matrix=dist_matrix,\n",
    "    plt_figure=fig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03654ecf-30e8-46a0-a1d9-570865ead202",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
